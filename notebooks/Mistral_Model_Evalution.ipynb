{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral Models RAG Pipeline Evaluation with LlamaIndex and Ragas\n",
    "\n",
    "> *This notebook should work well in the `Data Science 3.0` kernel on Amazon SageMaker Studio. It requires Python v3.10+*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook is designed to evaluate the performance of the Retrieval-Augmented Generation (RAG) pipeline. The RAG pipeline leverages a retriever component to identify relevant context from a knowledge base and a generator component to produce fluent and informative responses based on the retrieved context.\n",
    "\n",
    "In this notebook, we will explore the RAG pipeline using the [LlamaIndex Evaluation library](https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation), which provides a comprehensive set of tools for building and evaluating question-answering systems. Additionally, we will utilize the [Ragas](https://docs.Ragas.io/en/stable/) (RAG Assessment) framework, designed specifically for assessing the performance of RAG models.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Why We Need Evaluators like LlamaIndex Evaluators and Ragas\n",
    "\n",
    "We need evaluators like LlamaIndex evaluators and Ragas in a RAG pipeline primarily because language models, including those used in the RAG pipeline, can suffer from issues like hallucination, factual inconsistencies, and biases. Evaluators help us assess the performance and reliability of the RAG pipeline, ensuring that it provides accurate, relevant, and trustworthy responses.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "1. **Mitigating Hallucination**: Language models, especially large language models used in RAG pipelines, can sometimes generate plausible-sounding but factually incorrect or made-up information, a phenomenon known as hallucination. Evaluators help identify instances of hallucination by comparing the generated responses against the ground truth or the source knowledge base.\n",
    "\n",
    "2. **Ensuring Faithfulness and Factual Correctness**: Evaluators assess the faithfulness and factual correctness of the generated responses by comparing them with the source knowledge base or reference data. This is crucial in domains where accurate and reliable information is essential, such as healthcare, finance, or legal contexts.\n",
    "\n",
    "3. **Measuring Relevance and Context Understanding**: Evaluators can measure how relevant and contextually appropriate the generated responses are, given the input query and the retrieved context from the knowledge base. This helps identify cases where the RAG pipeline fails to understand the query or retrieves irrelevant information.\n",
    "\n",
    "4. **Quantifying Performance**: Evaluators provide quantitative metrics, such as accuracy, precision, recall, and F1-score, which allow for objective comparisons of different RAG pipeline configurations, retriever-generator model combinations, or training strategies.\n",
    "\n",
    "5. **Identifying Biases and Inconsistencies**: Evaluators can help identify biases and inconsistencies in the generated responses, which may arise due to biases in the training data or the language model itself. This is important for ensuring fairness and avoiding potentially harmful biases in the RAG pipeline's outputs.\n",
    "\n",
    "6. **Tailored Evaluation**: Frameworks like Ragas provide a structured approach to creating tailored test sets and evaluating the RAG pipeline's performance on specific types of queries or domains, allowing for more targeted assessments.\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The primary objectives of this notebook are:\n",
    "\n",
    "1. **Implement the RAG pipeline**: We will set up the RAG pipeline using LlamaIndex, configuring the retriever and generator components according to best practices. Mistral 7B Instruct LLM will be utilized as the generator component in this example.\n",
    "\n",
    "2. **Create and evaluate the LlamaIndex Query Engine**: We will create a LlamaIndex Query Engine to facilitate efficient retrieval and generation of answers from the knowledge base.\n",
    "\n",
    "3. **Generate test sets with LlamaIndex DatasetGenerator and Ragas TestsetGenerator**: To evaluate the RAG pipeline's performance, we will generate synthetic test sets using LlamaIndex [DatasetGenerator module](https://docs.llamaindex.ai/en/stable/api_reference/evaluation/dataset_generation/). In this example, **Mistral 7B Instruct** model will be used to generate Q&A pairs while the **Mistral Large** model will be used to generate \"ground truth\" for the synthetic dataset. Additionally, we will leverage the Ragas [TestsetGenerator module](https://docs.Ragas.io/en/latest/getstarted/testset_generation.html) to create tailored test sets aligned with our specific needs. As for the Ragas section, only Mistral 7B Instruct will be utilized.\n",
    "\n",
    "4. **Evaluate pipeline performance using LlamaIndex evaluators and Ragas**: We will leverage both the LlamaIndex evaluators and the Ragas framework to comprehensively assess the performance of the RAG pipeline on a range of question-answering tasks. Using the generated test sets, we will analyze metrics such as faithfulness, relevancy, correctness, and other relevant measures to gain insights into the pipeline's strengths and weaknesses.\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By the end of this notebook, we expect to achieve the following outcomes:\n",
    "\n",
    "1. A functional RAG pipeline implemented using LlamaIndex, capable of answering questions based on a knowledge base.\n",
    "\n",
    "2. A LlamaIndex Query Engine for efficient retrieval and generation of answers.\n",
    "\n",
    "3. Synthetic test sets generated using LlamaIndex DatasetGenerator and tailored test sets created with the Ragas TestsetGenerator.\n",
    "\n",
    "4. Comprehensive performance evaluation of the RAG pipeline using both LlamaIndex evaluators and the Ragas framework, including quantitative metrics such as faithfulness, relevancy, correctness, semantic similarity, and other relevant measures, as well as qualitative analysis.\n",
    "\n",
    "5. Insights into the impact of different pipeline configurations on performance and identification of limitations and potential areas for improvement in the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup and Requirements\n",
    "\n",
    "To start exploring RAG patterns with a practical example, we'll first install some libraries that might not be present in the default notebook kernel image:\n",
    "\n",
    "- [Amazon Bedrock](https://docs.aws.amazon.com/pythonsdk/) AWS Python SDKs `boto3` and `botocore` to be able to call the service\n",
    "- [LlamaIndex](https://docs.llamaindex.ai/en/stable/getting_started/installation/) is an open-source framework to help integrate LLMs with trusted data sources, and measure the performance of data-connected LLM use-cases\n",
    "- [Ragas](https://docs.Ragas.io/en/stable/) is a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines\n",
    "- [LangChain](https://python.langchain.com/docs/get_started/introduction) is an open-source framework for orchestrating common LLM patterns. In this example, it's only used with Ragas as an optional step to generate test dataset via langchain docs. The entire RAG pipeline in this example is implemented with LlamaIndex though.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install --upgrade --no-cache-dir --force-reinstall \\\n",
    "    boto3 \\\n",
    "    botocore \\\n",
    "    langchain \\\n",
    "    langchain-aws \\\n",
    "    llama-index \\\n",
    "    llama-index-embeddings-langchain \\\n",
    "    llama-index-llms-bedrock \\\n",
    "    llama-index-embeddings-bedrock \\\n",
    "    llama-index-llms-langchain \\\n",
    "    ragas \\\n",
    "    spacy \\\n",
    "    datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything installed, let's import the required libraries and do some initial setup. This will come in handy later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import os  # For dealing with folder paths\n",
    "import sys\n",
    "import textwrap\n",
    "from io import StringIO\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "\n",
    "# External Dependencies:\n",
    "import nest_asyncio  # Needed for some asyncio-based libs to work in Jupyter notebooks\n",
    "nest_asyncio.apply()  # Enable asyncio-based libs to work properly in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, **Mistral 7B Instruct** is our default model, but feel free to pick any other available Mistral model to experiment with this RAG pipeline. You just need to change the `DEFAULT_MODEL` variable. Also, you may want to change the AWS region as well. If so, just change the `AWS_REGION` variable below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_mistral7b_id=\"mistral.mistral-7b-instruct-v0:2\"\n",
    "instruct_mixtral8x7b_id=\"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "mistral_large_2402_id=\"mistral.mistral-large-2402-v1:0\"\n",
    "titan_embeddings_g1=\"amazon.titan-embed-text-v1\"\n",
    "titan_text_embeddings_v2=\"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "DEFAULT_MODEL=instruct_mistral7b_id\n",
    "DEFAULT_EMBEDDINGS=titan_text_embeddings_v2\n",
    "AWS_REGION=\"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download and pre-process documents with Titan Text Embeddings and LlamaIndex\n",
    "\n",
    "In this example, we'll create an in-memory semantic search index using:\n",
    "\n",
    "- [Amazon Titan Embeddings v2](https://aws.amazon.com/about-aws/whats-new/2024/04/amazon-titan-text-embeddings-v2-amazon-bedrock/) on Amazon Bedrock, as a model to convert text of documents and user queries into numerical \"embedding\" vectors.\n",
    "- LlamaIndex [VectorStoreIndex](https://docs.llamaindex.ai/en/stable/community/integrations/vector_stores/), to index the generated document vectors in-memory and retrieve the most similar documents for incoming queries/questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the sample document: Amazon's 2023 shareholder letter.\n",
    "\n",
    "In this example, we'll just use a single document for our RAG corpus: Amazon's 2023 annual letter to shareholders. Since the document itself is long, it'll be split into multiple separate entries in the search index.\n",
    "\n",
    "First, run the cell below to download the file locally. It'll also create the /data folder if it doesn't exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.82file/s]\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "URL_FILENAME_MAP = {\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2024/ar/Amazon-com-Inc-2023-Shareholder-Letter.pdf\": \"Amazon-com-Inc-2023-Shareholder-Letter.pdf\"\n",
    "}\n",
    "\n",
    "# Create the local folder if it doesn't exist\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "# Download files with progress bar\n",
    "for url, filename in tqdm(URL_FILENAME_MAP.items(), unit=\"file\"):\n",
    "    urlretrieve(url, os.path.join(DATA_ROOT, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can initially read the PDF files using LlamaIndex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "docs = SimpleDirectoryReader(input_files=[\"data/Amazon-com-Inc-2023-Shareholder-Letter.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and vectorize the documents\n",
    "\n",
    "Text vectorization models are machine learning models that convert text data into numerical vector representations. This process, known as vectorization, allows the text to be processed and analyzed using mathematical operations and algorithms. They typically place an upper limit on the length of text they can process as a single item. Additionally, we want each search result to be reasonably short for embedding results in the answer generation LLM prompt later.\n",
    "\n",
    "To address this, we need to **split** the source document into shorter passages for indexing. LlamaIndex's [TokenTextSplitter](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/#tokentextsplitter) offers a utility for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=512, chunk_overlap=102\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert each document chunk into a single vector, we'll use the **Amazon Titan Text Embeddings V2 model**. It's a lightweight, efficient model ideal for high accuracy retrieval tasks at different dimensions. The model supports flexible embeddings sizes (256, 512, 1,024) and prioritizes accuracy maintenance at smaller dimension sizes, helping to reduce storage costs without compromising on accuracy. When reducing from 1,024 to 512 dimensions, Titan Text Embeddings V2 retains approximately 99% retrieval accuracy, and when reducing from 1,024 to 256 dimensions, the model maintains 97% accuracy. Additionally, Titan Text Embeddings V2 includes multilingual support for 100+ languages in pre-training as well as unit vector normalization for improving accuracy of measuring vector similarity.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "embed_model = BedrockEmbedding(model=DEFAULT_EMBEDDINGS,\n",
    "                               region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After configuring the splitting and vectorization parameters, we can proceed to set up and execute LlamaIndex's [IngestionPipeline](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/) to load and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 28 chunks from 11 source docs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'page_label': '1',\n",
       " 'file_name': 'Amazon-com-Inc-2023-Shareholder-Letter.pdf',\n",
       " 'file_path': 'data/Amazon-com-Inc-2023-Shareholder-Letter.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 101160,\n",
       " 'creation_date': '2024-05-13',\n",
       " 'last_modified_date': '2024-05-13'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# Create an ingestion pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[text_splitter, embed_model])\n",
    "\n",
    "# save\n",
    "pipeline.persist(\"./pipeline_storage\")\n",
    "\n",
    "# Run the ingestion pipeline\n",
    "doc_nodes = pipeline.run(documents=docs)\n",
    "\n",
    "print(f\"Ingested {len(doc_nodes)} chunks from {len(docs)} source docs\")\n",
    "doc_nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating and Evaluating the LlamaIndex Query Engine\n",
    "\n",
    "After completing the chunking and vectorization processes, we can proceed to index the data into a queryable storage system.\n",
    "As the end-to-end querying process involves not only retrieving relevant documents but also generating textual answers from those documents, we need to define the configuration for Mistral at this stage. In this example, we will use **Mistral 7B Instruct**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 30\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "import boto3  # AWS SDK for Python\n",
    "boto3_bedrock = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "model_kwargs_mistral = {\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 200,\n",
    "    \"max_tokens\": 8192  # Max response length\n",
    "}\n",
    "\n",
    "# Initialize the Mistral model to formulate final answer from search results\n",
    "llm = Bedrock(\n",
    "    model=DEFAULT_MODEL,\n",
    "    streaming=True,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_kwargs_mistral,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "# Set LlamaIndex settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model \n",
    "Settings.chunk_size=512\n",
    "\n",
    "# Create a vector index from documents\n",
    "vector_index = VectorStoreIndex.from_documents(documents=docs, \n",
    "                                               doc_nodes=doc_nodes)\n",
    "print(\"Number of nodes:\", len(vector_index.docstore.docs))\n",
    "\n",
    "# Create a query engine\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=5,  # The top k=5 search results will be fed through to the LLM prompt\n",
    ")\n",
    "\n",
    "# store the created index to the local file system in case you need to re-load it into memory\n",
    "os.makedirs(\"./indices\", exist_ok=True)\n",
    "vector_index.storage_context.persist(\"./indices/amazon-shareholder-letters-2023-mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's execute some example questions against the vector index and the Mistral model. \n",
    "The query function takes the user's query as input and generates a response based on the relevant context and prompts. The answer should be present in the [source document](data/2023-Shareholder-Letter.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt viewing function\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}\" f\"**Text:** \"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template**Text:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template**Text:** "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Primitives are foundational building blocks that enable rapid innovation and experimentation in\n",
      "AWS. They are discrete, indivisible units that do one thing really well and are meant to be used\n",
      "together. By building primitives, AWS provides developers with maximum freedom and flexibility to\n",
      "create new applications and services. This approach is crucial for the overall AWS generative AI\n",
      "strategy because it allows for the development of a wide range of AI applications and services,\n",
      "democratizing this next seminal phase of AI and empowering both internal and external builders to\n",
      "transform customer experiences. Additionally, primitives enable AWS to respond quickly to real\n",
      "customer challenges and emerging technologies, ensuring that the company remains at the forefront of\n",
      "innovation.\n"
     ]
    }
   ],
   "source": [
    "# Defines the query or question that we want to ask the model.\n",
    "query=\"What is the importance of building primitives for innovation and experimentation in AWS? Why is this approach so important for the overall AWS generative AI strategy?\"\n",
    "\n",
    "# Retrieves the prompts that will be used to generate a response to the query.\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "\n",
    "# Displays the prompts that were generated for the given query.\n",
    "display_prompt_dict(prompts_dict)\n",
    "\n",
    "# Executes the query against the vector index and the Mistral model.\n",
    "# The query function takes the user's query as input and generates a response based on the relevant context and prompts.\n",
    "response = query_engine.query(query)\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some additional examples of in-context questions, that is, questions to which answers will be found in the source document loaded into this RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the context provided, several premium brands started listing on Amazon in 2023. Some of these\n",
      "brands include Coach, Victoria's Secret, Pit Viper, Martha Stewart, Clinique, Lancôme, and Urban\n",
      "Decay.\n",
      "---\n",
      " The context information mentions several countries as being part of Amazon's emerging geographies,\n",
      "specifically India, Brazil, Australia, Mexico, Middle East, Africa, and Thailand.\n",
      "---\n",
      " Based on the context, Amazon is building several GenAI applications for customer and seller service\n",
      "productivity. These applications include those for generating, customizing, and editing high-quality\n",
      "images, advertising copy, and videos, as well as customer and seller service productivity apps.\n",
      "Additionally, Amazon Q, an expert on AWS, is mentioned as a capable work assistant that can answer\n",
      "questions, summarize data, carry on coherent conversations, and take action. It is also mentioned\n",
      "that these AWS services will empower internal and external builders to transform virtually every\n",
      "customer experience.\n"
     ]
    }
   ],
   "source": [
    "# In-context questions:\n",
    "print_ww(query_engine.query(\"Which premium brands started listing on Amazon in 2023? (List at least 5 brands)\"))\n",
    "print(\"---\")\n",
    "print_ww(query_engine.query(\"Which countries does Amazon see meaningful progress in as emerging geographies?\"))\n",
    "print(\"---\")\n",
    "print_ww(query_engine.query(\"What are some of the GenAI applications that Amazon is building for customer and seller service productivity?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of questions non-related to the source document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Amazon's international expansion in the Netherlands is not explicitly mentioned in the provided\n",
      "context information. However, the text does mention that Amazon sees meaningful progress in their\n",
      "emerging geographies, including India, Brazil, Australia, Mexico, Middle East, Africa, and others.\n",
      "The company aims to reduce delivery times and better tailor the customer experience in these\n",
      "markets. While the Netherlands is not specifically named, it can be inferred that Amazon is\n",
      "expanding its reach and focusing on improving services in various international markets.\n",
      "---\n",
      " The context does not provide the name of the new Amazon large language model (LLM) mentioned in the\n",
      "text.\n",
      "---\n",
      " The context information provided does not mention the cash and investment balances at Amazon.com at\n",
      "the end of the year 1927. The information given pertains to the years 1996, 1997, and 2023, with the\n",
      "cash and investment balances being $125 million at the end of 1997.\n"
     ]
    }
   ],
   "source": [
    "# Out-of-context questions:\n",
    "print_ww(query_engine.query(\"Tell me more about Amazon's international expansion in the Netherlands\"))\n",
    "print(\"---\")\n",
    "print_ww(query_engine.query(\"What's the name of the new Amazon's LLM?\"))\n",
    "print(\"---\")\n",
    "print_ww(query_engine.query(\"What was the total cash and investment balances at Amazon.com at the end of 1927?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The revenue growth rate in 2023 was bigger than the one in 1997 in all segments: North America (12%\n",
      "YoY vs. 838% increase), International (11% YoY vs. 738% increase), and AWS (13% YoY vs. 838%\n",
      "increase). Therefore, the number of times the revenue growth rate in 2023 is bigger than the one in\n",
      "1997 depends on how you define \"bigger.\" If you mean \"higher percentage increase,\" then the answer\n",
      "is three times for North America and International, and once for AWS. If you mean \"absolute growth,\"\n",
      "then the answer is significantly more than three times for all segments.\n"
     ]
    }
   ],
   "source": [
    "# Cross-context question:\n",
    "print_ww(query_engine.query(\"How many times is the revenue growth rate in 2023 bigger than the one in 1997?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAG Automated Pipeline evaluation with LlamaIndex evaluators\n",
    "\n",
    "In the sections below, we'll show 4 automated evaluations available throught LlamaIndex. However, there are some additional metrics out-of-the-box that can be found [here](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/):\n",
    "\n",
    "1. **Faithfulness**: This metric verifies whether the final response is in agreement with (doesn't contradict) the retrieved document snippets.\n",
    "2. **Relevancy**: This metrics checks whether the response and retrieved content were relevant to the query.\n",
    "3. **Correctness**: This metric evaluates whether the generated answer is relevant and agreeing with a reference answer.\n",
    "4. **Semantic Similarity**: Evaluates the quality of a question answering system by comparing the similarity between embeddings of the generated answer and the reference answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Faithfulness to source documents\n",
    "\n",
    "The **Faithfulness** metric evaluates the coherence between the generated response and the source document snippets retrieved during the search process. This assessment is essential for identifying any discrepancies or hallucinations introduced by the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ----------------\n",
      "What technology transformation is Andy Jassy comparing the potential impact of Generative AI to?\n",
      "\n",
      "Answer: ----------------\n",
      " Andy Jassy is comparing the potential impact of Generative AI to that of the cloud technology\n",
      "transformation.\n",
      "\n",
      "----------------\n",
      "Evaluation Result: True\n",
      "Reasoning:\n",
      " YES\n",
      "The context mentions the development and use of foundation models (FMs) and generative AI (GenAI)\n",
      "applications, as well as the importance of having access to powerful compute resources and software\n",
      "tools for building and deploying these models. The information provided is consistent with the\n",
      "context.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "query=\"What technology transformation is Andy Jassy comparing the potential impact of Generative AI to?\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(\"Question: ----------------\")\n",
    "print_ww(query)\n",
    "print(\"\\nAnswer: ----------------\")\n",
    "print_ww(response)\n",
    "print(\"\\n----------------\")\n",
    "\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "eval_result = faithfulness_evaluator.evaluate_response(response=response)\n",
    "\n",
    "print_ww(\"Evaluation Result:\", eval_result.passing)\n",
    "print_ww(f\"Reasoning:\\n{eval_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Relevancy of response + source nodes to the query\n",
    "\n",
    "The **Relevancy** metric verifies the correspondence between the response and the retrieved source documents with the user's query. This evaluation is crucial for assessing whether the response properly addresses the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: True\n",
      "Reasoning:\n",
      " YES, the response is in line with the context information provided. Andy Jassy is comparing the\n",
      "potential impact of Generative AI to that of the cloud technology transformation in the context of\n",
      "the text.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RelevancyEvaluator\n",
    "\n",
    "relevancy_evaluator = RelevancyEvaluator(llm=llm)\n",
    "eval_result = relevancy_evaluator.evaluate_response(query=query, response=response)\n",
    "\n",
    "print_ww(\"Evaluation Result:\", eval_result.passing)\n",
    "print_ww(f\"Reasoning:\\n{eval_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Exploring differences between Relevancy and Faithfulness Evaluators\n",
    "\n",
    "To illustrate the contrast between **relevancy** and **faithfulness**, let's examine the following question which isn't in the source data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Amazon 'Bedrock Studio' will be launched?\n",
      " The context information provided does not mention a specific launch date for Amazon's Bedrock\n",
      "Studio. The text describes Bedrock as a service that is off to a strong start with tens of thousands\n",
      "of active customers and that Amazon continues to iterate on, adding new models and features. It also\n",
      "mentions that the majority of GenAI applications will ultimately be built by other companies using\n",
      "the primitives that Amazon is building in AWS.\n"
     ]
    }
   ],
   "source": [
    "# Out-of-context question:\n",
    "ooc_query = \"When Amazon 'Bedrock Studio' will be launched?\"\n",
    "\n",
    "ooc_response = query_engine.query(ooc_query)\n",
    "print_ww(ooc_query)\n",
    "print_ww(ooc_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_print_response(evaluator, ooc_response, ooc_query=None):\n",
    "    \"\"\"\n",
    "    Evaluates the relevancy or faithfulness of a response to a given query using\n",
    "    a provided evaluator, and prints the evaluation result, reasoning, and contexts.\n",
    "\n",
    "    Args:\n",
    "        evaluator (Union[RelevancyEvaluator, FaithfulnessEvaluator]): The evaluator\n",
    "            to use for evaluation.\n",
    "        ooc_response (str): The response to evaluate.\n",
    "        ooc_query (str, optional): The original out-of-context query. If not provided,\n",
    "            the response will be evaluated without a specific query context.\n",
    "\n",
    "    Returns:\n",
    "        Union[RelevancyEvaluationResult, FaithfulnessEvaluationResult]: The result\n",
    "            of the evaluation.\n",
    "    \"\"\"\n",
    "    evaluation_result = evaluator.evaluate_response(query=ooc_query, response=ooc_response)\n",
    "\n",
    "    print_ww(\"Evaluation Result:\", evaluation_result.passing)\n",
    "    print_ww(f\"Reasoning:\\n{evaluation_result.feedback}\")\n",
    "    #print_ww(\"Contexts:\\n\", evaluation_result.contexts)\n",
    "    #print_ww(\"Source:\\n\", ooc_response.source_nodes)\n",
    "\n",
    "    return evaluation_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Relevancy Evaluator** module is useful to measure if the response + source nodes match the query. Therefore, it helps measuring if the query was actually answered by the response. In this example, as the context information does not provide any details about the launch date of Amazon Bedrock Studio, then the evaluation result is **FALSE**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: False\n",
      "Reasoning:\n",
      " NO. The context information provided does not mention any launch date for Amazon Bedrock Studio.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(query=\"When Amazon 'Bedrock Studio' will be launched?\", contexts=['service. Amazon Bedrock invented this layer and provides customers with the easiest way to build and scale\\nGenAI applications with the broadest selection of first- and third-party FMs, as well as leading ease-of-usecapabilities that allow GenAI builders to get higher quality model outputs more quickly. Bedrock is off to avery strong start with tens of thousands of active customers after just a few months. The team continuesto iterate rapidly on Bedrock, recently delivering Guardrails (to safeguard what questions applications will\\nanswer), Knowledge Bases (to expand models’ knowledge base with Retrieval Augmented Generation—or\\nRAG—and real-time queries), Agents (to complete multi-step tasks), and Fine-Tuning (to keep teaching\\nand refining models), all of which improve customers’ application quality. We also just added new modelsfrom Anthropic (their newly-released Claude 3 is the best performing large language model in the world),Meta (with Llama 2), Mistral, Stability AI, Cohere, and our own Amazon Titan family of FMs. Whatcustomers have learned at this early stage of GenAI is that there’s meaningful iteration required to build aproduction GenAI application with the requisite enterprise quality at the cost and latency needed. Customersdon’t want only one model. They want access to various models and model sizes for different types of\\napplications. Customers want a service that makes this experimenting and iterating simple, and this is whatBedrock does, which is why customers are so excited about it. Customers using Bedrock already include ADP,Amdocs, Bridgewater Associates, Broadridge, Clariant, Dana-Farber Cancer Institute, Delta Air Lines,Druva, Genesys, Genomics England, GoDaddy, Intuit, KT, Lonely Planet, LexisNexis, Netsmart, PerplexityAI, Pfizer, PGA TOUR, Ricoh, Rocket Companies, and Siemens.\\nThetoplayer of this stack is the application layer. We’re building a substantial number of GenAI applications\\nacross every Amazon consumer business.', 'Customersdon’t want only one model. They want access to various models and model sizes for different types of\\napplications. Customers want a service that makes this experimenting and iterating simple, and this is whatBedrock does, which is why customers are so excited about it. Customers using Bedrock already include ADP,Amdocs, Bridgewater Associates, Broadridge, Clariant, Dana-Farber Cancer Institute, Delta Air Lines,Druva, Genesys, Genomics England, GoDaddy, Intuit, KT, Lonely Planet, LexisNexis, Netsmart, PerplexityAI, Pfizer, PGA TOUR, Ricoh, Rocket Companies, and Siemens.\\nThetoplayer of this stack is the application layer. We’re building a substantial number of GenAI applications\\nacross every Amazon consumer business. These range from Rufus (our new, AI-powered shopping assistant),to an even more intelligent and capable Alexa, to advertising capabilities (making it simple with natural\\nlanguage prompts to generate, customize, and edit high-quality images, advertising copy, and videos), tocustomer and seller service productivity apps, to dozens of others. We’re also building several apps in AWS,including arguably the most compelling early GenAI use case—a coding companion. We recently launchedAmazon Q, an expert on AWS that writes, debugs, tests, and implements code, while also doingtransformations (like moving from an old version of Java to a new one), and querying customers’ variousdata repositories (e.g. Intranets, wikis, Salesforce, Amazon S3, ServiceNow, Slack, Atlassian, etc.) to answerquestions, summarize data, carry on coherent conversation, and take action. Q is the most capable workassistant available today and evolving fast.\\nWhile we’re building a substantial number of GenAI applications ourselves, the vast majority will ultimately\\nbe built by other companies. However, what we’re building in AWS is not just a compelling app or foundationmodel. These AWS services, at all three layers of the stack, comprise a set of primitives that democratize thisnext seminal phase of AI, and will empower internal and external builders to transform virtually everycustomer experience that we know (and invent altogether new ones as well). We’re optimistic that much ofthis world-changing AI will be built on top of AWS.', 'transform the customer health experience: Acute Care (via Amazon Clinic), Primary Care (via One\\nMedical), and a Pharmacy service to buy whatever medication a patient may need. Because of our growingsuccess, Amazon customers are now asking us to help them with all kinds of wellness and nutritionopportunities—which can be partially unlocked with some of our existing grocery building blocks, includingWhole Foods Market or Amazon Fresh.\\nAs a builder, it’s hard to wait for these building blocks to be built versus just combining a bunch of\\ncomponents together to solve a specific problem. The latter can be faster, but almost always slows you downin the future. We’ve seen this temptation in our robotics efforts in our fulfillment network. There aredozens of processes we seek to automate to improve safety, productivity, and cost. Some of the biggestopportunities require invention in domains such as storage automation, manipulation, sortation, mobilityof large cages across long distances, and automatic identification of items. Many teams would skip right to thecomplex solution, baking in “just enough” of these disciplines to make a concerted solution work, butwhich doesn’t solve much more, can’t easily be evolved as new requirements emerge, and that can’t be reusedfor other initiatives needing many of the same components. However, when you think in primitives, likeour Robotics team does, you prioritize the building blocks, picking important initiatives that can benefit fromeach of these primitives, but which build the tool chest to compose more freely (and quickly) for futureand complex needs. Our Robotics team has built primitives in each of the above domains that will belynchpins in our next set of automation, which includes multi-floor storage, trailer loading and unloading,large pallet mobility, and more flexible sortation across our outbound processes (including in vehicles). Theteam is also building a set of foundation AI models to better identify products in complex environments,optimize the movement of our growing robotic fleet, and better manage the bottlenecks in our facilities.\\nSometimes, people ask us “what’s your next pillar? Y ou have Marketplace, Prime, and AWS, what’s next?”\\nThis, of course, is a thought-provoking question.', 'Thebottom layer is for developers and companies wanting to build foundation models (“FMs”). The\\nprimary primitives are the compute required to train models and generate inferences (or predictions), andthe software that makes it easier to build these models. Starting with compute, the key is the chip inside it. Todate, virtually all the leading FMs have been trained on Nvidia chips, and we continue to offer the broadestcollection of Nvidia instances of any provider. That said, supply has been scarce and cost remains an issue ascustomers scale their models and applications. Customers have asked us to push the envelope onprice-performance for AI chips, just as we have with Graviton for generalized CPU chips. As a result, we’vebuilt custom AI training chips (named Trainium) and inference chips (named Inferentia). In 2023, weannounced second versions of our Trainium and Inferentia chips, which are both meaningfully moreprice-performant than their first versions and other alternatives. This past fall, leading FM-maker, Anthropic,announced it would use Trainium and Inferentia to build, train, and deploy its future FMs. We alreadyhave several customers using our AI chips, including Anthropic, Airbnb, Hugging Face, Qualtrics, Ricoh,and Snap.\\nCustomers building their own FM must tackle several challenges in getting a model into production.\\nGetting data organized and fine-tuned, building scalable and efficient training infrastructure, and thendeploying models at scale in a low latency, cost-efficient manner is hard. It’s why we’ve built AmazonSageMaker, a managed, end-to-end service that’s been a game changer for developers in preparing their datafor AI, managing experiments, training models faster (e.g. Perplexity AI trains models 40% faster inSageMaker), lowering inference latency (e.g. Workday has reduced inference latency by 80% with SageMaker),and improving developer productivity (e.g. NatWest reduced its time-to-value for AI from 12-18 months tounder seven months using SageMaker).\\nThemiddle layer is for customers seeking to leverage an existing FM, customize it with their own data, and\\nleverage a leading cloud provider’s security and features to build a GenAI application—all as a managed', '(More on how we’re approaching GenAI and why webelieve we’ll be successful later in the letter.)\\nWe’re also making progress on many of our newer business investments that have the potential to be\\nimportant to customers and Amazon long-term. Touching on two of them:\\nWe have increasing conviction that Prime Video can be a large and profitable business on its own. This\\nconfidence is buoyed by the continued development of compelling, exclusive content (e.g. Thursday Night\\nFootball, Lord of the Rings, Reacher, The Boys, Citadel, Road House , etc.), Prime Video customers’ engagement\\nwith this content, growth in our marketplace programs (through our third-party Channels program, aswell as the broad selection of shows and movies customers rent or buy), and the addition of advertising inPrime Video.\\nIn October, we hit a major milestone in our journey to commercialize Project Kuiper when we launched two\\nend-to-end prototype satellites into space, and successfully validated all key systems and sub-systems—\\nrare in an initial launch like this. Kuiper is our low Earth orbit satellite initiative that aims to providebroadband connectivity to the 400-500 million households who don’t have it today (as well as governmentsand enterprises seeking better connectivity and performance in more remote areas), and is a very large revenueopportunity for Amazon. We’re on track to launch our first production satellites in 2024. We’ve still got along way to go, but are encouraged by our progress.\\nOverall, 2023 was a strong year, and I’m grateful to our collective teams who delivered on behalf of\\ncustomers. These results represent a lot of invention, collaboration, discipline, execution, and reimagination'], response=\" The context information provided does not mention a specific launch date for Amazon's Bedrock Studio. The text describes Bedrock as a service that is off to a strong start with tens of thousands of active customers and that Amazon continues to iterate on, adding new models and features. It also mentions that the majority of GenAI applications will ultimately be built by other companies using the primitives that Amazon is building in AWS.\", passing=False, feedback=' NO. The context information provided does not mention any launch date for Amazon Bedrock Studio.', score=0.0, pairwise_source=None, invalid_result=False, invalid_reason=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate relevance of result to the original question:\n",
    "\n",
    "evaluate_and_print_response(RelevancyEvaluator(llm=llm),\n",
    "                            ooc_query=ooc_query, \n",
    "                            ooc_response=ooc_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Faithfulness Evaluator** module is helpful to measure if the response from a query engine matches any source nodes. The context does not provide any information regarding the launch date of Amazon Bedrock Studio. Therefore, it can't answer that question.\n",
    "\n",
    "This helps to measure if the response has been **HALLUCINATED**, which hasn't happened in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result: False\n",
      "Reasoning:\n",
      " NO. The context does not mention any specific launch date for Amazon's Bedrock Studio.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(query=None, contexts=['service. Amazon Bedrock invented this layer and provides customers with the easiest way to build and scale\\nGenAI applications with the broadest selection of first- and third-party FMs, as well as leading ease-of-usecapabilities that allow GenAI builders to get higher quality model outputs more quickly. Bedrock is off to avery strong start with tens of thousands of active customers after just a few months. The team continuesto iterate rapidly on Bedrock, recently delivering Guardrails (to safeguard what questions applications will\\nanswer), Knowledge Bases (to expand models’ knowledge base with Retrieval Augmented Generation—or\\nRAG—and real-time queries), Agents (to complete multi-step tasks), and Fine-Tuning (to keep teaching\\nand refining models), all of which improve customers’ application quality. We also just added new modelsfrom Anthropic (their newly-released Claude 3 is the best performing large language model in the world),Meta (with Llama 2), Mistral, Stability AI, Cohere, and our own Amazon Titan family of FMs. Whatcustomers have learned at this early stage of GenAI is that there’s meaningful iteration required to build aproduction GenAI application with the requisite enterprise quality at the cost and latency needed. Customersdon’t want only one model. They want access to various models and model sizes for different types of\\napplications. Customers want a service that makes this experimenting and iterating simple, and this is whatBedrock does, which is why customers are so excited about it. Customers using Bedrock already include ADP,Amdocs, Bridgewater Associates, Broadridge, Clariant, Dana-Farber Cancer Institute, Delta Air Lines,Druva, Genesys, Genomics England, GoDaddy, Intuit, KT, Lonely Planet, LexisNexis, Netsmart, PerplexityAI, Pfizer, PGA TOUR, Ricoh, Rocket Companies, and Siemens.\\nThetoplayer of this stack is the application layer. We’re building a substantial number of GenAI applications\\nacross every Amazon consumer business.', 'Customersdon’t want only one model. They want access to various models and model sizes for different types of\\napplications. Customers want a service that makes this experimenting and iterating simple, and this is whatBedrock does, which is why customers are so excited about it. Customers using Bedrock already include ADP,Amdocs, Bridgewater Associates, Broadridge, Clariant, Dana-Farber Cancer Institute, Delta Air Lines,Druva, Genesys, Genomics England, GoDaddy, Intuit, KT, Lonely Planet, LexisNexis, Netsmart, PerplexityAI, Pfizer, PGA TOUR, Ricoh, Rocket Companies, and Siemens.\\nThetoplayer of this stack is the application layer. We’re building a substantial number of GenAI applications\\nacross every Amazon consumer business. These range from Rufus (our new, AI-powered shopping assistant),to an even more intelligent and capable Alexa, to advertising capabilities (making it simple with natural\\nlanguage prompts to generate, customize, and edit high-quality images, advertising copy, and videos), tocustomer and seller service productivity apps, to dozens of others. We’re also building several apps in AWS,including arguably the most compelling early GenAI use case—a coding companion. We recently launchedAmazon Q, an expert on AWS that writes, debugs, tests, and implements code, while also doingtransformations (like moving from an old version of Java to a new one), and querying customers’ variousdata repositories (e.g. Intranets, wikis, Salesforce, Amazon S3, ServiceNow, Slack, Atlassian, etc.) to answerquestions, summarize data, carry on coherent conversation, and take action. Q is the most capable workassistant available today and evolving fast.\\nWhile we’re building a substantial number of GenAI applications ourselves, the vast majority will ultimately\\nbe built by other companies. However, what we’re building in AWS is not just a compelling app or foundationmodel. These AWS services, at all three layers of the stack, comprise a set of primitives that democratize thisnext seminal phase of AI, and will empower internal and external builders to transform virtually everycustomer experience that we know (and invent altogether new ones as well). We’re optimistic that much ofthis world-changing AI will be built on top of AWS.', 'transform the customer health experience: Acute Care (via Amazon Clinic), Primary Care (via One\\nMedical), and a Pharmacy service to buy whatever medication a patient may need. Because of our growingsuccess, Amazon customers are now asking us to help them with all kinds of wellness and nutritionopportunities—which can be partially unlocked with some of our existing grocery building blocks, includingWhole Foods Market or Amazon Fresh.\\nAs a builder, it’s hard to wait for these building blocks to be built versus just combining a bunch of\\ncomponents together to solve a specific problem. The latter can be faster, but almost always slows you downin the future. We’ve seen this temptation in our robotics efforts in our fulfillment network. There aredozens of processes we seek to automate to improve safety, productivity, and cost. Some of the biggestopportunities require invention in domains such as storage automation, manipulation, sortation, mobilityof large cages across long distances, and automatic identification of items. Many teams would skip right to thecomplex solution, baking in “just enough” of these disciplines to make a concerted solution work, butwhich doesn’t solve much more, can’t easily be evolved as new requirements emerge, and that can’t be reusedfor other initiatives needing many of the same components. However, when you think in primitives, likeour Robotics team does, you prioritize the building blocks, picking important initiatives that can benefit fromeach of these primitives, but which build the tool chest to compose more freely (and quickly) for futureand complex needs. Our Robotics team has built primitives in each of the above domains that will belynchpins in our next set of automation, which includes multi-floor storage, trailer loading and unloading,large pallet mobility, and more flexible sortation across our outbound processes (including in vehicles). Theteam is also building a set of foundation AI models to better identify products in complex environments,optimize the movement of our growing robotic fleet, and better manage the bottlenecks in our facilities.\\nSometimes, people ask us “what’s your next pillar? Y ou have Marketplace, Prime, and AWS, what’s next?”\\nThis, of course, is a thought-provoking question.', 'Thebottom layer is for developers and companies wanting to build foundation models (“FMs”). The\\nprimary primitives are the compute required to train models and generate inferences (or predictions), andthe software that makes it easier to build these models. Starting with compute, the key is the chip inside it. Todate, virtually all the leading FMs have been trained on Nvidia chips, and we continue to offer the broadestcollection of Nvidia instances of any provider. That said, supply has been scarce and cost remains an issue ascustomers scale their models and applications. Customers have asked us to push the envelope onprice-performance for AI chips, just as we have with Graviton for generalized CPU chips. As a result, we’vebuilt custom AI training chips (named Trainium) and inference chips (named Inferentia). In 2023, weannounced second versions of our Trainium and Inferentia chips, which are both meaningfully moreprice-performant than their first versions and other alternatives. This past fall, leading FM-maker, Anthropic,announced it would use Trainium and Inferentia to build, train, and deploy its future FMs. We alreadyhave several customers using our AI chips, including Anthropic, Airbnb, Hugging Face, Qualtrics, Ricoh,and Snap.\\nCustomers building their own FM must tackle several challenges in getting a model into production.\\nGetting data organized and fine-tuned, building scalable and efficient training infrastructure, and thendeploying models at scale in a low latency, cost-efficient manner is hard. It’s why we’ve built AmazonSageMaker, a managed, end-to-end service that’s been a game changer for developers in preparing their datafor AI, managing experiments, training models faster (e.g. Perplexity AI trains models 40% faster inSageMaker), lowering inference latency (e.g. Workday has reduced inference latency by 80% with SageMaker),and improving developer productivity (e.g. NatWest reduced its time-to-value for AI from 12-18 months tounder seven months using SageMaker).\\nThemiddle layer is for customers seeking to leverage an existing FM, customize it with their own data, and\\nleverage a leading cloud provider’s security and features to build a GenAI application—all as a managed', '(More on how we’re approaching GenAI and why webelieve we’ll be successful later in the letter.)\\nWe’re also making progress on many of our newer business investments that have the potential to be\\nimportant to customers and Amazon long-term. Touching on two of them:\\nWe have increasing conviction that Prime Video can be a large and profitable business on its own. This\\nconfidence is buoyed by the continued development of compelling, exclusive content (e.g. Thursday Night\\nFootball, Lord of the Rings, Reacher, The Boys, Citadel, Road House , etc.), Prime Video customers’ engagement\\nwith this content, growth in our marketplace programs (through our third-party Channels program, aswell as the broad selection of shows and movies customers rent or buy), and the addition of advertising inPrime Video.\\nIn October, we hit a major milestone in our journey to commercialize Project Kuiper when we launched two\\nend-to-end prototype satellites into space, and successfully validated all key systems and sub-systems—\\nrare in an initial launch like this. Kuiper is our low Earth orbit satellite initiative that aims to providebroadband connectivity to the 400-500 million households who don’t have it today (as well as governmentsand enterprises seeking better connectivity and performance in more remote areas), and is a very large revenueopportunity for Amazon. We’re on track to launch our first production satellites in 2024. We’ve still got along way to go, but are encouraged by our progress.\\nOverall, 2023 was a strong year, and I’m grateful to our collective teams who delivered on behalf of\\ncustomers. These results represent a lot of invention, collaboration, discipline, execution, and reimagination'], response=\" The context information provided does not mention a specific launch date for Amazon's Bedrock Studio. The text describes Bedrock as a service that is off to a strong start with tens of thousands of active customers and that Amazon continues to iterate on, adding new models and features. It also mentions that the majority of GenAI applications will ultimately be built by other companies using the primitives that Amazon is building in AWS.\", passing=False, feedback=\" NO. The context does not mention any specific launch date for Amazon's Bedrock Studio.\", score=0.0, pairwise_source=None, invalid_result=False, invalid_reason=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate faithfulness of response to retrieved content:\n",
    "\n",
    "evaluate_and_print_response(FaithfulnessEvaluator(llm=llm), \n",
    "                            ooc_response=ooc_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Automating Q&A Generation with LllamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LllamaInex provides tools designed to automatically generate datasets when provided with a set of documents to query. In the example below, we use the **DatasetGenerator**, which is a class that generates evaluation questions from the source documents and the specified number of questions per node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Synthetic Test Set with LlamaIndex DatasetGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⏰ **Note:** The code block below is used to generate a dataset of questions from the source  document and may take some time to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import DatasetGenerator\n",
    "\n",
    "# define generator and generate questions\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    documents=docs,\n",
    "    llm=llm,\n",
    "    num_questions_per_chunk=10,  # set the number of questions per nodes\n",
    ")\n",
    "\n",
    "eval_questions = dataset_generator.generate_questions_from_nodes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions created: 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What was the total revenue growth rate of Amazon in 2023 compared to the previous year?',\n",
       " \"Which segment of Amazon's business had the highest revenue growth rate in 2023?\",\n",
       " \"What was Amazon's operating income in 2023, and how did it compare to the previous year in terms of percentage?\",\n",
       " \"What was Amazon's Free Cash Flow (FCF) in 2023, and how did it change from the previous year?\",\n",
       " 'Which premium brands started selling on Amazon in 2023?',\n",
       " 'Why did Amazon focus on price during the holiday season in 2023?',\n",
       " 'What was the name of the exclusive event for Prime members during the holiday season in 2023?',\n",
       " 'What was the name of the holiday shopping event open to all customers in 2023?',\n",
       " \"How did the number of products available in Amazon's Stores business change from the previous year?\",\n",
       " \"What was the significance of the revenue growth in Amazon's AWS segment in 2023?\",\n",
       " \"What two factors contributed to Amazon's record-breaking revenue event in Q4 2023?\",\n",
       " 'How many premium brands started selling on Amazon in 2023?',\n",
       " 'In which region did Amazon deliver the most items at the fastest speeds to Prime members in 2023?',\n",
       " 'What percentage increase was seen in customer savings in 2023 compared to the prior year?',\n",
       " 'How many billions of items were delivered same or next day in the U.S. in 2023?',\n",
       " \"What was the result of Amazon's regionalization efforts in terms of transportation distances?\",\n",
       " 'In which continent did Amazon deliver over 2 billion items to customers in 2023?',\n",
       " \"What was the growth rate of Amazon's everyday essentials business in Q4 2023?\",\n",
       " 'How much was the cost to serve reduced per unit in the U.S. in 2023 compared to the prior year?',\n",
       " 'What year was it since 2018 that Amazon globally reduced its cost to serve on a per unit basis?',\n",
       " 'In what year did Amazon first reduce its cost to serve on a per unit basis globally since 2018?',\n",
       " 'What percentage increase was there in the number of items delivered same day or overnight in 2023 compared to the previous year?',\n",
       " 'How did the cost to serve decrease in the US alone in 2023 compared to the previous year?',\n",
       " \"What is the growth rate of Amazon's everyday essentials business in Q4 2023?\",\n",
       " 'In which areas does Amazon believe it can lower costs further while also delivering faster for customers in 2024?',\n",
       " 'Which countries does Amazon see meaningful progress in as emerging geographies?',\n",
       " 'What was the global cost to serve per unit in 2023?',\n",
       " 'How has the cost to serve changed in the US since 2018?',\n",
       " \"What percentage growth rate was there in Amazon's everyday essentials business in 2023?\",\n",
       " 'What was the reason given for the decrease in cost to serve in 2023?']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of questions created: {len(eval_questions)}\")\n",
    "eval_questions[:30] # display the first 30 created evaluation questions only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can run the evaluation on the dataset and visualize the results in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Response\n",
    "import pandas as pd\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(query: str, response: Response, eval_result: str) -> None:\n",
    "\n",
    "    eval_df = pd.DataFrame(columns=['Query', 'Response', 'Source', 'Evaluation Result'])\n",
    "        \n",
    "    new_record = {\n",
    "                    \"Query\": query,\n",
    "                    \"Response\": str(response),\n",
    "                    \"Source\": (\n",
    "                        response.source_nodes[0].node.get_content()[:100] + \"...\"\n",
    "                    ),\n",
    "                    \"Evaluation Result\": eval_result,\n",
    "                }\n",
    "    eval_df = eval_df._append(new_record, ignore_index=True)\n",
    "\n",
    "\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the first generated evaluation question with the **RelevancyEvaluator** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- Question ---------\n",
      "What was the total revenue growth rate of Amazon in 2023 compared to the previous year?\n",
      "\n",
      "--------- Response ---------\n",
      " The total revenue of Amazon grew by 12% year-over-year in 2023.\n",
      "\n",
      "--------- Passed ---------\n",
      "True\n",
      "\n",
      "--------- Feedback ---------\n",
      " YES, the response is in line with the context information provided as the context mentions that\n",
      "Amazon's total revenue grew 12% year-over-year in 2023.\n",
      "\n",
      "--------- Source ---------\n",
      "on-premises. These businesses will keep shifting online and into the cloud. In Media and\n",
      "Advertising,\n",
      "content will continue to migrate from linear formats to streaming. Globally, hundreds of millions of\n",
      "peoplewho don’t have adequate broadband access will gain that connectivity in the next few years.\n",
      "Last butcertainly not least, Generative AI may be the largest technology transformation since the\n",
      "cloud (whichitself, is still in the early stages), and perhaps since the Internet. Unlike the mass\n",
      "modernization of on-premises infrastructure to the cloud, where there’s work required to migrate,\n",
      "this GenAI revolution will bebuilt from the start on top of the cloud. The amount of societal and\n",
      "business benefit from the solutions thatwill be possible will astound us all.\n",
      "There has never been a time in Amazon’s history where we’ve felt there is so much opportunity to\n",
      "make our\n",
      "customers’ lives better and easier. We’re incredibly excited about what’s possible, focused on\n",
      "inventing thefuture, and look forward to working together to make it so.\n",
      "Sincerely,\n",
      "Andy Jassy\n",
      "President and Chief Executive OfficerAmazon.com, Inc.\n",
      "P .S. As we have always done, our original 1997 Shareholder Letter follows. What’s written there is\n",
      "as true\n",
      "today as it was in 1997.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b8b52_row0_col1, #T_b8b52_row0_col2 {\n",
       "  inline-size: 600px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b8b52\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b8b52_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
       "      <th id=\"T_b8b52_level0_col1\" class=\"col_heading level0 col1\" >Response</th>\n",
       "      <th id=\"T_b8b52_level0_col2\" class=\"col_heading level0 col2\" >Source</th>\n",
       "      <th id=\"T_b8b52_level0_col3\" class=\"col_heading level0 col3\" >Evaluation Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b8b52_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b8b52_row0_col0\" class=\"data row0 col0\" >What was the total revenue growth rate of Amazon in 2023 compared to the previous year?</td>\n",
       "      <td id=\"T_b8b52_row0_col1\" class=\"data row0 col1\" > The total revenue of Amazon grew by 12% year-over-year in 2023.</td>\n",
       "      <td id=\"T_b8b52_row0_col2\" class=\"data row0 col2\" >Dear Shareholders:\n",
       "Last year at this time, I shared my enthusiasm and optimism for Amazon’s future. ...</td>\n",
       "      <td id=\"T_b8b52_row0_col3\" class=\"data row0 col3\" >True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1393b9cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator_mistral = RelevancyEvaluator(llm=llm)\n",
    "eval_question=eval_questions[0]\n",
    "response_vector = query_engine.query(eval_question)\n",
    "eval_result = evaluator_mistral.evaluate_response(\n",
    "    query=eval_question, response=response_vector\n",
    ")\n",
    "\n",
    "# print results\n",
    "print(\"\\n--------- Question ---------\")\n",
    "print_ww(eval_question)\n",
    "print(\"\\n--------- Response ---------\")\n",
    "print_ww(str(response_vector))\n",
    "print(\"\\n--------- Passed ---------\")\n",
    "print_ww(str(eval_result.passing))\n",
    "print(\"\\n--------- Feedback ---------\")\n",
    "print_ww(str(eval_result.feedback))\n",
    "print(\"\\n--------- Source ---------\")\n",
    "print_ww(response.source_nodes[0].node.get_content())\n",
    "\n",
    "# Show a DataFrame\n",
    "display_eval_df(eval_questions[0], response_vector, eval_result.passing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Run LlamaIndex Evaluations for Faithfulness, Relevancy, and Correctness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Correctness** metric checks the correctness of a question answering system, relying on a provided reference answer(\"ground truth\"), query, and response. It assigns a score from 1 to 5 (with higher values indicating better quality) alongside an explanation for the rating. Conversely, both the Relevancy and Faithfulness evaluators return a score between 0 and 1, with higher values indicating better results.\n",
    "\n",
    "That said, we'll be initializing the Mistral Large model to create reference answers(ground truth) for the Test Dataset created previously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Mistral Large model to create the ground truth data required by the Correctness metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 30\n",
      "CPU times: user 346 ms, sys: 18 ms, total: 364 ms\n",
      "Wall time: 9.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "llm_ground_truth = Bedrock(model=mistral_large_2402_id,\n",
    "                           streaming=True,\n",
    "                           client=boto3_bedrock,\n",
    "                           model_kwargs=model_kwargs_mistral,\n",
    "                           region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "# Set LlamaIndex settings\n",
    "Settings.llm = llm_ground_truth\n",
    "\n",
    "# Create a vector index from documents\n",
    "vector_ground_truth_index = VectorStoreIndex.from_documents(documents=docs, doc_nodes=doc_nodes)\n",
    "print(\"Number of nodes:\", len(vector_index.docstore.docs))\n",
    "\n",
    "# Create a query engine\n",
    "query_engine_ground_truth = vector_ground_truth_index.as_query_engine(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def generate_ground_truth(evaluation_questions: List[str], query_engine):\n",
    "    ground_truth_answers = []\n",
    "    for question in evaluation_questions:\n",
    "        response = query_engine.query(question)\n",
    "        ground_truth_answer = str(response)\n",
    "        ground_truth_answers.append(ground_truth_answer)\n",
    "    return ground_truth_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 136 ms, sys: 6.59 ms, total: 143 ms\n",
      "Wall time: 10.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' The total revenue of Amazon grew by 12% year-over-year in 2023, increasing from $514 billion to $575 billion.',\n",
       " ' AWS revenue increased 13% Y o Y from $80B to $91B, which is the highest revenue growth rate among the provided segments in 2023.',\n",
       " \" Amazon's operating income in 2023 was $36.9 billion, representing a 201% increase compared to the previous year. The operating margin also improved from 2.4% to 6.4%.\",\n",
       " \" Amazon's Free Cash Flow (FCF) in 2023, adjusted for equipment finance leases, was $35.5 billion. This represented an improvement from the previous year, with an increase of $48.3 billion. In 2022, the FCF was negative at -$12.8 billion.\",\n",
       " ' Several premium brands started listing on Amazon in 2023, including Coach, Victoria’s Secret, Pit Viper, Martha Stewart, Clinique, Lancôme, and Urban Decay.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Run LlamaIndex evaluations for the first five questions\n",
    "# You may want to increase :5 to a different number to add more questions to be evaluated\n",
    "evaluation_questions = eval_questions[:5]\n",
    "\n",
    "# Generate ground truth by calling Mistral large\n",
    "evaluation_ground_truth = generate_ground_truth(evaluation_questions, \n",
    "                                                query_engine_ground_truth)\n",
    "\n",
    "\n",
    "Settings.llm = llm # redefine the llm to the original one after ground truth creation\n",
    "\n",
    "evaluation_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import CorrectnessEvaluator\n",
    "\n",
    "def run_evaluations(evaluation_questions: List[str], evaluation_ground_truth: List[str], query_engine, language_model):\n",
    "    \"\"\"Run a batch evaluation on a list of questions using a provided query engine.\n",
    "\n",
    "    Args:\n",
    "        evaluation_questions (List[str]): A list of questions to evaluate.\n",
    "        evaluation_ground_truth (List[str]): A list of ground truth answers for the questions.\n",
    "        query_engine (BaseQueryEngine): The query engine to use for answering the questions.\n",
    "        llm (LLM): The language model to use for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the evaluation results, including the query,\n",
    "            generated answer, faithfulness evaluation, and relevancy evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    results_list = []\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator(llm=language_model)\n",
    "    relevancy_evaluator = RelevancyEvaluator(llm=language_model)\n",
    "    correctness_evaluator = CorrectnessEvaluator(llm=language_model)\n",
    "\n",
    "    for question, ground_truth in zip(evaluation_questions, evaluation_ground_truth):\n",
    "        response = query_engine.query(question)\n",
    "        generated_answer = str(response)\n",
    "\n",
    "        faithfulness_results = faithfulness_evaluator.evaluate_response(response=response)\n",
    "        relevancy_results = relevancy_evaluator.evaluate_response(query=question, response=response)\n",
    "        correctness_results = correctness_evaluator.evaluate(\n",
    "            query=question,\n",
    "            response=generated_answer,\n",
    "            reference=ground_truth\n",
    "        )\n",
    "\n",
    "        current_evaluation = {\n",
    "            \"query\": question,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"faithfulness\": faithfulness_results.passing,\n",
    "            \"faithfulness_feedback\": faithfulness_results.feedback,\n",
    "            \"faithfulness_score\": faithfulness_results.score,\n",
    "            \"relevancy\": relevancy_results.passing,\n",
    "            \"relevancy_feedback\": relevancy_results.feedback,\n",
    "            \"relevancy_score\": relevancy_results.score,\n",
    "            \"correctness\": correctness_results.passing,\n",
    "            \"correctness_feedback\": correctness_results.feedback,\n",
    "            \"correctness_score\": correctness_results.score,\n",
    "        }\n",
    "        results_list.append(current_evaluation)\n",
    "\n",
    "    evaluations_df = pd.DataFrame(results_list)\n",
    "    return evaluations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 431 ms, sys: 20.6 ms, total: 451 ms\n",
      "Wall time: 21.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>faithfulness_feedback</th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>relevancy_feedback</th>\n",
       "      <th>relevancy_score</th>\n",
       "      <th>correctness</th>\n",
       "      <th>correctness_feedback</th>\n",
       "      <th>correctness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the total revenue growth rate of Amaz...</td>\n",
       "      <td>The total revenue of Amazon grew by 12% year-...</td>\n",
       "      <td>The total revenue of Amazon grew by 12% year-...</td>\n",
       "      <td>True</td>\n",
       "      <td>YES\\n(The context mentions that Apple pie is ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>YES, the response is in line with the context...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated answer is identical to the refer...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which segment of Amazon's business had the hig...</td>\n",
       "      <td>The segment of Amazon's business with the hig...</td>\n",
       "      <td>AWS revenue increased 13% Y o Y from $80B to ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NO, the context does not provide any informat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>YES, the response is in line with the context...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated answer is identical to the refer...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What was Amazon's operating income in 2023, an...</td>\n",
       "      <td>In 2023, Amazon's operating income was $36.9 ...</td>\n",
       "      <td>Amazon's operating income in 2023 was $36.9 b...</td>\n",
       "      <td>True</td>\n",
       "      <td>The context supports the information that Ama...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>YES, the response is in line with the context...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated answer is fully relevant to the ...</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What was Amazon's Free Cash Flow (FCF) in 2023...</td>\n",
       "      <td>In 2023, Amazon's Free Cash Flow (FCF) was $3...</td>\n",
       "      <td>Amazon's Free Cash Flow (FCF) in 2023, adjust...</td>\n",
       "      <td>True</td>\n",
       "      <td>YES, the context supports the information tha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>YES, the response is in line with the context...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated answer is fully relevant and cor...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which premium brands started selling on Amazon...</td>\n",
       "      <td>Several premium brands started selling on Ama...</td>\n",
       "      <td>Several premium brands started listing on Ama...</td>\n",
       "      <td>True</td>\n",
       "      <td>YES (The context mentions several premium bra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>YES. The response is in line with the context...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Both the generated and reference answers are i...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  What was the total revenue growth rate of Amaz...   \n",
       "1  Which segment of Amazon's business had the hig...   \n",
       "2  What was Amazon's operating income in 2023, an...   \n",
       "3  What was Amazon's Free Cash Flow (FCF) in 2023...   \n",
       "4  Which premium brands started selling on Amazon...   \n",
       "\n",
       "                                    generated_answer  \\\n",
       "0   The total revenue of Amazon grew by 12% year-...   \n",
       "1   The segment of Amazon's business with the hig...   \n",
       "2   In 2023, Amazon's operating income was $36.9 ...   \n",
       "3   In 2023, Amazon's Free Cash Flow (FCF) was $3...   \n",
       "4   Several premium brands started selling on Ama...   \n",
       "\n",
       "                                        ground_truth  faithfulness  \\\n",
       "0   The total revenue of Amazon grew by 12% year-...          True   \n",
       "1   AWS revenue increased 13% Y o Y from $80B to ...         False   \n",
       "2   Amazon's operating income in 2023 was $36.9 b...          True   \n",
       "3   Amazon's Free Cash Flow (FCF) in 2023, adjust...          True   \n",
       "4   Several premium brands started listing on Ama...          True   \n",
       "\n",
       "                               faithfulness_feedback  faithfulness_score  \\\n",
       "0   YES\\n(The context mentions that Apple pie is ...                 1.0   \n",
       "1   NO, the context does not provide any informat...                 0.0   \n",
       "2   The context supports the information that Ama...                 1.0   \n",
       "3   YES, the context supports the information tha...                 1.0   \n",
       "4   YES (The context mentions several premium bra...                 1.0   \n",
       "\n",
       "   relevancy                                 relevancy_feedback  \\\n",
       "0       True   YES, the response is in line with the context...   \n",
       "1       True   YES, the response is in line with the context...   \n",
       "2       True   YES, the response is in line with the context...   \n",
       "3       True   YES, the response is in line with the context...   \n",
       "4       True   YES. The response is in line with the context...   \n",
       "\n",
       "   relevancy_score  correctness  \\\n",
       "0              1.0         True   \n",
       "1              1.0         True   \n",
       "2              1.0         True   \n",
       "3              1.0         True   \n",
       "4              1.0         True   \n",
       "\n",
       "                                correctness_feedback  correctness_score  \n",
       "0  The generated answer is identical to the refer...                5.0  \n",
       "1  The generated answer is identical to the refer...                5.0  \n",
       "2  The generated answer is fully relevant to the ...                4.5  \n",
       "3  The generated answer is fully relevant and cor...                5.0  \n",
       "4  Both the generated and reference answers are i...                5.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "evaluation_results_df = run_evaluations(evaluation_questions, evaluation_ground_truth, query_engine, llm)\n",
    "evaluation_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Query\n",
      "Which segment of Amazon's business had the highest revenue growth rate in 2023?\n",
      "\n",
      "--- Generated Answer\n",
      " The segment of Amazon's business with the highest revenue growth rate in 2023 was AWS, with a\n",
      "growth rate of 13%.\n",
      "\n",
      "--- Ground Truth\n",
      " AWS revenue increased 13% Y o Y from $80B to $91B, which is the highest revenue growth rate among\n",
      "the provided segments in 2023.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query                    Which segment of Amazon's business had the hig...\n",
       "generated_answer          The segment of Amazon's business with the hig...\n",
       "ground_truth              AWS revenue increased 13% Y o Y from $80B to ...\n",
       "faithfulness                                                         False\n",
       "faithfulness_feedback     NO, the context does not provide any informat...\n",
       "faithfulness_score                                                     0.0\n",
       "relevancy                                                             True\n",
       "relevancy_feedback        YES, the response is in line with the context...\n",
       "relevancy_score                                                        1.0\n",
       "correctness                                                           True\n",
       "correctness_feedback     The generated answer is identical to the refer...\n",
       "correctness_score                                                      5.0\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a single row\n",
    "row=1\n",
    "print(\"\\n--- Query\")\n",
    "print_ww(evaluation_results_df.iloc[row,0])\n",
    "print(\"\\n--- Generated Answer\")\n",
    "print_ww(evaluation_results_df.iloc[row,1])\n",
    "print(\"\\n--- Ground Truth\")\n",
    "print_ww(evaluation_results_df.iloc[row,2])\n",
    "print(\"\\n\")\n",
    "evaluation_results_df.iloc[row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Semantic Similarity Evaluation \n",
    "\n",
    "The **SemanticSimilarityEvaluator** evaluates the quality of a question answering system by comparing the similarity between embeddings of the generated answer and the reference answer. Now that we have reference answer (ground-truth) created by Mistral Large in the previous steps, we can use this evaluator to check quality of this Q&A dataset via semantic similarity. What it does behind the scenes is calculating the similarity score between embeddings of the generated answer and the reference answer(ground-truth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Generated Answer\n",
      " In 2023, Amazon's Free Cash Flow (FCF) was $35.5 billion, which represents an improvement of $48.3\n",
      "billion compared to the previous year.\n",
      "------ Ground Truth\n",
      " Amazon's Free Cash Flow (FCF) in 2023, adjusted for equipment finance leases, was $35.5 billion.\n",
      "This represented an improvement from the previous year, with an increase of $48.3 billion. In 2022,\n",
      "the FCF was negative at -$12.8 billion.\n",
      "------\n",
      "\n",
      "Score:  0.9756784845586329\n",
      "\n",
      "Passing:  True\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "\n",
    "similiratity_evaluator = SemanticSimilarityEvaluator()\n",
    "\n",
    "# Picks generated ansnwer and ground truth for the 3rd row of DataFrame. (feel free to use any other)\n",
    "response = evaluation_results_df.loc[3,'generated_answer'] # generated_answer column\n",
    "reference = evaluation_results_df.loc[3,'ground_truth'] # ground_truth column\n",
    "\n",
    "print_ww(\"------ Generated Answer\")\n",
    "print_ww(response)\n",
    "print_ww(\"------ Ground Truth\")\n",
    "print_ww(reference)\n",
    "print_ww(\"------\")\n",
    "\n",
    "similarity_result = await similiratity_evaluator.aevaluate(\n",
    "    response=response,\n",
    "    reference=reference,\n",
    ")\n",
    "\n",
    "print(\"\\nScore: \", similarity_result.score)\n",
    "print(\"\\nPassing: \", similarity_result.passing)  # default similarity threshold is 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Automating Q&A Generation and Evaluation with Ragas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ragas (RAG Assessment) offers a framework designed to assess RAG pipelines. The evaluation metrics used by Ragas, similarly to the ones we used above with LlamaIndex evaluators, are a set of metrics designed to assess the performance and safety of AI applications, particularly in the context of grounded conversational AI systems. Below you will find the metric we will be using with Ragas in this notebook.\n",
    "\n",
    "1. **Faithfulness**\n",
    "Faithfulness measures the extent to which an AI assistant's response is faithful to the provided context or information. It evaluates whether the assistant's response is consistent with the given facts and does not contradict or deviate from the provided context.\n",
    "2. **Answer Relevancy**\n",
    "Answer relevancy assesses how relevant the AI assistant's response is to the user's query or question. It evaluates whether the response addresses the core intent of the query and provides information that is directly relevant to the user's needs.\n",
    "3. **Answer Similarity**\n",
    "Answer similarity measures the semantic similarity between the AI assistant's response and the expected or ideal answer. It evaluates how closely the generated response matches the desired or ground truth answer in terms of meaning and content.\n",
    "4. **Answer Correctness**\n",
    "Answer correctness evaluates the factual accuracy and correctness of the AI assistant's response. It assesses whether the information provided in the response is true, accurate, and free from factual errors or inconsistencies.\n",
    "5. **Context Precision**\n",
    "Context precision measures the precision of the AI assistant's response concerning the provided context. It evaluates how well the assistant's response incorporates and utilizes the relevant information from the given context, without including irrelevant or extraneous information.\n",
    "6. **Context Recall**\n",
    "Context recall measures the recall of the AI assistant's response concerning the provided context. It evaluates how much of the relevant information from the given context is included and covered in the assistant's response.\n",
    "7. **Context Entity Recall**\n",
    "Context entity recall specifically evaluates the AI assistant's ability to identify and include relevant entities (e.g., names, places, organizations) from the provided context in its response.\n",
    "8. **Harmfulness**\n",
    "Harmfulness assesses the potential for the AI assistant's response to cause harm or promote harmful or unethical content. It evaluates whether the response contains offensive, biased, or potentially harmful language or information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_similarity,\n",
    "    answer_correctness,    \n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    context_entity_recall\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    context_entity_recall,\n",
    "    harmfulness,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Generate a Synthetic Test Set with Ragas TestsetGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By leveraging the **TestsetGenerator**, you can create test sets tailored to specific domains, topics, or use cases, ensuring that your AI assistant is thoroughly evaluated across a wide range of scenarios. The generated test sets can include various types of queries, contexts, and expected responses, allowing you to assess the assistant's performance metrics such as faithfulness, relevance, similarity, correctness, precision, recall, and potential harmfulness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "\n",
    "# init the embeddings\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    region_name=AWS_REGION,\n",
    "    model_id=DEFAULT_EMBEDDINGS\n",
    ")\n",
    "\n",
    "bedrock_model_mistral = ChatBedrock(\n",
    "    model_id=DEFAULT_MODEL,\n",
    "    model_kwargs=model_kwargs_mistral,\n",
    "    region_name=AWS_REGION,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TestsetGenerator:** This module is responsible for generating test sets for evaluating RAG pipelines. It provides a variety of test generation strategies, including simple, reasoning, and multi-context strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=bedrock_model_mistral,\n",
    "    critic_llm=bedrock_model_mistral,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1 - Generate with LlamaIndex documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step will involve loading a set of documents and text chunks, allowing the Mistral model to generate potential questions based on these documents. Additionally, it will create reference answers (referred to as 'ground truth') for these questions, all based on the provided documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Start dataset generation...\")\n",
    "\n",
    "testset = generator.generate_with_llamaindex_docs(documents=docs, \n",
    "                                                  test_size=5,\n",
    "                                                  raise_exceptions=False,\n",
    "                                                  with_debugging_logs=False, \n",
    "                                                  distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n",
    "#testset = generator.generate(test_size=5)\n",
    "print(\"Test dataset generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"How does Amazon's discipline in experimentat...</td>\n",
       "      <td>[===Recently, I was asked a provocative questi...</td>\n",
       "      <td>Amazon's discipline in experimentation contrib...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Amazon-com-Inc-2023-Sharehol...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What new international locations did Amazon's...</td>\n",
       "      <td>[expand selection and features, and move towar...</td>\n",
       "      <td>Mexico is the latest international Stores loca...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'page_label': '2', 'file_name': 'Amazon-com-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can a financial institution accelerate Gen...</td>\n",
       "      <td>[ SageMaker),and improving developer productiv...</td>\n",
       "      <td>The middle layer of Amazon SageMaker allows fi...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'page_label': '6', 'file_name': 'Amazon-com-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what ways do Amazon's advanced upstream war...</td>\n",
       "      <td>[warehouses. And, in the last few years, our s...</td>\n",
       "      <td>Amazon's advanced upstream warehouses, special...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'page_label': '5', 'file_name': 'Amazon-com-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0   \"How does Amazon's discipline in experimentat...   \n",
       "1   What new international locations did Amazon's...   \n",
       "2  How can a financial institution accelerate Gen...   \n",
       "3  In what ways do Amazon's advanced upstream war...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [===Recently, I was asked a provocative questi...   \n",
       "1  [expand selection and features, and move towar...   \n",
       "2  [ SageMaker),and improving developer productiv...   \n",
       "3  [warehouses. And, in the last few years, our s...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  Amazon's discipline in experimentation contrib...         simple   \n",
       "1  Mexico is the latest international Stores loca...         simple   \n",
       "2  The middle layer of Amazon SageMaker allows fi...      reasoning   \n",
       "3  Amazon's advanced upstream warehouses, special...  multi_context   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': 'data/Amazon-com-Inc-2023-Sharehol...          True  \n",
       "1  [{'page_label': '2', 'file_name': 'Amazon-com-...          True  \n",
       "2  [{'page_label': '6', 'file_name': 'Amazon-com-...          True  \n",
       "3  [{'page_label': '5', 'file_name': 'Amazon-com-...          True  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = testset.to_pandas()\n",
    "df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - Generate with Langchain documents (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is optional since documents have been loaded via LlamaIndex above using _**generator.generate_with_llamaindex_docs()**_ method. This is only for those who want to use Ragas with Langchain instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "data = PyPDFLoader(\"data/Amazon-com-Inc-2023-Shareholder-Letter.pdf\").load_and_split()\n",
    "\n",
    "print(\"Start dataset generation...\")\n",
    "\n",
    "# generate testset\n",
    "testset_langchain = generator.generate_with_langchain_docs(documents=data, \n",
    "                                                           test_size=5,\n",
    "                                                           raise_exceptions=False,\n",
    "                                                           with_debugging_logs=False, \n",
    "                                                           distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n",
    "\n",
    "print(\"Test dataset generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the potential for growth in the cloud...</td>\n",
       "      <td>[ opportunities for\\ngrowth across the areas i...</td>\n",
       "      <td>There is significant potential for growth in t...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'page_label': '7', 'file_name': 'Amazon-com-...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"In what area is there a significant shift ha...</td>\n",
       "      <td>[on-premises. These businesses will keep shift...</td>\n",
       "      <td>In the Media and Advertising industry, there i...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Amazon-com-Inc-2023-Sharehol...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"How does sub-one-hour delivery through servi...</td>\n",
       "      <td>[Or, take a service that some people have ques...</td>\n",
       "      <td>Sub-one-hour delivery through services like Am...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/Amazon-com-Inc-2023-Sharehol...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0   What is the potential for growth in the cloud...   \n",
       "1   \"In what area is there a significant shift ha...   \n",
       "2   \"How does sub-one-hour delivery through servi...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [ opportunities for\\ngrowth across the areas i...   \n",
       "1  [on-premises. These businesses will keep shift...   \n",
       "2  [Or, take a service that some people have ques...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  There is significant potential for growth in t...         simple   \n",
       "1  In the Media and Advertising industry, there i...         simple   \n",
       "2  Sub-one-hour delivery through services like Am...         simple   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'page_label': '7', 'file_name': 'Amazon-com-...          True  \n",
       "1  [{'source': 'data/Amazon-com-Inc-2023-Sharehol...          True  \n",
       "2  [{'source': 'data/Amazon-com-Inc-2023-Sharehol...          True  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_langchain = testset_langchain.to_pandas()\n",
    "df_langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------- Question ---------------\n",
      " \"How does Amazon's discipline in experimentation contribute to their resilience and growth in the face of unprecedented change in the retail and IT industries?\n",
      "Answer:  Amazon's discipline in experimentation is a key factor in their resilience and growth in\n",
      "the face of unprecedented change in the retail and IT industries. By focusing on hiring builders who\n",
      "are motivated to continually improve and expand what's possible, Amazon is able to innovate and\n",
      "experiment at a high rate. They prioritize solving real customer challenges rather than being\n",
      "distracted by what may be interesting technology. Amazon also builds in primitives, which are\n",
      "foundational building blocks that enable them to innovate and experiment in new ways. They do not\n",
      "waste time trying to fight gravity, but instead embrace technology that enables better customer\n",
      "experiences. Finally, they learn from their failed experiments and use that knowledge to inform\n",
      "future attempts. This approach allows Amazon to adapt to changing markets and stay ahead of the\n",
      "competition.\n",
      "\n",
      "--------------- Question ---------------\n",
      " What new international locations did Amazon's Stores business turn profitable in during Q4 2023, and what is the company's conviction for their long-term growth?\n",
      "Answer:  The context information does not provide details about which specific international\n",
      "locations Amazon's Stores business turned profitable in during Q4 2023. However, it mentions the\n",
      "company's increasing conviction that their business in emerging geographies, such as India, Brazil,\n",
      "Australia, Mexico, Middle East, Africa, etc., is making meaningful progress.\n",
      "\n",
      "--------------- Question ---------------\n",
      "How can a financial institution accelerate GenAI app development with Amazon SageMaker's middle layer and maintain top-tier cloud security?\n",
      "Answer:  The middle layer of Amazon's AI stack, as described in the context, is designed to help\n",
      "customers leverage existing foundation models (FMs) and build custom GenAI applications using\n",
      "leading cloud provider's security and features. Amazon SageMaker is a managed, end-to-end service\n",
      "that simplifies the process of preparing data for AI, managing experiments, training models faster,\n",
      "and deploying them at scale with low latency and cost efficiency. By using Amazon SageMaker,\n",
      "financial institutions can accelerate their GenAI app development while maintaining top-tier cloud\n",
      "security, as AWS and its partners offer the strongest security capabilities and track record in the\n",
      "industry.\n",
      "\n",
      "--------------- Question ---------------\n",
      "In what ways do Amazon's advanced upstream warehouses, specialized in storage and automated restocking, enhance the handling of perishable goods and enable swift delivery of diverse product categories, such as groceries and medications?\n",
      "Answer:  Amazon's advanced upstream warehouses, specialized in storage and automated restocking,\n",
      "play a crucial role in enabling swift delivery of diverse product categories, particularly\n",
      "perishable goods like groceries and medications. By providing ample storage space for these items,\n",
      "Amazon can maintain higher in-stock rates and shorter delivery times. Additionally, the automated\n",
      "replenishment system ensures that inventory levels are constantly monitored and restocked as needed,\n",
      "reducing the likelihood of stockouts and ensuring a consistent supply of perishable items. This not\n",
      "only benefits Amazon's own consumer business but also external sellers who use Amazon's logistics\n",
      "services, such as Fulfillment by Amazon (FBA) and Multi-Channel Fulfillment. The integration of\n",
      "these logistics primitives into external applications, like Buy with Prime and Supply Chain by\n",
      "Amazon, further expands the degrees of freedom for Amazon and its customers, allowing for rapid\n",
      "innovation and convenient shopping experiences.\n"
     ]
    }
   ],
   "source": [
    "# add a new column to the DataFrame as some of the metrics \n",
    "# being evaluated require the answer column in the dataset\n",
    "df[\"answer\"]=''\n",
    "\n",
    "# iterate through df variable and populate answer column \n",
    "for i, row in df.iterrows():\n",
    "    # add a new column do df\n",
    "    print(\"\\n--------------- Question ---------------\")\n",
    "    print(df.question[i])\n",
    "    response = query_engine.query(df.question[i])\n",
    "    df.loc[i, \"answer\"]=response.response\n",
    "    print_ww(\"Answer: \" + df.answer[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"How does Amazon's discipline in experimentat...</td>\n",
       "      <td>[===Recently, I was asked a provocative questi...</td>\n",
       "      <td>Amazon's discipline in experimentation contrib...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'creation_date': None, 'file_name': None, 'f...</td>\n",
       "      <td>True</td>\n",
       "      <td>Amazon's discipline in experimentation is a k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What new international locations did Amazon's...</td>\n",
       "      <td>[expand selection and features, and move towar...</td>\n",
       "      <td>Mexico is the latest international Stores loca...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'creation_date': '2024-05-13', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>The context information does not provide deta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can a financial institution accelerate Gen...</td>\n",
       "      <td>[ SageMaker),and improving developer productiv...</td>\n",
       "      <td>The middle layer of Amazon SageMaker allows fi...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'creation_date': '2024-05-13', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>The middle layer of Amazon's AI stack, as des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what ways do Amazon's advanced upstream war...</td>\n",
       "      <td>[warehouses. And, in the last few years, our s...</td>\n",
       "      <td>Amazon's advanced upstream warehouses, special...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'creation_date': '2024-05-13', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Amazon's advanced upstream warehouses, specia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0   \"How does Amazon's discipline in experimentat...   \n",
       "1   What new international locations did Amazon's...   \n",
       "2  How can a financial institution accelerate Gen...   \n",
       "3  In what ways do Amazon's advanced upstream war...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [===Recently, I was asked a provocative questi...   \n",
       "1  [expand selection and features, and move towar...   \n",
       "2  [ SageMaker),and improving developer productiv...   \n",
       "3  [warehouses. And, in the last few years, our s...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  Amazon's discipline in experimentation contrib...         simple   \n",
       "1  Mexico is the latest international Stores loca...         simple   \n",
       "2  The middle layer of Amazon SageMaker allows fi...      reasoning   \n",
       "3  Amazon's advanced upstream warehouses, special...  multi_context   \n",
       "\n",
       "                                            metadata  episode_done  \\\n",
       "0  [{'creation_date': None, 'file_name': None, 'f...          True   \n",
       "1  [{'creation_date': '2024-05-13', 'file_name': ...          True   \n",
       "2  [{'creation_date': '2024-05-13', 'file_name': ...          True   \n",
       "3  [{'creation_date': '2024-05-13', 'file_name': ...          True   \n",
       "\n",
       "                                              answer  \n",
       "0   Amazon's discipline in experimentation is a k...  \n",
       "1   The context information does not provide deta...  \n",
       "2   The middle layer of Amazon's AI stack, as des...  \n",
       "3   Amazon's advanced upstream warehouses, specia...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "\n",
    "synthetic_dataset = Dataset.from_pandas(df)\n",
    "synthetic_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragas Evaluation module\n",
    "\n",
    "The evaluation step leverages the questions from the generated test set to assess the performance of the RAG pipeline. In our example scenario, Mistral is utilized to validate the answers produced by our RAG pipeline against the questions provided in the previously created test set.\n",
    "\n",
    "Then, the LLM is tasked with evaluating how well the retrieved contexts align with the given questions. This step ensures that the contextual information provided to the LLM is relevant and appropriate for answering the queries.\n",
    "\n",
    "Finally, the answers generated by Mistral are compared against the ground truth answers included in the test set. This comparison allows for a comprehensive evaluation of the LLM's performance in generating accurate and relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5d973112da47ddb7dfdf55b90aa657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 468 ms, sys: 50.2 ms, total: 518 ms\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from ragas import evaluate\n",
    "import nest_asyncio  \n",
    "\n",
    "# Only used when running on a jupyter notebook, otherwise you may want to remove this function\n",
    "nest_asyncio.apply()\n",
    "\n",
    "result = evaluate(\n",
    "    synthetic_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=bedrock_model_mistral,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "      <th>answer</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>harmfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"How does Amazon's discipline in experimentat...</td>\n",
       "      <td>[===Recently, I was asked a provocative questi...</td>\n",
       "      <td>Amazon's discipline in experimentation contrib...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'creation_date': None, 'file_name': None, 'f...</td>\n",
       "      <td>True</td>\n",
       "      <td>Amazon's discipline in experimentation is a k...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.762105</td>\n",
       "      <td>0.852866</td>\n",
       "      <td>0.588216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What new international locations did Amazon's...</td>\n",
       "      <td>[expand selection and features, and move towar...</td>\n",
       "      <td>Mexico is the latest international Stores loca...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'creation_date': '2024-05-13', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>The context information does not provide deta...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791031</td>\n",
       "      <td>0.197758</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can a financial institution accelerate Gen...</td>\n",
       "      <td>[ SageMaker),and improving developer productiv...</td>\n",
       "      <td>The middle layer of Amazon SageMaker allows fi...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'creation_date': '2024-05-13', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>The middle layer of Amazon's AI stack, as des...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.731659</td>\n",
       "      <td>0.874606</td>\n",
       "      <td>0.668651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In what ways do Amazon's advanced upstream war...</td>\n",
       "      <td>[warehouses. And, in the last few years, our s...</td>\n",
       "      <td>Amazon's advanced upstream warehouses, special...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'creation_date': '2024-05-13', 'file_name': ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Amazon's advanced upstream warehouses, specia...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.412327</td>\n",
       "      <td>0.858150</td>\n",
       "      <td>0.589537</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0   \"How does Amazon's discipline in experimentat...   \n",
       "1   What new international locations did Amazon's...   \n",
       "2  How can a financial institution accelerate Gen...   \n",
       "3  In what ways do Amazon's advanced upstream war...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [===Recently, I was asked a provocative questi...   \n",
       "1  [expand selection and features, and move towar...   \n",
       "2  [ SageMaker),and improving developer productiv...   \n",
       "3  [warehouses. And, in the last few years, our s...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  Amazon's discipline in experimentation contrib...         simple   \n",
       "1  Mexico is the latest international Stores loca...         simple   \n",
       "2  The middle layer of Amazon SageMaker allows fi...      reasoning   \n",
       "3  Amazon's advanced upstream warehouses, special...  multi_context   \n",
       "\n",
       "                                            metadata  episode_done  \\\n",
       "0  [{'creation_date': None, 'file_name': None, 'f...          True   \n",
       "1  [{'creation_date': '2024-05-13', 'file_name': ...          True   \n",
       "2  [{'creation_date': '2024-05-13', 'file_name': ...          True   \n",
       "3  [{'creation_date': '2024-05-13', 'file_name': ...          True   \n",
       "\n",
       "                                              answer  faithfulness  \\\n",
       "0   Amazon's discipline in experimentation is a k...           1.0   \n",
       "1   The context information does not provide deta...           0.5   \n",
       "2   The middle layer of Amazon's AI stack, as des...           1.0   \n",
       "3   Amazon's advanced upstream warehouses, specia...           1.0   \n",
       "\n",
       "   answer_relevancy  answer_similarity  answer_correctness  context_precision  \\\n",
       "0          0.762105           0.852866            0.588216                1.0   \n",
       "1          0.000000           0.791031            0.197758                1.0   \n",
       "2          0.731659           0.874606            0.668651                1.0   \n",
       "3          0.412327           0.858150            0.589537                1.0   \n",
       "\n",
       "   context_recall  context_entity_recall  harmfulness  \n",
       "0             1.0               0.333333            1  \n",
       "1             1.0               0.500000            0  \n",
       "2             1.0               0.000000            0  \n",
       "3             1.0               0.200000            0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "### Benefits of Using LLM Evaluators\n",
    "\n",
    "Utilizing LLM evaluators like LlamaIndex's evaluator and Ragas can provide valuable insights into the performance and reliability of your RAG pipeline as you have explored along this example, particularly when evaluating the outputs of language models such as Mistral. Here are some key benefits:\n",
    "\n",
    "### Assessing Response Quality\n",
    "\n",
    "LLM evaluators can help assess the quality of the responses generated by your RAG pipeline by comparing them against various criteria, such as:\n",
    "\n",
    "- **Correctness**: Evaluating if the generated answer matches the reference or ground truth answer, if available\n",
    "- **Semantic Similarity**: Measuring the semantic similarity between the generated answer and the reference answer, even if they differ in wording.\n",
    "- **Faithfulness**: Determining if the generated answer is faithful to the retrieved context, avoiding hallucinations or irrelevant information.\n",
    "\n",
    "### Evaluating Retrieval Relevance\n",
    "These evaluators can also assess the relevance of the retrieved context to the input query, ensuring that the RAG pipeline is providing appropriate information to the language model.\n",
    "\n",
    "### Guideline Adherence\n",
    "Evaluators like LlamaIndex and Ragas, can evaluate if the generated responses adhere to specific guidelines or constraints, which is crucial for maintaining control over the language model's outputs.\n",
    "\n",
    "### Automated Question Generation\n",
    "Tools like LlamaIndex and Ragas can automatically generate questions based on your data, allowing you to test your RAG pipeline's performance on a diverse set of queries without manual effort.\n",
    "\n",
    "By leveraging these evaluation capabilities, you can systematically identify areas for improvement in your RAG pipeline, retrieval components, and ultimately enhance the accuracy and reliability of your Generative AI-powered applications."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
