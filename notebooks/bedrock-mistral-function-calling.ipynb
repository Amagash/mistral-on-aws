{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3eed142-6144-4ba2-a693-2bcfdeeae823",
   "metadata": {},
   "source": [
    "# Bedrock Function Calling with Mistral models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a66db-4d22-4e0f-883c-8e8daf6a4291",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we walkthrough an implementation of function calling and agentic workflows using Mistral models on Amazon Bedrock. Function Calling is a powerful technique that allows large language models to be decomposed into smaller, more interpretable functions, which can be combined and executed in various ways to perform complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fcd65-01b7-418d-988e-f80d05ebb4a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Mistral Model Selection\n",
    "\n",
    "Today, there are two Mistral models available on Amazon Bedrock:\n",
    "\n",
    "### 1. Mistral 7B Instruct\n",
    "\n",
    "- **Description:** A 7B dense Transformer model, fast-deployed and easily customizable. Small yet powerful for a variety of use cases.\n",
    "- **Max Tokens:** 32K\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 2. Mixtral 8X7B Instruct\n",
    "\n",
    "- **Description:** A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Utilizes 12B active parameters out of 45B total.\n",
    "- **Max Tokens:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 3. Mistral Large\n",
    "\n",
    "- **Description:** A cutting-edge text generation model with top-tier reasoning capabilities. It can be used for complex multilingual reasoning tasks, including text understanding, transformation, and code generation.\n",
    "- **Max Tokens:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Synthetic Text Generation, Code Generation, RAG, or Agents\n",
    "\n",
    "### Performance and Cost Trade-offs\n",
    "\n",
    "The table below compares the model performance on the Massive Multitask Language Understanding (MMLU) benchmark and their on-demand pricing on Amazon Bedrock.\n",
    "\n",
    "| Model           | MMLU Score | Price per 1,000 Input Tokens | Price per 1,000 Output Tokens |\n",
    "|-----------------|------------|------------------------------|-------------------------------|\n",
    "| Mistral 7B Instruct | 62.5%      | \\$0.00015                    | \\$0.0002                      |\n",
    "| Mixtral 8x7B Instruct | 70.6%      | \\$0.00045                    | \\$0.0007                      |\n",
    "| Mistral Large | 81.2%      | \\$0.008                   | \\$0.024                     |\n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Mistral Model Selection Guide](https://docs.mistral.ai/guides/model-selection/)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3978ae9-57d0-4dca-b94b-b6493f8eade0",
   "metadata": {},
   "source": [
    "---\n",
    "## Supported papameters\n",
    "\n",
    "The Mistral AI models have the following inference parameters.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"max_tokens\" : int,\n",
    "    \"stop\" : [string],    \n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"top_k\": int\n",
    "}\n",
    "```\n",
    "\n",
    "The Mistral AI models have the following inference parameters:\n",
    "\n",
    "- **Temperature** - Tunes the degree of randomness in generation. Lower temperatures mean less random generations.\n",
    "- **Top P** - If set to float less than 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "- **Top K** - Can be used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.\n",
    "- **Maximum Length** - Maximum number of tokens to generate. Responses are not guaranteed to fill up to the maximum desired length.\n",
    "- **Stop sequences** - Up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22cf2e-971a-4df9-9e27-1cd7a05d8307",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fde7eb-a354-4934-9126-b793080328c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.17\n",
    "langchain-experimental==0.0.57\n",
    "boto3==1.34.97\n",
    "sqlalchemy==2.0.29\n",
    "pandas==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cfea5-782f-49c1-8885-c8fc8955e09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc6fb1-8ed0-45ab-b30c-c7f7a004e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-aws --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8193291-1bf2-478e-afff-d6afd33a358b",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444b9b77-84de-4709-aca1-98b6df29d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_mistral7b_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "instruct_mixtral8x7b_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "mistral_large_2402_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "\n",
    "DEFAULT_MODEL = instruct_mistral7b_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0645a9-bfcf-4daf-b8c7-b4a80c15cca8",
   "metadata": {},
   "source": [
    "## Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e4de92-afc7-4fc4-be46-241fff91319b",
   "metadata": {},
   "source": [
    "Function calling is the ability to reliably connect a large language model (LLM) to external tools and enable effective tool usage and interaction with external APIs. Mistral models provide the ability for building LLM powered chatbots or agents that need to retrieve context for the model or interact with external tools by converting natural language into API calls to retrieve specific domain knowledge. From conversational agents and math problem solving to API integration and information extraction, multiple use cases can benefit from this capability provided by Mistral models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc107b3-7dfa-494a-9269-6229b6dccb24",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18477cd8-aeb2-4fec-b9ea-74b73f7386f4",
   "metadata": {},
   "source": [
    "Let's say, we have a transactional dataset tracking customers, payment amounts, payment dates, and whether payments have been fully processed for each transaction identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c642f3-7aaf-4c37-9071-36a19188e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data()-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from a JSON file into a Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A Pandas DataFrame containing the data loaded from the JSON file.\n",
    "        If an error occurs during file loading, an error message is returned instead.\n",
    "\n",
    "    \"\"\"\n",
    "    local_path = \"sample_data/transactions.json\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_json(local_path)\n",
    "        return df\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        return  f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ad9e5a-5525-49a8-be42-1b8e7263aa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_id customer_id  payment_amount payment_date payment_status\n",
      "0          T1001        C001          125.50   2021-10-05           Paid\n",
      "1          T1002        C002           89.99   2021-10-06         Unpaid\n",
      "2          T1003        C003          120.00   2021-10-07           Paid\n",
      "3          T1004        C002           54.30   2021-10-05           Paid\n",
      "4          T1005        C001          210.20   2021-10-08        Pending\n"
     ]
    }
   ],
   "source": [
    "df = load_data()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175bb60-9bbb-4367-9741-32b5dc913b42",
   "metadata": {},
   "source": [
    "## Initialize Bedrock with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95573bd3-6c76-40b4-b01d-070d5618eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdee70f0-d3dc-40de-85e1-3aa8fa0f5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=DEFAULT_MODEL,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18150cc-1aac-4a7f-83f2-49d3c1a91abf",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8ce00-28f4-4074-9c27-14c9d2263b28",
   "metadata": {},
   "source": [
    "LangChain Tools are interfaces that allow agents, chains, or language models to interact with the world. They typically include a name, description, JSON schema for input parameters, and the function to call. This information is used to prompt the language model on how to specify and take actions. We will use LangChain Tools to quickly transform our functions as a prompt that can be used by Mistral to execute function calls.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc27426-c930-4ba2-885b-db9b8e837677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.tools import tool\n",
    "from pydantic.v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37320d-94c0-4e25-9457-74ca7cd37297",
   "metadata": {},
   "source": [
    "Let’s consider we have two functions as our two tools: **retrieve_payment_status** and **retrieve_payment_date** to retrieve payment status and payment date given a transaction ID.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "953f6f54-ff34-422b-9d99-873fe6f401c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(BaseModel):\n",
    "    transaction_id: str = Field(..., description='Transaction ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d4ad2-93a3-434c-b3a1-b310917784af",
   "metadata": {},
   "source": [
    "### Function Call 1: Retrieve payment status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db00433-5194-40e5-a1cc-a05a7d5c2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_payment_status(params: Params) -> str:\n",
    "    \"Get payment status of a transaction\"\n",
    "    data = load_data()\n",
    "\n",
    "    try:\n",
    "        # Attempt to retrieve the payment status for the given transaction ID\n",
    "        status = data[data.transaction_id == params.transaction_id].payment_status.item()\n",
    "    except ValueError:\n",
    "        # If the transaction ID is not found, return an error message\n",
    "        return {'error': f\"Transaction ID {params.transaction_id} not found.\"}\n",
    "\n",
    "    # Retrieve the payment status for the corresponding index\n",
    "    return json.dumps({'status': f\"{status}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3fe51-fe41-45dc-ba79-fcfa13dd7a7b",
   "metadata": {},
   "source": [
    "### Function Call 2: Retrieve payment date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0be1320-f94d-46ae-a0cc-738681a4ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_payment_date(params: Params) -> str:\n",
    "    \"Get payment date of a transaction\"\n",
    "    data = load_data()\n",
    "    \n",
    "    try:\n",
    "        # Attempt to retrieve the payment date for the given transaction ID\n",
    "        date = data[data.transaction_id == params.transaction_id].payment_date.item()\n",
    "    except ValueError:\n",
    "        # If the transaction ID is not found, return an error message\n",
    "        return {'error': f\"Transaction ID {params.transaction_id} not found.\"}\n",
    "    \n",
    "    return json.dumps({'date': date})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779e6db-2322-4cdf-8f4b-2b97e532c380",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae18c68-8ddd-4162-a036-f7dc496fca62",
   "metadata": {},
   "source": [
    "In this step, we will utilize another function from the LangChain library to transform a raw function or class into a format that can be easily understood and processed by a large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5e6964-e502-451c-9be5-5e7c171a133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions import convert_to_openai_function as convert_to_llm_fn\n",
    "tools = [retrieve_payment_status, retrieve_payment_date]\n",
    "functions = [convert_to_llm_fn(f) for f in tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a932f565-d50a-4536-81b7-bd9af2fc1aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'retrieve_payment_status',\n",
       "  'description': 'retrieve_payment_status(params: __main__.Params) -> str - Get payment status of a transaction',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'params': {'type': 'object',\n",
       "     'properties': {'transaction_id': {'description': 'Transaction ID',\n",
       "       'type': 'string'}},\n",
       "     'required': ['transaction_id']}},\n",
       "   'required': ['params']}},\n",
       " {'name': 'retrieve_payment_date',\n",
       "  'description': 'retrieve_payment_date(params: __main__.Params) -> str - Get payment date of a transaction',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'params': {'type': 'object',\n",
       "     'properties': {'transaction_id': {'description': 'Transaction ID',\n",
       "       'type': 'string'}},\n",
       "     'required': ['transaction_id']}},\n",
       "   'required': ['params']}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855eea6-7a35-4a4f-b0db-93c4a9d3641a",
   "metadata": {},
   "source": [
    "## Agentic Workflow Orchestration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3804805-66c7-43dc-bae5-d6b19024e6ab",
   "metadata": {},
   "source": [
    "In this section, we defined helper functions to automate the process of identifying the **function call** to be used by the LLM, extract the function from its response, and execute the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "057071ff-7922-4c6d-9978-ed22aefd9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ea878-0da2-4a9d-ab1a-4af3e8a10638",
   "metadata": {},
   "source": [
    "Helper Function: Extract function call representations enclosed within XML tags (`<functioncall>` and `<multiplefunctions>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c4e1954-aed0-416d-966a-56d3e4d8ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_calls(completion: str):\n",
    "    if isinstance(completion, str):\n",
    "        content = completion\n",
    "    else:\n",
    "        content = completion.content\n",
    "\n",
    "    # Multiple functions lookup\n",
    "    mfn_pattern = r\"<multiplefunctions>(.*?)</multiplefunctions>\"\n",
    "    mfn_match = re.search(mfn_pattern, content, re.DOTALL)\n",
    "\n",
    "    # Single function lookup\n",
    "    single_pattern = r\"<functioncall>(.*?)</functioncall>\"\n",
    "    single_match = re.search(single_pattern, content, re.DOTALL)\n",
    "    \n",
    "    functions = []\n",
    "    \n",
    "    if not mfn_match and not single_match:\n",
    "         # No function calls found\n",
    "        return None\n",
    "    elif mfn_match:\n",
    "        # Multiple function calls found\n",
    "        multiplefn = mfn_match.group(1)\n",
    "        for fn_match in re.finditer(r\"<functioncall>(.*?)</functioncall>\", multiplefn, re.DOTALL):\n",
    "            fn_text = fn_match.group(1)\n",
    "            try:\n",
    "                functions.append(json.loads(fn_text.replace('\\\\', '')))\n",
    "            except json.JSONDecodeError:\n",
    "                pass  # Ignore invalid JSON\n",
    "    else:\n",
    "        # Single function call found\n",
    "        fn_text = single_match.group(1)\n",
    "        try:\n",
    "            functions.append(json.loads(fn_text.replace('\\\\', '')))\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # Ignore invalid JSON\n",
    "    return functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2f00a-6f53-4572-ab12-889d5a4397ca",
   "metadata": {},
   "source": [
    "Helper Function: Execute function call with the arguments captured by the LLM during the AI/user interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2be629d9-fe51-43ba-98fd-8ca85459ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function(function_list: list):\n",
    "    for function_dict in function_list:\n",
    "        function_name = function_dict['name']\n",
    "        arguments = function_dict['arguments']\n",
    "        \n",
    "        # Check if the function exists in the current scope\n",
    "        if function_name in globals():\n",
    "            func = globals()[function_name]\n",
    "            \n",
    "            # Call the function with the provided arguments\n",
    "            result = func.invoke(input=arguments)\n",
    "            return result\n",
    "        else:\n",
    "            return {'error': f\"Function '{function_name}' not found.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996ba3f-4838-4c2e-845e-207ea363ece7",
   "metadata": {},
   "source": [
    "Helper Function: The agentic workflow function contains the logic for orchestrating the execution of function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "908dbb19-8636-470d-84db-d85f3dc3df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agentic_workflow_(prompt: str, model: str, functions: list):\n",
    "    # Define the function call format\n",
    "    fn = \"\"\"{\"name\": \"function_name\", \"arguments\": {\"arg_1\": \"value_1\", \"arg_2\": value_2, ...}}\"\"\"\n",
    "\n",
    "    # Prepare the function string for the system prompt\n",
    "    fn_str = \"\\n\".join([str(f) for f in functions])\n",
    "    \n",
    "    # Define the system prompt\n",
    "    system_prompt = f\"\"\"\n",
    "You are a helpful assistant with access to the following functions:\n",
    "\n",
    "{fn_str}\n",
    "\n",
    "To use these functions respond with:\n",
    "\n",
    "<multiplefunctions>\n",
    "    <functioncall> {fn} </functioncall>\n",
    "    <functioncall> {fn} </functioncall>\n",
    "    ...\n",
    "</multiplefunctions>\n",
    "\n",
    "Edge cases you must handle:\n",
    "- If there are no functions that match the user request, you will respond politely that you cannot help.\n",
    "- If the user has not provided all information to execute the function call, ask for more details. Only, respond with the information requested and nothing else.\n",
    "- If asked something that cannot be determined with the user's request details, respond that it is not possible to fullfill the request and explain why.\n",
    "\"\"\"\n",
    "    # Prepare the messages for the language model\n",
    "    messages = [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"user\", prompt),\n",
    "    ]\n",
    "    \n",
    "    # Invoke the language model and get the completion\n",
    "    completion = llm.invoke(messages)\n",
    "    content = completion.content.strip()\n",
    "\n",
    "    # Extract function calls from the completion\n",
    "    functions = extract_function_calls(content)\n",
    "\n",
    "    if functions:\n",
    "        # If function calls are found, execute them and return the response\n",
    "        fn_response = execute_function(functions)\n",
    "        return fn_response\n",
    "    else:\n",
    "        # If no function calls are found, return the completion content\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a824dbd1-1c4b-4e9d-a4c5-0c385650dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_func = partial(run_agentic_workflow_, model=llm, functions=functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968670b-bca2-483c-86a0-777ab8fd14b0",
   "metadata": {},
   "source": [
    "## Q&A: Agentic Worklflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a8d2df-4287-437b-8b52-1fe7855ba478",
   "metadata": {},
   "source": [
    "Here, we defined a prompt to handle the conversation history and answer follow-up questions withi the agentic workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb120d74-3299-4191-8fb5-a5f3b394ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_history_prompt = \"\"\"\n",
    "#############\n",
    "Chat History:\n",
    "{chat_history}\n",
    "#############\n",
    "\n",
    "You are an AI assistant designed to human-like responses based on the transaction details provided to you.\n",
    "\n",
    "Transaction details:\n",
    "{transaction_details}\n",
    "\n",
    "Provide clear and concise responses based solely on the data, without making any assumptions or inferences beyond what is contained in the transaction details. \n",
    "If the transaction details shows an 'error' message, it means we do not have enough information. In this case, respond that it is not possible to fullfill the request and explain why\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb9a3e-f1f9-48bf-bb4b-5c990f40ff68",
   "metadata": {},
   "source": [
    "Next, the following function allows a user to have a conversation with Mistral models, where Mistral generates responses based on the user's input and some action or function call is executed to perform actions on your behalf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78590e50-0fe1-4f98-aaf3-9a22f7a47d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qa_agent():\n",
    "    # Initialize conversation history\n",
    "    conversation_history = []\n",
    "    \n",
    "    print(\"Welcome to the LLM Conversation! Type 'exit' to end the conversation.\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"Ask a question: \\n\")\n",
    "    \n",
    "        # Check if the user wants to exit\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "    \n",
    "        # Add user input to the conversation history\n",
    "        conversation_history.append((\"user\", user_input))\n",
    "    \n",
    "        # Prepare the prompt from the conversation history\n",
    "        prompt = \"\\n\".join([q[1] for q in conversation_history if 'user' in q])\n",
    "    \n",
    "        # Generate the action to take based on the detected function call\n",
    "        action_response = generation_func(prompt)\n",
    "    \n",
    "        # Prepare the question-answer prompt\n",
    "        qa_prompt = conv_history_prompt.format(chat_history=action_response, transaction_details=prompt)\n",
    "    \n",
    "        # Prepare the messages for final LLM response\n",
    "        messages = [\n",
    "            (\"system\", qa_prompt),\n",
    "            (\"user\", str(action_response)),\n",
    "        ]\n",
    "    \n",
    "        # Get the response from the LLM\n",
    "        response = llm.invoke(messages).content.strip()\n",
    "\n",
    "        # Display the LLM response\n",
    "        print(\"Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f1f9380-d620-4a5c-8a4d-c97b69795f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the LLM Conversation! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " What's the status of my transaction?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " I'm sorry, but I cannot provide the status of your transaction as the transaction ID you provided was not found in our system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " My transaction ID is T1001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Based on the transaction details provided, your transaction with ID T1001 has been marked as \"Paid\". This means that the payment associated with this transaction has been successfully processed. If you have any further questions or need assistance with anything else, feel free to ask.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "run_qa_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4eee6d2-1689-46f1-972f-d008614fbce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the LLM Conversation! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " On what date was my transaction ID made?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " I'm sorry, but we cannot fulfill your request as the transaction ID \"your_transaction_ID\" was not found in our system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " My transaction ID is T1001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Based on the transaction details provided, the transaction ID T1001 was made on the date of October 5, 2021.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask a question: \n",
      " exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "run_qa_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458ce6a-216a-478d-968b-862fcfc8fe18",
   "metadata": {},
   "source": [
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Mistral AI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a51e5-3c22-496d-9110-bbc7f07e9dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
