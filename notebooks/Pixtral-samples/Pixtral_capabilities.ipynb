{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Depth Exploration of Pixtral's Capabilities\n",
    "\n",
    "Welcome to this comprehensive notebook where we delve into the diverse capabilities of **Pixtral**, a cutting-edge multimodal language model. In this notebook, we will examine Pixtral's performance across a variety of tasks, including:\n",
    "\n",
    "- **Optical Character Recognition (OCR)**\n",
    "- **Image Classification**\n",
    "- **Object Detection**\n",
    "- **Image Captioning**\n",
    "- **Visual Question Answering (VQA)**\n",
    "- **Handwriting Recognition**\n",
    "- **And More**\n",
    "\n",
    "The objective of this notebook is to provide you with a clear understanding of where Pixtral excels and to identify areas where it may face challenges. While these insights are based on our observations, your experiences and results may vary depending on the datasets and use cases you explore.\n",
    "\n",
    "### Pixtral 12B in Short\n",
    "\n",
    "- **Natively multimodal:** Trained with interleaved image and text data.\n",
    "- **Strong performance on multimodal tasks:** Excels in instruction following.\n",
    "- **State-of-the-art text-only benchmarks:** Maintains top performance in text-based evaluations.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "- **Vision Encoder:** New 400M parameter encoder trained from scratch.\n",
    "- **Multimodal Decoder:** 12B parameter decoder based on Mistral Nemo.\n",
    "- **Flexible Image Support:** Handles variable image sizes and aspect ratios.\n",
    "- **Long Context Window:** Supports multiple images within a 128k token context.\n",
    "\n",
    "### Use\n",
    "\n",
    "- **License:** Apache 2.0\n",
    "\n",
    "Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models. The model shows strong abilities in tasks such as chart and figure understanding, document question answering, multimodal reasoning and instruction following. Pixtral is able to ingest images at their natural resolution and aspect ratio, giving the user flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Unlike previous open-source models, Pixtral does not compromise on text benchmark performance to excel in multimodal tasks.\n",
    "\n",
    "<div style=\"display: flex; gap: 20px; justify-content: center; align-items: center;\">\n",
    "  <img src=\"https://mistral.ai/images/news/pixtral-12b/pixtral-benchmarks.png\" alt=\"Benchmarks\" style=\"width: 45%; height: auto;\"/>\n",
    "  <img src=\"https://mistral.ai/images/news/pixtral-12b/pixtral-comparison.png\" alt=\"Evals\" style=\"width: 45%; height: auto;\"/>\n",
    "</div>\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "[The instructions for how to get started using this notebook can be found in the Pixtral LMI notebook](https://github.com/aws-samples/mistral-on-aws/blob/59ab4ab9736122200a2d284039cb4557782e4a20/notebooks/Pixtral-samples/Pixtral-12b-LMI-SageMaker-realtime-inference.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.djl_inference import DJLModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session() # sagemaker session for interacting with different AWS APIs\n",
    "\n",
    "sagemaker_session_bucket = None # bucket to house artifacts\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role() # execution role for the endpoint\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region = sess.boto_region_name\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri =f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.30.0-lmi12.0.0-cu124\" \n",
    "\n",
    "# You can also obtain the image_uri programatically as follows.\n",
    "# image_uri = image_uris.retrieve(framework=\"djl-lmi\", version=\"0.30.0\", region=\"us-west-2\")\n",
    "\n",
    "model = DJLModel(\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": \"mistralai/Pixtral-12B-2409\",\n",
    "        \"HF_TOKEN\": \"<HF_Token>\", #since the model \"mistralai/Pixtral-12B-2409\" is gated model, you need HF_TOKEN\n",
    "        \"OPTION_ENGINE\": \"Python\",\n",
    "        \"OPTION_MPI_MODE\": \"true\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"8192\", # this can be tuned depending on instance type + memory available\n",
    "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\", # this can be tuned depending on instance type + memory available\n",
    "        \"OPTION_TOKENIZER_MODE\": \"mistral\",\n",
    "        \"OPTION_ENTRYPOINT\": \"djl_python.huggingface\",\n",
    "        \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "        \"OPTION_LIMIT_MM_PER_PROMPT\": \"image=4\", # this can be tuned to control how many images per prompt are allowed\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "If you want to use the Pixtral 12B model with its full capabilities—including the maximum context window of 128k tokens and support for multiple images—you should look to use a ml.p4d.24xlarge instance. This instance provides the necessary GPU memory and computational power to ensure optimal performance. \n",
    "\n",
    "If you prefer a balance between performance and cost, using the Pixtral 12B model at half precision on an ml.g5.12xlarge instance is a great choice. This setup handles context windows up to 8192 tokens efficiently and supports multiple images per prompt.\n",
    "\n",
    "For a minimal setup that keeps costs low, you can run the Pixtral 12B model on an ml.g5.xlarge instance. This is suitable for basic tasks with smaller context windows and single-image prompts.\n",
    "\n",
    "This is the author's back-of-the-napkin math - feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(instance_type=\"ml.g5.24xlarge\", initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many tokens make up an image?\n",
    "\n",
    "Images are passed through the vision encoder at their native resolution and aspect ratio, converting them into image tokens for each 16x16 patch in the image. These tokens are then flattened to create a sequence, with [IMG BREAK] and [IMG END] tokens added between rows and at the end of the image.[IMG BREAK] tokens let the model distinguish between images of different aspect ratios with the same number of tokens.\n",
    "\n",
    "For example, a 1024x1024 image is (1024/16)**2 = 4096 tokens. The image of luggage in the Pixtral data folder is 512x512 and is 1056 tokens. \n",
    "\n",
    "[More information can be found in the Pixtral blogpost](https://mistral.ai/news/pixtral-12b/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Use Cases\n",
    "\n",
    "encode_image_to_data_url(image_path): Converts an image file into a base64-encoded data URL.\n",
    "\n",
    "send_images_to_model(predictor, prompt, image_paths): Sends a text prompt and images (as data URLs) to a model and returns the response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_data_url(image_path):\n",
    "    \"\"\"\n",
    "    Reads an image from a local file path and encodes it to a data URL.\n",
    "    \"\"\"\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    base64_encoded = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    # Determine the image MIME type (e.g., image/jpeg, image/png)\n",
    "    mime_type = Image.open(image_path).get_format_mimetype()\n",
    "    data_url = f\"data:{mime_type};base64,{base64_encoded}\"\n",
    "    return data_url\n",
    "\n",
    "def send_images_to_model(predictor, prompt, image_paths):\n",
    "    \"\"\"\n",
    "    Sends images and a prompt to the model and returns the response in plain text.\n",
    "    \"\"\"\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "    \n",
    "    content_list = [{\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt\n",
    "    }]\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        # Encode image to data URL\n",
    "        data_url = encode_image_to_data_url(image_path)\n",
    "        \n",
    "        content_list.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": data_url\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content_list\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    \n",
    "    response = predictor.predict(payload)\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Extract and transcribe all text visible in the image, preserving its exact formatting, layout, and any special characters. Include line breaks and maintain the original capitalization and punctuation.\"\n",
    "image_path = \"Pixtral_data/amazon_s1_2.jpg\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legal and financial terminology was accurately recognized, which is crucial for documents like registration statements. The model effectively captured sections, such as headers and subheaders (e.g., \"### CALCULATION OF REGISTRATION FEE\"), indicating a good understanding of hierarchical text structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the attached image of an earnings report.\n",
    "\n",
    "Extract Key Data: Identify and summarize main financial metrics:\n",
    "\n",
    "Title\n",
    "\n",
    "Revenue\n",
    "Net income or loss\n",
    "Earnings per share (EPS)\n",
    "Operating expenses\n",
    "Significant one-time items or adjustments\n",
    "Diluted earnings per share\n",
    "Insights:\n",
    "\n",
    "Evaluate overall financial health based on profitability, revenue growth, or cost management.\n",
    "Note any risks or positive signals impacting future performance.\n",
    "Conclusion: Provide a brief summary of the company’s performance this quarter, highlighting potential growth areas or concerns for investors. If specific data isn't present, then leave blank.\n",
    "\"\"\"\n",
    "image_path = \"Pixtral_data/AMZN-Q2-2024-Earnings-Release.jpg\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Pixtral is provided with a low-resolution image, the model may hallucinate or misinterpret the image data. In this instance, Pixtral incorrectly extracted the dates from our earnings report due to the poor image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the attached image of an earnings report.\n",
    "\n",
    "Extract Key Data: Identify and summarize main financial metrics:\n",
    "\n",
    "Title\n",
    "\n",
    "Revenue\n",
    "Net income or loss\n",
    "Earnings per share (EPS)\n",
    "Operating expenses\n",
    "Significant one-time items or adjustments\n",
    "Diluted earnings per share\n",
    "Insights:\n",
    "\n",
    "Evaluate overall financial health based on profitability, revenue growth, or cost management.\n",
    "Note any risks or positive signals impacting future performance.\n",
    "Conclusion: Provide a brief summary of the company’s performance this quarter, highlighting potential growth areas or concerns for investors. If specific data isn't present, then leave blank.\n",
    "\"\"\"\n",
    "image_path = \"Pixtral_data/AMZN-Q2-2024-Earning-High-Quality.png\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, when we use the same image at a higher resolution, Pixtral generates the correct completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwriting Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Analyze the image and transcribe any handwritten text present. Convert the handwriting into a single, continuous string of text. Maintain the original spelling, punctuation, and capitalization as written. Ignore any printed text, drawings, or other non-handwritten elements in the image.\"\n",
    "image_path = \"Pixtral_data/a01-082u-01.png\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"\"\"\n",
    "\n",
    "Analyze the attached image of the chart or graph. Your tasks are to:\n",
    "\n",
    "Identify the type of chart or graph (e.g., bar chart, line graph, pie chart, etc.).\n",
    "Extract the key data points, including labels, values, and any relevant scales or units.\n",
    "Identify and describe the main trends, patterns, or significant observations presented in the chart.\n",
    "Generate a clear and concise paragraph summarizing the extracted data and insights. The summary should highlight the most important information and provide an overview that would help someone understand the chart without seeing it.\n",
    "Ensure that your summary is well-structured, accurately reflects the data, and is written in a professional tone.\n",
    "\"\"\"\n",
    "image_path = \"Pixtral_data/Amazon_Chart.png\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the image and provide a detailed description of what you see. Include:\n",
    "\n",
    "1. The main subject or focus of the image\n",
    "2. Key elements or objects present\n",
    "3. Colors, lighting, and overall mood\n",
    "4. Spatial arrangement and composition\n",
    "5. Any text or symbols visible\n",
    "6. Actions or events taking place, if applicable\n",
    "7. Background and setting details\n",
    "8. Distinctive features or unusual aspects\n",
    "9. Estimated time of day or season, if relevant\n",
    "10. Overall context or type of scene (e.g., natural landscape, urban setting, indoor space)\n",
    "\n",
    "Describe the image as if explaining it to someone who cannot see it. Be thorough but concise, focusing on the most important and interesting aspects of the image.\n",
    "\"\"\"\n",
    "image_path = \"Pixtral_data/3a1SR_oZI0-dCEvLG7US5g.jpg\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the image and identify all distinct objects present. For each object detected:\n",
    "\n",
    "1. Name the object\n",
    "2. Specify its approximate location in the image (e.g., top-left, center, bottom-right)\n",
    "3. Estimate its size relative to the image (e.g., small, medium, large)\n",
    "4. Note any relevant characteristics (color, shape, condition)\n",
    "5. Identify if it's partially obscured or fully visible\n",
    "\n",
    "List all objects detected, including people, animals, vehicles, furniture, buildings, natural elements, and any other identifiable items. If multiple instances of the same object type are present, count and report them separately. Ignore very small or indistinct elements that can't be clearly identified. If applicable, note any obvious interactions or relationships between objects.\n",
    "\"\"\"\n",
    "image_path = \"Pixtral_data/dresser.jpg\"  # Replace with your local image path\n",
    "response = send_images_to_model(predictor, prompt, image_path)\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Data Extraction\n",
    "\n",
    "Extracting structured data from product images is essential for efficient inventory management, e-commerce listings, and data analysis. Pixtral's multimodal capabilities enable the accurate transformation of visual information into a standardized JSON format, facilitating seamless integration with databases and applications.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The following code demonstrates how to utilize Pixtral to analyze product images and output the information in a predefined JSON structure. This ensures consistency and accuracy in capturing essential product details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_images_to_model(predictor, prompt, image_paths):\n",
    "    \"\"\"\n",
    "    Sends multiple images and a prompt to the model and returns the response in plain text.\n",
    "    \"\"\"\n",
    "    # Construct the content list starting with the prompt\n",
    "    content_list = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Add each image as an image_url entry\n",
    "    for image_path in image_paths:\n",
    "        data_url = encode_image_to_data_url(image_path)\n",
    "        content_list.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": data_url\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Construct the payload matching the expected structure\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content_list\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 3000,  # Adjusted max_tokens as needed\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "\n",
    "    response = predictor.predict(payload)\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a product analyst your job is to analyze the images provided and output the information in the exact JSON structure specified below. Ensure that you populate each field accurately based on the visible details in the image. If any information is not available or cannot be determined, use 'Unknown' for string fields and an empty array [] for lists.\n",
    "\n",
    "Use the format shown exactly, ensuring all fields and values align with the JSON schema requirements.\n",
    "\n",
    "Use this JSON schema:\n",
    "\n",
    "{\n",
    "  \"title\": \"string\",\n",
    "  \"description\": \"string\",\n",
    "  \"category\": {\n",
    "    \"type\": \"string\",\n",
    "    \"enum\": [\"Electronics\", \"Furniture\", \"Clothing\", \"Appliances\", \"Toys\", \"Books\", \"Tools\", \"Other\"]\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"color\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": { \"type\": \"string\" }\n",
    "    },\n",
    "    \"shape\": {\n",
    "      \"type\": \"string\",\n",
    "      \"enum\": [\"Round\", \"Square\", \"Rectangular\", \"Irregular\", \"Other\"]\n",
    "    },\n",
    "    \"condition\": {\n",
    "      \"type\": \"string\",\n",
    "      \"enum\": [\"New\", \"Like New\", \"Good\", \"Fair\", \"Poor\", \"Unknown\"]\n",
    "    },\n",
    "    \"material\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": { \"type\": \"string\" }\n",
    "    },\n",
    "    \"brand\": { \"type\": \"string\" }\n",
    "  },\n",
    "  \"image_quality\": {\n",
    "    \"type\": \"string\",\n",
    "    \"enum\": [\"High\", \"Medium\", \"Low\"]\n",
    "  },\n",
    "  \"background\": \"string\",\n",
    "  \"additional_features\": {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": { \"type\": \"string\" }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "image_paths = [\n",
    "    \"Pixtral_data/luggage.jpg\",\n",
    "    \"Pixtral_data/dresser.jpg\",\n",
    "    \"Pixtral_data/dog_bag.jpg\",\n",
    "]  # Replace with your actual image paths\n",
    "\n",
    "# Send to model\n",
    "response = send_multiple_images_to_model(predictor, prompt, image_paths)\n",
    "print('Response from the model:\\n\\n')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Q&A\n",
    "\n",
    "Visual Question and Answering (Visual Q&A) is a powerful feature of Pixtral that allows users to interact with images through natural language queries. By enabling multi-turn conversations, Pixtral can provide detailed and contextually relevant answers based on the visual content of the images. This capability is invaluable for applications such as customer support, educational tools, and interactive data analysis.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The following code demonstrates how to utilize Pixtral's Visual Q&A functionality. Users can pass their own images or use images from the Pixtral data folder. Additionally, the max_turns parameter can be adjusted to allow for more extended conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_qa(predictor, image_paths, max_turns=2):\n",
    "    \"\"\"\n",
    "    Performs visual Q&A with multiple images and multi-turn conversation.\n",
    "\n",
    "    Parameters:\n",
    "    - predictor: The SageMaker Predictor object.\n",
    "    - image_paths: A list of local image file paths.\n",
    "    - max_turns: The maximum number of conversational turns.\n",
    "\n",
    "    Returns:\n",
    "    - None. Outputs are printed directly.\n",
    "    \"\"\"\n",
    "    import base64\n",
    "    from PIL import Image\n",
    "    from IPython.display import display, Image as IPythonImage\n",
    "\n",
    "    # Encode images to data URLs\n",
    "    data_urls = [encode_image_to_data_url(image_path) for image_path in image_paths]\n",
    "\n",
    "    # Initialize conversation messages\n",
    "    messages = []\n",
    "\n",
    "    # Define the initial prompt within the function\n",
    "    initial_prompt = (\"You're an extremely friendly helper. Help the user answer questions about the images shown to you. \"\n",
    "                      \"If the answer isn't in the image, say 'I'm sorry, the answer is not in the provided image.'\")\n",
    "\n",
    "    # Start the conversation loop\n",
    "    for turn in range(max_turns):\n",
    "        # Get user input\n",
    "        user_question = input(\"\\nYou: \")\n",
    "        if user_question.strip() == '':\n",
    "            print(\"Please enter a question.\")\n",
    "            continue\n",
    "\n",
    "        # Build user's message content\n",
    "        if turn == 0:\n",
    "            # Include initial prompt and images in the first message\n",
    "            user_content = [{\"type\": \"text\", \"text\": initial_prompt + \" \" + user_question}]\n",
    "            for data_url in data_urls:\n",
    "                user_content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": data_url}\n",
    "                })\n",
    "        else:\n",
    "            user_content = user_question\n",
    "\n",
    "        # Append user's message to messages\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_content\n",
    "        })\n",
    "\n",
    "        # Construct the payload\n",
    "        payload = {\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": 3000,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "\n",
    "        # Send payload to model and get assistant's response\n",
    "        response = predictor.predict(payload)\n",
    "        assistant_response = response['choices'][0]['message']['content']\n",
    "\n",
    "        # Append assistant's response to messages\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": assistant_response\n",
    "        })\n",
    "\n",
    "        print(\"\\nAssistant:\", assistant_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of image paths\n",
    "image_paths = [\n",
    "    \"Pixtral_data/trimmed_green_beans.jpg\",\n",
    "    \"Pixtral_data/amazon_gloves.jpg\",\n",
    "    \"Pixtral_data/cleaner.jpg\"\n",
    "]\n",
    "\n",
    "# Run the visual Q&A function\n",
    "visual_qa(predictor, image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up resources\n",
    "predictor.delete_endpoint()\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution for street sign images\n",
    "This image is licensed under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license.\n",
    "\n",
    "Creator: Unknown\n",
    "Source: https://www.mapillary.com/dataset/trafficsign\n",
    "License: Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International\n",
    "Modifications: none\n",
    "\n",
    "\n",
    "U. Marti and H. Bunke. The IAM-database: An English Sentence Database for Off-line Handwriting Recognition. Int. Journal on Document Analysis and Recognition, Volume 5, pages 39 - 46, 2002."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
