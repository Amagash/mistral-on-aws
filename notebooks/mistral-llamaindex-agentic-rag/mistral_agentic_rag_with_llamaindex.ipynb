{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c020350b-d116-4a18-b38e-cd539f4180ed",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Please check this blog post for more details: blog link [TBC]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12801837-d6cd-44ac-ab1b-17f0b4688874",
   "metadata": {},
   "source": [
    "## Install Packages \n",
    "\n",
    "Install below ptyhon packages in requirements.txt: \n",
    "- **llama-index**: an open-source framework that helps build applications using LLMs. \n",
    "- **llama-index-llms-bedrock-converse**: Amazon Bedrock Converse integration with LlamaIndex.\n",
    "- **llama-index-embeddings-bedrock**: Amazon Bedrock embedding models integration with LlamaIndex. \n",
    "- **llama-index-retrievers-bedrock**: Amazon Bedrock Knowledge Bases integration with LlamaIndex. \n",
    "- **llama-index-tools-arxiv**: A prebuilt tool to query arxiv.org\n",
    "- **llama-index-tools-duckduckgo**: A prebuilt tool integrating DuckDuckGo search capabilities.\n",
    "- **llama-index-postprocessor-bedrock-rerank**: A LlamaIndex plugin that uses Amazon Bedrock's Rerank API to reorder retrieved documents by relevance. \n",
    "- **llama-index-vector-stores-opensearch**: A LlamaIndex integration that uses Amazon OpenSearch as a vector store for embedding storage and similarity search.\n",
    "- **feedparser**: A Python library for parsing for downloading and parsing syndicated feeds including RSS, Atom & RDF Feeds\n",
    "- **opensearch-py**:A Python library for connecting to and interacting with OpenSearch/Elasticsearch clusters\n",
    "- **requests-aws4auth**: A Python library that handles AWS Signature Version 4 authentication for making signed HTTP requests to AWS services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a21c87-2c96-4658-b026-f39f5697335e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b3949-6348-4d72-a1ef-9766109c65cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2f50c-99d7-47a1-8c5e-ae12a8392581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialise and configure the BedrockConverse LLM with the Mistral Large 2 model and set it as the default in Settings\n",
    "\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = BedrockConverse(model=\"mistral.mistral-large-2407-v1:0\", max_tokens = 2048)\n",
    "Settings.llm = BedrockConverse(model=\"mistral.mistral-large-2407-v1:0\", max_tokens = 2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d6b40-83e6-432d-90ef-8d31b1c8da26",
   "metadata": {},
   "source": [
    "## API tools integration \n",
    "\n",
    "We implement two functions to interact with GitHub and TechCrunch APIs. To ensure clear communication between the agent and the LLM model, we follow Python function best practices including:\n",
    "- Type hints for parameter and return value validation\n",
    "- Detailed docstrings explaining function purpose, parameters, and expected returns\n",
    "- Clear function descriptions\n",
    "\n",
    "For arXiv and DuckDuckGo integration, we leverage LlamaIndex's pre-built tools instead of creating custom functions. You can explore other available pre-built tools in the [LlamaIndex documentation](https://docs.llamaindex.ai/en/stable/api_reference/tools/) to avoid duplicating existing solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694f1aa-fcac-4667-8d21-fcbf1a3ff926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to search GitHub repositories by topic, sorting by stars or update date, and return top results\n",
    "\n",
    "import requests\n",
    "\n",
    "def github_search(topic: str, num_results: int = 3, sort_by: str = \"stars\") -> list:\n",
    "    \"\"\"\n",
    "    Retrieve a specified number of GitHub repositories based on a given topic, \n",
    "    ranked by the specified criteria.\n",
    "\n",
    "    This function uses the GitHub API to search for repositories related to a \n",
    "    specific topic or keyword. The results can be sorted by the number of stars \n",
    "    (popularity) or the most recent update, with the most relevant repositories \n",
    "    appearing first according to the chosen sorting method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    topic : str\n",
    "        The topic or keyword to search for in GitHub repositories.\n",
    "        The topic cannot contain blank spaces.\n",
    "    num_results : int, optional\n",
    "        The number of repository results to retrieve. Defaults to 3.\n",
    "    sort_by : str, optional\n",
    "        The criterion for sorting the results. Options include:\n",
    "        - 'stars': Sort by the number of stars (popularity).\n",
    "        - 'updated': Sort by the date of the last update (most recent first).\n",
    "        Defaults to 'stars'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        A list of dictionaries, where each dictionary contains information \n",
    "        about a repository. Each dictionary includes:\n",
    "        - 'html_url': The URL of the repository.\n",
    "        - 'description': A brief description of the repository.\n",
    "        - 'stargazers_count': The number of stars (popularity) the repository has.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    url = f\"https://api.github.com/search/repositories?q=topic:{topic}&sort={sort_by}&order=desc\"\n",
    "\n",
    "    response = requests.get(url).json()\n",
    "    \n",
    "    code_repos = [\n",
    "        {\n",
    "            'html_url': item['html_url'],\n",
    "            'description': item['description'],\n",
    "            'stargazers_count': item['stargazers_count'],\n",
    "            # 'topics': item['topics']\n",
    "        }\n",
    "        for item in response['items'][:num_results]\n",
    "    ]\n",
    "    \n",
    "    return code_repos\n",
    "\n",
    "github_tool = FunctionTool.from_defaults(fn=github_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883388e-7eff-46b3-b537-12fd4b24eae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to search for TechCrunch news articles by topic and return details for a specified number of results\n",
    "\n",
    "import feedparser\n",
    "    \n",
    "def news_search(topic: str, num_results: int = 3) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve a specified number of news articles from TechCrunch based on a given topic.\n",
    "\n",
    "    This function queries the TechCrunch RSS feed to search for news articles related to the \n",
    "    provided topic and returns a list of the most relevant articles. Each article includes \n",
    "    details such as the title, link, publication date, and a summary or description.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    topic : str\n",
    "        The keyword or subject to search for in the TechCrunch news feed.\n",
    "        The topic cannot contain blank spaces.\n",
    "        If multiple words are needed, connect them with \"+\" (e.g., artificial+intelligence).\n",
    "    num_results : int, optional\n",
    "        The number of articles to retrieve from the search results. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        A list of dictionaries, where each dictionary contains information about a retrieved \n",
    "        news article. Each dictionary includes:\n",
    "        - 'title': The title of the article.\n",
    "        - 'link': The URL to the article.\n",
    "        - 'published': The publication date of the article.\n",
    "        - 'summary': A brief summary or description of the article, if available.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    url = f\"https://techcrunch.com/tag/{topic}/feed/\"\n",
    "    feed = feedparser.parse(url)\n",
    "    \n",
    "    news = []\n",
    "    \n",
    "    # Loop through the top num_results articles\n",
    "    for entry in feed.entries[:num_results]:\n",
    "        # Create a dictionary for each article\n",
    "        article = {\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "            'published': entry.published,\n",
    "            'summary': entry.summary if hasattr(entry, 'summary') else entry.description if hasattr(entry, 'description') else None\n",
    "        }\n",
    "\n",
    "        # Add the article dictionary to the list\n",
    "        news.append(article)\n",
    "    \n",
    "    return news\n",
    "\n",
    "news_tool = FunctionTool.from_defaults(fn=news_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895f9a6c-2d98-4587-a502-ad66eb52614a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import and configure the ArxivToolSpec and DuckDuckGoSearchToolSpec from LlamaIndex prebuilt tools\n",
    "\n",
    "from llama_index.tools.arxiv import ArxivToolSpec\n",
    "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
    "\n",
    "arxiv_tool = ArxivToolSpec()\n",
    "search_tool = DuckDuckGoSearchToolSpec()\n",
    "\n",
    "api_tools = arxiv_tool.to_tool_list() + search_tool.to_tool_list(spec_functions=[\"duckduckgo_full_search\"])\n",
    "\n",
    "# Consolidate all tools into one list. \n",
    "api_tools.extend([news_tool, github_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f6fb6-1257-42b4-9b9f-b1bf463f10d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up an agent with access to GitHub, arXiv, and TechCrunch APIs, using a system prompt to guide interactions.\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "import time\n",
    "current_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a technology expert with access to the GitHub API, arXiv API, DuckDuckGo API and TechCrunch API. \n",
    "You can search for the latest code repositories, papers, and news related to technology.\n",
    "Always try to use the tools available to you. \n",
    "If you donâ€™t know the answer, do not make up any information; simply say: Sorry, I donâ€™t know.\n",
    "\n",
    "Current time is: {current_time}\n",
    "\"\"\"\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    api_tools, \n",
    "    llm=llm, \n",
    "    verbose=True, # Set verbose=True to display the full trace of steps. \n",
    "    system_prompt = system_prompt,\n",
    "    allow_parallel_tool_calls = True #Â this line to allow multiple tool invocations\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af59152-876a-4051-93ce-794f8d799f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"Can you give me top 2 papers about GenAI, and recent news about bedrock\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4afe6b-fe5e-4265-b36b-9d6c564e699c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple chatbot UI. Enter \"exit\" to quit. \n",
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    response = agent.chat(text_input)\n",
    "\n",
    "    print(\"-\" * 120)\n",
    "    print(\" \" * 120)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print(\"=\" * 120)\n",
    "    print(\" New question: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d7192-7c86-4d4d-a672-cb32564a035b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agent.memory.get() #Â retrieve conversation history\n",
    "# agent.memory.reset() # clear the chat memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056a046-2b98-44f9-9711-7fa057e4eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test questions: \n",
    "# 1. any news about GenAI\n",
    "# 2. can you give me top5 github code repo related to bedrock\n",
    "# 3. can you show me the top 3 paper that releted to GenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a213ec-877d-4d3a-8102-2fb41d16aba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "427f3df8-00aa-4c3f-a50b-fb0c3faf3e9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Documents RAG Integration with Amazon OpenSearch Serverless\n",
    "\n",
    "Below, we download two PDF files of decision guide documents from the AWS website, which provide recommendations for selecting GenAI and ML services in different scenarios, and outline what should be evaluated and considered in the decision-making process. You can provide and replace these with your internal business documents in this step.\n",
    "\n",
    "In the section below, we use LlamaIndex to process documents into chunks and convert them into embedding vectors using the Amazon Bedrock Embedding model. We then use Amazon OpenSearch Serverless as a vector store to persist the vectors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a4889-8664-4441-a4f7-052863021f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Ubuntu/Debian systems\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install ca-certificates -y\n",
    "\n",
    "# download test documents from below links\n",
    "!wget -O docs/genai_on_aws.pdf \"https://docs.aws.amazon.com/pdfs/decision-guides/latest/generative-ai-on-aws-how-to-choose/generative-ai-on-aws-how-to-choose.pdf?did=wp_card&trk=wp_card\"\n",
    "!wget -O docs/ml_on_aws.pdf \"https://docs.aws.amazon.com/pdfs/decision-guides/latest/machine-learning-on-aws-how-to-choose/machine-learning-on-aws-how-to-choose.pdf?did=wp_card&trk=wp_card#guide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d09000-c72b-477a-aaa4-453eb1746cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Llamaindex to load documents \n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "loader = SimpleDirectoryReader('docs/')\n",
    "documents = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574fec2c-6c7f-4fce-849f-2d11ece9c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise and configure the embedding model with Amazon Titan Text Embeddings V2, and set it as the default in Settings\n",
    "\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "embed_model = BedrockEmbedding(model_name=\"amazon.titan-embed-text-v2:0\")\n",
    "Settings.embed_model = BedrockEmbedding(model_name=\"amazon.titan-embed-text-v2:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe1a63-871a-447d-9478-2055e7a829c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Amazon OpenSearch Serverless collection \n",
    "from utils import *\n",
    "import sagemaker \n",
    "import random\n",
    "\n",
    "region_name = \"us-west-2\"\n",
    "suffix = random.randrange(1, 500)\n",
    "collection_name = \"mistral-test-\"+str(suffix)\n",
    "notebook_execution_role = sagemaker.get_execution_role()\n",
    "\n",
    "endpoint = create_collection(collection_name, notebook_execution_role)\n",
    "print(\"Amazon OpenSearch Collection endpoint is: \", endpoint)\n",
    "\n",
    "import time\n",
    "# Wait for 1 minute (60 seconds) for collection creation complete\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4709b-f8c8-4eaa-a90a-35445c486840",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an index in the collection\n",
    "index_name = \"pdf-docs\"\n",
    "create_index(index_name, endpoint, emb_dim=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01491a1c-c7dd-480d-b4ee-e8f4a2e0cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## integrate Amazon OpenSearch Serverless collection and index to llamaindex \n",
    "\n",
    "import boto3\n",
    "from llama_index.vector_stores.opensearch import  OpensearchVectorStore,   OpensearchVectorClient\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "dim = 1024 # Amazon Titan Embedding V2 model dimension \n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "client = OpensearchVectorClient(\n",
    "    endpoint, \n",
    "    index_name, \n",
    "    dim, \n",
    "    embedding_field=\"vector\", \n",
    "    text_field=\"chunk\",\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb635e-0fc5-4748-b2c3-96301756348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise vector store and save document chunks to the vector store \n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "vector_store = OpensearchVectorStore(client)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    storage_context=storage_context,\n",
    "    transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)]\n",
    ")\n",
    "\n",
    "# LlamaIndex provides various text splitters, more information: https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3abaeba-7189-425e-8334-d6f5f6ad183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Wait for 5 minutes (300 seconds) for document ingesion to complete\n",
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60f8d7-350c-431c-ba66-6c4fd88e5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use reranking to improve the quality and relevance of the context retrieval in RAG.\n",
    "\n",
    "from llama_index.postprocessor.bedrock_rerank import AWSBedrockRerank\n",
    "reranker = AWSBedrockRerank(\n",
    "    top_n=3,\n",
    "    model_id=\"amazon.rerank-v1:0\",#  another rerank model option is: cohere.rerank-v3-5:0\n",
    "    region_name=\"us-west-2\",\n",
    ")\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[reranker],\n",
    ")\n",
    "\n",
    "# test the vector store with a simple question \n",
    "response = query_engine.query(\n",
    "    \"In which situation should I use Amazon Bedrock over Amazon SageMaker?\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b956a183-aeff-42bf-9282-0eacc4daecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create QueryEngineTool based on the OpenSearch vector store \n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "oss_tool = QueryEngineTool(\n",
    "        query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"oss_guide_tool\",\n",
    "            description=\"\"\"\n",
    "            These decision guides help users select appropriate AWS machine learning and generative AI services based on specific needs. \n",
    "            They cover pre-built solutions, customizable platforms, and infrastructure options for ML workflows, \n",
    "            while outlining how generative AI can automate processes, personalize content, augment data, reduce costs, \n",
    "            and enable faster experimentation in various business contexts.\"\"\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8597751-e594-43fa-b4e6-d010163332d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tools = api_tools +[oss_tool]\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    all_tools, \n",
    "    llm=llm, \n",
    "    verbose=True, # Set verbose=True to display the full trace of steps. \n",
    "    system_prompt = system_prompt,\n",
    "    # allow_parallel_tool_calls = True  #Â Uncomment this line to allow multiple tool invocations\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0b9cee-0e07-4aad-97d2-882683bc2a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chatbot UI. Enter \"exit\" to quit. \n",
    "\n",
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    response = agent.chat(text_input)\n",
    "\n",
    "    print(\"-\" * 120)\n",
    "    print(\" \" * 120)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print(\"=\" * 120)\n",
    "    print(\" New question: \")\n",
    "    # what services bedrock platform is offering\n",
    "    #  what are the LLM models available from bedrock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd63e2-b02e-48b7-bd98-7f05921ff26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f09b212b-2d1d-4898-9de6-7f0bb36aeee6",
   "metadata": {},
   "source": [
    "## Documents RAG Integration with Bedrock Knowledge Bases Service\n",
    "\n",
    "In the section below, we use Amazon Bedrock Knowledge Bases to build the RAG framework. You can create a Bedrock Knowledge Base from the [AWS console](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-create.html) or follow this [notebook example](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/02_KnowledgeBases_and_RAG/0_create_ingest_documents_test_kb.ipynb) to create it programmatically. \n",
    "\n",
    "First, create a new S3 bucket for the Bedrock Knowledge Bases. Then, upload the previously downloaded files to this S3 bucket. You can select different embedding models and chunking strategies that work better for your data.\n",
    "\n",
    "Once the Knowledge Base is created, remember to sync the data. Data synchronisation may take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aaaa3e-4a9e-42b1-9111-82350dfaf1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After you create the knowledge base, provide Bedrock Knowledge Base ID \n",
    "knowledge_base_id = \"[KNOWLEDGE_BASE_ID]\" # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80cd10-23da-4d60-9e0e-83bbd5e30bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure a knowledge base retriever using AmazonKnowledgeBasesRetriever\n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "# maximum number of relevant text chunks that will be retrieved\n",
    "# If you need quick, focused answers: lower numbers (1-3)\n",
    "# If you need detailed, comprehensive answers: higher numbers (5-10)\n",
    "top_k = 10\n",
    "\n",
    "# search mode options: HYBRID, SEMANTIC\n",
    "# HYBRID search combines the strengths of semantic search and keyword search \n",
    "# Balances semantic understanding with exact matching\n",
    "# https://docs.llamaindex.ai/en/stable/examples/retrievers/bedrock_retriever/\n",
    "search_mode = \"HYBRID\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c6989-7989-4800-9b58-63154938d99f",
   "metadata": {},
   "source": [
    "#### Note ðŸ”¥:Â KB service role should have permission to bedrock:Rerank and invoke the rerank model\n",
    "- Create below IAM inline policy and attach it to Amazon Knowledge Base service role from AWS console \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"bedrock:InvokeModel\",\n",
    "            \"Resource\": \"arn:aws:bedrock:us-west-2::foundation-model/amazon.rerank-v1:0\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"bedrock:Rerank\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee8501-cfb5-4bb1-98eb-cf1a27156536",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=knowledge_base_id,\n",
    "    retrieval_config={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\": top_k,\n",
    "            \"overrideSearchType\": search_mode,\n",
    "            'rerankingConfiguration': {\n",
    "                'bedrockRerankingConfiguration': {\n",
    "                    'modelConfiguration': {\n",
    "                        'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.rerank-v1:0'\n",
    "                    },\n",
    "                    'numberOfRerankedResults': 3\n",
    "                },\n",
    "                'type': 'BEDROCK_RERANKING_MODEL'\n",
    "            }\n",
    "        },\n",
    "        \n",
    "    }\n",
    ")\n",
    "kb_engine = RetrieverQueryEngine(retriever=kb_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33839357-d390-4d2f-87d8-4190bf5159f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the knowledge base with a simple question \n",
    "response = kb_engine.query(\n",
    "    \"In which situation should I use Amazon Bedrock over Amazon SageMaker?\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e10c73-6813-4df8-b1fa-1b46a80d1dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a query tool for Bedrock Knowledge Base\n",
    "\n",
    "kb_tool = QueryEngineTool(\n",
    "        query_engine=kb_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"kb_tool\",\n",
    "            description=\"\"\"\n",
    "            These decision guides help users select appropriate AWS machine learning and generative AI services based on specific needs. \n",
    "            They cover pre-built solutions, customizable platforms, and infrastructure options for ML workflows, \n",
    "            while outlining how generative AI can automate processes, personalize content, augment data, reduce costs, \n",
    "            and enable faster experimentation in various business contexts.\"\"\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e721af-ae9b-4e6f-bea4-c21723012316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "current_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a technology expert with access to the GitHub API, arXiv API, DuckDuckGo API and  TechCrunch API. \n",
    "You can search for the latest code repositories, research papers, news related to technology, and search the Internet using DuckDuckGo API.\n",
    "You have access to the Amazon Bedrock user guide, which provides information about services offered by Bedrock, \n",
    "such as Agents, Knowledge Bases, Guardrails, Model Evaluation, and Model Fine-Tuning. \n",
    "It also provides third-party foundation models and Amazon LLMs via the Bedrock platform \n",
    "Always utilise the tools at your disposal.\n",
    "If you donâ€™t know the answer, do not make up any information; simply say: Sorry, I donâ€™t know.\n",
    "\n",
    "Current time is: {current_time}\n",
    "\"\"\"\n",
    "\n",
    "#Â Update the agent to include all API tools and the Knowledge Base tool.\n",
    "\n",
    "all_tools = api_tools +[kb_tool]\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    all_tools, \n",
    "    llm=llm, \n",
    "    verbose=True, # Set verbose=True to display the full trace of steps. \n",
    "    system_prompt = system_prompt,\n",
    "    # allow_parallel_tool_calls = True  #Â Uncomment this line to allow multiple tool invocations\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400e55c-55f6-405d-a486-ec14e78ab25c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple chatbot UI. Enter \"exit\" to quit. \n",
    "\n",
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    response = agent.chat(text_input)\n",
    "\n",
    "    print(\"-\" * 120)\n",
    "    print(\" \" * 120)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print(\"=\" * 120)\n",
    "    print(\" New question: \")\n",
    "    # what services bedrock platform is offering\n",
    "    #  what are the LLM models available from bedrock\n",
    "    # \"I don't have many ML experts, but I want to build a GenAI application. Which AWS service should I choose?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b898f6-e756-4136-90e7-dcbb7c234ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agent.memory.reset() # clear the chat memory\n",
    "# agent.memory.get() #Â retrieve conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc4fc4-8f64-461e-b9fe-688f6007d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Â Test question: \n",
    "# 1. I don't have many ML experts, but I want to build a GenAI application. Which AWS service should I choose?\n",
    "# 2. whats the benefits of using bedrock service\n",
    "# 3. can you give me top 5 git repos related to bedrock "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7796d-08d2-4dcb-b77d-cbd3f9acde8e",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "1. Navigate to the Amazon S3 console and delete the S3 bucket and data created for this solution.\n",
    "2. In the Amazon OpenSearch Service console, go to Serverless, select Collection, and delete the collection that was created for storing the embedding vectors. \n",
    "3. From the Amazon Bedrock Knowledge Bases console, select the knowledge base created in option 2 and delete it. \n",
    "4. In the Amazon SageMaker console, select your domain and user profile. Launch SageMaker Studio to stop or delete the JupyterLab instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad478b4b-350c-47b5-a31f-2ebbbc864935",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook shows how we can combine LLMs (Mistral Large 2), internet searching tools, and knowledge bases to build an intelligent research helper. We can see how this solution works well for finding and understanding technical information, and it can easily be made more powerful by adding more data sources and features."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
