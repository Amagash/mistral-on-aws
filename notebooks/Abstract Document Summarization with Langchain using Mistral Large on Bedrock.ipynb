{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfda1339-68fe-416d-b3b8-6349872524b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Abstract Document Summarization with Langchain using Mistral Large on Bedrock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353a3bf-98e8-4eb2-a24d-05dd210e6b3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "This notebook is meant to demonstrate using the [Mistral models](https://docs.mistral.ai/deployment/cloud/aws/) on Amazon Bedrock for Abstract document summarization tasks. Although all the Mistral models have relatively large context window sizes, when working with multiple large documents, there are several challenges that can arise. One of the main challenges is that the input text might exceed the model's context length. This limitation can lead to incomplete or inaccurate responses, as the model may not have access to all the relevant information within the document. Another challenge is that language models can sometimes hallucinate or generate factually incorrect responses when dealing with very long documents. This can happen because the model may lose track of the overall context or make incorrect inferences based on partial information. Additionally, processing large documents can lead to out-of-memory errors, especially on resource-constrained systems or when working with large language models that have high memory requirements.\n",
    "\n",
    "To address these challenges, this notebook will go through various summarization strategies that will use [LangChain](https://python.langchain.com/docs/get_started/introduction.html), a popular framework for developing applications powered by large language models (LLMs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3664e61-3f39-4229-9cf6-26ee090a8608",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Mistral Model Selection\n",
    "\n",
    "Today, there are three Mistral models available on Amazon Bedrock:\n",
    "\n",
    "### 1. Mistral 7B Instruct\n",
    "\n",
    "- **Description:** A 7B dense Transformer model, fast-deployed and easily customizable. Small yet powerful for a variety of use cases.\n",
    "- **Max Tokens:** 8,196\n",
    "- **Context Window:** 32K\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 2. Mixtral 8X7B Instruct\n",
    "\n",
    "- **Description:** A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Utilizes 12B active parameters out of 45B total.\n",
    "- **Max Tokens:** 4,096\n",
    "- **Context Window:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 3. Mistral Large\n",
    "\n",
    "- **Description:** A cutting-edge text generation model with top-tier reasoning capabilities. It can be used for complex multilingual reasoning tasks, including text understanding, transformation, and code generation.\n",
    "- **Max Tokens:** 8,196\n",
    "- **Context Window:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Synthetic Text Generation, Code Generation, RAG, or Agents\n",
    "\n",
    "### Performance and Cost Trade-offs\n",
    "\n",
    "The table below compares the model performance on the Massive Multitask Language Understanding (MMLU) benchmark and their on-demand pricing on Amazon Bedrock.\n",
    "\n",
    "| Model           | MMLU Score | Price per 1,000 Input Tokens | Price per 1,000 Output Tokens |\n",
    "|-----------------|------------|------------------------------|-------------------------------|\n",
    "| Mistral 7B Instruct | 62.5%      | \\$0.00015                    | \\$0.0002                      |\n",
    "| Mixtral 8x7B Instruct | 70.6%      | \\$0.00045                    | \\$0.0007                      |\n",
    "| Mistral Large | 81.2%      | \\$0.008                   | \\$0.024                     |\n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Mistral Model Selection Guide](https://docs.mistral.ai/guides/model-selection/)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce6389-42ff-405e-8c3b-1855c9db22cf",
   "metadata": {},
   "source": [
    "### Local Setup (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed96c9c-58c0-49a8-a032-8b547aa03419",
   "metadata": {
    "tags": []
   },
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a29e7-6312-4ac6-a338-a33cbd83b084",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f67cb8-1a0c-416c-a8c8-66814a52b72c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose ml.t3.medium.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f52c7-7500-4cd6-be0d-920374622dfd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e4415-8fbb-46ac-92e2-3ebcc4888153",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "Before we start building the agentic workflow, we'll first install some libraries:\n",
    "\n",
    "+ AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "+ [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) is a framework that provides off the shelf components to make it easier to build applications with large language models. It is supported in multiple programming languages, such as Python, JavaScript, Java and Go. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b1ce28-362c-44a7-bbde-fec066fea4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.14\n",
    "boto3==1.34.58\n",
    "botocore==1.34.93\n",
    "sqlalchemy==2.0.29\n",
    "pypdf==4.1.0\n",
    "langchain-aws==0.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ad3619-b8c3-49d3-9175-be3013c079e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.41.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -r requirements.txt --quiet\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b8017-48a6-4369-a556-fd0ecdbd80f8",
   "metadata": {},
   "source": [
    "#### Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d653b27c-dc6f-4d01-a4c5-3249540c19a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c26578-567b-4629-a081-698ff82533fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c109e15-ad6f-4a53-a077-5eae296c67ec",
   "metadata": {},
   "source": [
    "\n",
    "## Initiate the Bedrock Client\n",
    "\n",
    "Import the necessary libraries, along with langchain for bedrock model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ee536a3-1cb4-4bab-a8a0-6e41023c9cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import botocore.config\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "#import tiktoken\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac5aff5b-4b38-48ea-b5ab-5bebc44b1d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "\n",
    "config = Config(read_timeout=2000)\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', \n",
    "                    region_name='us-west-2',\n",
    "                     config=config)\n",
    "#bedrock = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876864d8-d99b-45a3-b7e5-c8824ad8e9f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> Ensure that you have access to the Mistral model you wish to use through Bedrock.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61231ed9-0cac-4fee-b932-059fc253bbf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure LangChain with Boto3\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "With LangChain, you can access Bedrock once you pass the boto3 session information to LangChain. Below, we also specify Mistral Large in `model_id` and pass Mistral's inference parameters as desired in `model_kwargs`.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Supported parameters\n",
    "\n",
    "The Mistral AI models have the following inference parameters.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"max_tokens\" : int,\n",
    "    \"stop\" : [string],    \n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"top_k\": int\n",
    "}\n",
    "```\n",
    "\n",
    "The Mistral AI models have the following inference parameters:\n",
    "\n",
    "- **Temperature** - Tunes the degree of randomness in generation. Lower temperatures mean less random generations.\n",
    "- **Top P** - If set to float less than 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "- **Top K** - Can be used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.\n",
    "- **Maximum Length** - Maximum number of tokens to generate. Responses are not guaranteed to fill up to the maximum desired length.\n",
    "- **Stop sequences** - Up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a841ed0-eab4-4921-87b7-a2d07b961405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set the desired mistral model as the default model\n",
    "instruct_mistral7b_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "instruct_mixtral8x7b_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "mistral_large_2402_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "\n",
    "DEFAULT_MODEL = mistral_large_2402_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d7901c8-7a58-4c98-bdeb-db0dc4d04dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "llm = ChatBedrock(\n",
    "    model_id=DEFAULT_MODEL,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 8192,  ## MAXIMUM NUMBER OF TOKENS for Mistral Large\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 1\n",
    "    },\n",
    "    client=bedrock,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38569ac6-5fad-4162-b0c0-c624e742ecb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello! It's a pleasure to meet you. I'm here to provide information, answer questions, and engage in friendly conversation. How can I assist you today? Remember, I'm here to provide lots of specific details from my context, so feel free to ask about a wide range of topics. If I don't know the answer, I'll be honest and let you know. Let's have a great conversation!\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=False, memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8307f-d16d-427a-b9e7-d7deac5cdb05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816dab8e-af09-456d-b6f1-4b11d33b5644",
   "metadata": {},
   "source": [
    "## Document Processing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23f76d-0a55-490e-8060-a6e821b8eb99",
   "metadata": {},
   "source": [
    "In this example, to demonstrate summarization, we will be using two documents that are both whitepapers from AWS. \n",
    "\n",
    "> The first document is a [whitepaper](https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf) on architecting HIIPA compliant workloads on AWS.\n",
    "\n",
    "> The second document is a [whitepaper](https://docs.aws.amazon.com/whitepapers/latest/containers-on-aws/containers-on-aws.pdf) about containers on AWS. \n",
    "\n",
    "Let's first download these files to build our document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3c71e6ec-8500-404c-9dc3-d81b338fcd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf',\n",
    "    'https://docs.aws.amazon.com/whitepapers/latest/containers-on-aws/containers-on-aws.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AWS-security-whitepaper.pdf',\n",
    "    'AWS-containers-whitepaper.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2023, source=filenames[0]),\n",
    "    dict(year=2023, source=filenames[1])\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2214de-07e2-4958-bf06-1339048abceb",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of `DirectoryLoader` from `PyPDF` available under LangChain and splitting them into smaller chunks.\n",
    "\n",
    "Note: For the sake of this use-case we are creating chunks of roughly 4000 characters with an overlap of 100 characters using `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d8742-23aa-4814-8e15-1c720ff9234b",
   "metadata": {},
   "source": [
    "#### HIIPA Compliance document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "930338ac-1950-4e1b-8388-8972ca92e4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='AWS Whitepaper\\nArchitecting for HIPAA Security and \\nCompliance on Amazon Web Services\\nCopyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.' metadata={'year': 2023, 'source': 'AWS-security-whitepaper.pdf'}\n",
      "\n",
      "Number of documents chunked and created from the HIPAA Security document: 152\n"
     ]
    }
   ],
   "source": [
    "#document 1 (HIPAA COMPLIANCE ON AWS)\n",
    "import numpy as np\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "hipaa_documents = []\n",
    "\n",
    "# Load only the first file\n",
    "hipaa_file = filenames[0]\n",
    "hipaa_loader = PyPDFLoader(data_root + hipaa_file)\n",
    "hipaa_document = hipaa_loader.load()\n",
    "\n",
    "for idx, hipaa_document_fragment in enumerate(hipaa_document):\n",
    "    hipaa_document_fragment.metadata = metadata[0] if metadata else {}\n",
    "    hipaa_documents.append(hipaa_document_fragment)\n",
    "    \n",
    "#chunking\n",
    "hipaa_doc_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a  small chunk size, just to show.\n",
    "    chunk_size=4000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "hipaa_docs = hipaa_doc_text_splitter.split_documents(hipaa_documents)\n",
    "print(hipaa_docs[0])\n",
    "\n",
    "#chunked doc count\n",
    "hipaa_chunked_count = len(hipaa_docs)\n",
    "print(\n",
    "    f\"\\nNumber of documents chunked and created from the HIPAA Security document: {hipaa_chunked_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d759ea-f15f-4acd-a60d-15abc519135f",
   "metadata": {},
   "source": [
    "#### Containers on AWS Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "12233e31-3079-4128-8ff2-0e79d07e2461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Containers on AWS AWS Whitepaper\\nContainers on AWS: AWS Whitepaper\\nCopyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.\\nAmazon's trademarks and trade dress may not be used in connection with any product or service \\nthat is not Amazon's, in any manner that is likely to cause confusion among customers, or in any \\nmanner that disparages or discredits Amazon. All other trademarks not owned by Amazon are \\nthe property of their respective owners, who may or may not be aﬃliated with, connected to, or \\nsponsored by Amazon.\" metadata={'year': 2023, 'source': 'AWS-security-whitepaper.pdf'}\n",
      "\n",
      "Number of documents chunked and created from the original: 57\n"
     ]
    }
   ],
   "source": [
    "#document 2 (Containers on AWS)\n",
    "import numpy as np\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "container_documents = []\n",
    "\n",
    "# Load only the second file\n",
    "container_file = filenames[1]\n",
    "container_loader = PyPDFLoader(data_root + container_file)\n",
    "container_document = container_loader.load()\n",
    "\n",
    "for idx, container_document_fragment in enumerate(container_document):\n",
    "    container_document_fragment.metadata = metadata[0] if metadata else {}\n",
    "    container_documents.append(container_document_fragment)\n",
    "    \n",
    "#chunking\n",
    "container_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "container_docs = container_text_splitter.split_documents(container_documents)\n",
    "print(container_docs[1])\n",
    "\n",
    "#chunked doc count\n",
    "container_chunked_count = len(container_docs)\n",
    "print(\n",
    "    f\"\\nNumber of documents chunked and created from the original: {container_chunked_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a712e-99d8-403d-85a2-cfa048ceaabf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d90c42-7a08-447d-8143-10571c411c90",
   "metadata": {},
   "source": [
    "## Summarizing Long Documents with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34410f8c-6d6e-4163-9428-7481abd02ef6",
   "metadata": {},
   "source": [
    "In the following sections, we will go over three different summarization techniques with LangChain:\n",
    "    \n",
    " #####   1. Stuff\n",
    " #####   2. Map Reduce\n",
    " #####   3. Refine\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c27632-85ba-46cf-bd43-763e328a8001",
   "metadata": {},
   "source": [
    "### 1. Stuff with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b9890-02bf-4927-8fbb-fea029ddc24c",
   "metadata": {},
   "source": [
    "Stuffing is the simplest method to pass data to a language model. It \"stuffs\" text into the prompt as context in a way that all of the relevant information can be processed by the model to get what you want.\n",
    "\n",
    "In LangChain, you can use `StuffDocumentsChain` as part of the `load_summarize_chain` method. What you need to do is set `stuff` as the `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4175ddb7-9a5a-4c9f-8524-0f36a98c8e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "stuff_summary_chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c9859-f01f-4b60-8139-a9b3dd2d0fb2",
   "metadata": {},
   "source": [
    "Next, let's take a look at the Prompt template used by the Stuff summarize chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c4690f8f-4883-4291-829c-030374ac3b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afeaa3-21ff-4dd0-b9d5-d691e86e2a77",
   "metadata": {},
   "source": [
    "Here, we see that by default, the Prompt template for `llm_chain` has been set to: 'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'\n",
    "\n",
    "This can be altered by instantiating using `from_template` with LangChain to set a new prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "41262dc2-3d6e-4296-9954-49edb989ec7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "stuff_prompt = PromptTemplate.from_template('Write a detailed and complete summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nDETAILED SUMMARY:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "469f2796-0dd6-4fc5-9101-fbcdc1e782d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template = stuff_prompt.template #set new prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf5e04-8189-4294-8927-5ad4280b82ba",
   "metadata": {},
   "source": [
    "Now that we have set the new prompt template, let us first try generating a summary of the **Containers on AWS** whitepaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4232059e-ef40-49b1-9290-14f38f5b1522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    stuff_container_summary = stuff_summary_chain.run(container_docs) # (ValidationException error) prompt over 32k window length / number of tokens exceeds window size \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "110c7395-0a40-4f77-8d90-95467aff5d0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AWS Whitepaper \"Containers on AWS\" provides guidance and options for running containers on Amazon Web Services (AWS). Containers offer a way to develop, ship, and run applications in an isolated environment. AWS complements containers by offering scalable orchestration and infrastructure services for their deployment. The paper covers container orchestration and compute options such as AWS App Runner, Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), and AWS Fargate. It also discusses key considerations for container workloads on AWS, including container runtime, container-enabled AMIs, compute options, scheduling, container repositories, observability, storage, networking, security, build and deploy automation, infrastructure as code and platform automation, and scaling. The paper also mentions the AWS Well-Architected Framework, which helps users understand the pros and cons of decisions when building systems in the cloud.\n"
     ]
    }
   ],
   "source": [
    "print(stuff_container_summary.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b054f-e936-4595-825d-ebab6019dfc4",
   "metadata": {},
   "source": [
    "From the cell ouput above, we can see that since Stuffing only requires a single call to the LLM, it can be faster than other methods that require multiple calls. When summarizing text, the model has access to all the data at once, which can result in a better summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a33dffb-c033-49a2-bf2f-6af050d01160",
   "metadata": {},
   "source": [
    "Next, let us use the **HIPAA and Security Compliance** on AWS whitepaper to see how the model deals with summarization using the `StuffDocumentChain` when presented with a longer document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d4b9f80d-bc8b-4850-bcb8-dd0a76df371d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: This model's maximum context length is 32768 tokens. Please reduce the length of the prompt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stuff_hipaa_summary = stuff_summary_chain.run(hipaa_docs) # (ValidationException error) prompt over 32k window length / number of tokens exceeds window size \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cbe28-64c6-4302-8c42-deba21fa516f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes:\n",
    "In the output for the above cell, we see that an error is raised due to the prompt far exceeding the model's maximum context length. Since stuffing summarizes text by feeding the entire document to a large language model (LLM) in a single call, it is difficult to process long documents. The Mistral models have a context length of 32k tokens, which is the maximum number of tokens that can be processed in a single call. If the document is longer than the context length, stuffing will not work. Also the stuffing method is not suitable for summarizing large documents, as it can be slow and may not produce a good summary.\n",
    "\n",
    "Let's explore a couple chunk-wise summarization techniques with [LangChain](https://python.langchain.com/docs/get_started/introduction.html) to be able to mitigate the restrictions of your large documents not fitting into the context window of the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b1252-171a-43c7-a184-41fc47e1b836",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Map Reduce with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef8232-dc84-44c8-8124-4f4ff7e69f64",
   "metadata": {},
   "source": [
    "The `Map-Reduce` method involves summarizing each document individually (map step) and then combining these summaries into a final summary (reduce step). This approach is more scalable and can handle larger volumes of text.The map_reduce technique is designed for summarizing large documents that exceed the token limit of the language model. It involves dividing the document into chunks, generating summaries for each chunk, and then combining these summaries to create a final summary. This method is efficient for handling large files and significantly reduces processing time.\n",
    "\n",
    "In LangChain, you can use `MapReduceDocumentsChain` as part of the `load_summarize_chain method`. What you need to do is set `map_reduce` as the `chain_type` of your chain.\n",
    "\n",
    "---\n",
    "\n",
    "![map-reduce](imgs/mapreduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "103e086a-d021-4357-901b-5670731087e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain, it then combines and iteratively reduces the mapped document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "map_reduce_summary_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ec2137d7-1d09-4439-bb6b-1804996d3290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReduceDocumentsChain(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='\\n                      Write a detailed summary of the following text delimited by triple backquotes.\\n                      Return your response in bullet points which covers the key points of the text.\\n                      ```{text}```\\n                      BULLET POINT SUMMARY:\\n                      '), llm=ChatBedrock(client=<botocore.client.BedrockRuntime object at 0x7fedea591de0>, model_id='mistral.mistral-large-2402-v1:0', model_kwargs={'max_tokens': 8192, 'temperature': 0.5, 'top_p': 1})), document_variable_name='text'))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_summary_chain.reduce_documents_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae04456-08d3-4f3e-8491-e37d736f18db",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `ReduceDocumentsChain` handles taking the document mapping results and reducing them into a single output. It wraps a generic `CombineDocumentsChain` (like `StuffDocumentsChain`) but adds the ability to collapse documents before passing it to the `CombineDocumentsChain` if their cumulative size exceeds token_max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6e8de7ca-64d6-4720-9278-a477638b4b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiation using from_template (recommended)\n",
    "\n",
    "#this sets the prompt template for the summaries generated for all the individual document chunks.\n",
    "initial_map_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.llm_chain.prompt.template = initial_map_prompt.template\n",
    "\n",
    "#sets the prompt template for generating a cumulative summary of all the document chunks for reduce documents chain.\n",
    "reduce_documents_prompt= PromptTemplate.from_template(\"\"\"\n",
    "                      Write a detailed summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt.template = reduce_documents_prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9794d-7ff2-496c-8998-2801b1846a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Map-Reduce`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Map-Reduce works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5c76502a-13c8-4501-9544-d957ec56bdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    map_reduce_summary = map_reduce_summary_chain.run(hipaa_docs[50:70])  \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "334fad6e-1ded-4cb3-8da2-1b6bd534b115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The text discusses the requirements for HIPAA security and compliance on Amazon Web Services (AWS).\n",
      "            - Amazon Elastic File System (EFS) provides two options for encrypting Protected Health Information (PHI) at rest: enabling encryption while creating a new file system, or encrypting data before placing it on EFS. It is recommended not to use PHI as part of any file name or folder name.\n",
      "            - For encrypting PHI in transit on Amazon EFS, Transport Layer Security (TLS) is provided between the EFS service and the instance mounting the file system.\n",
      "            - Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that simplifies running Kubernetes on AWS.\n",
      "            - Amazon ElastiCache for Redis is an in-memory data structure service that can be used as a data store or cache, and it is compatible with Redis. To store PHI, customers must use the latest HIPAA-eligible ElastiCache for Redis engine version and current generation node types.\n",
      "            - Customers are advised to configure their clusters and nodes to encrypt data at rest, enable transport encryption, and authenticate Redis commands. Amazon ElastiCache for Redis provides data encryption for its cluster to protect data at rest.\n",
      "            - Amazon ElastiCache for Redis uses Transport Layer Security (TLS) to encrypt data in transit.\n",
      "            - Amazon OpenSearch Service allows customers to run a managed OpenSearch or legacy Elasticsearch OSS cluster in a dedicated Amazon Virtual Private Cloud (Amazon VPC). When using OpenSearch Service with PHI, customers should use OpenSearch or Elasticsearch 6.0 or later and ensure that PHI is encrypted at-rest and in-transit.\n",
      "            - Amazon EventBridge, a serverless event bus, delivers a stream of real-time data from various event sources and is encrypted using 256-bit Advanced Encryption Standard (AES-256) for security purposes.\n",
      "            - Amazon Forecast is a fully managed service that uses machine learning to deliver highly accurate forecasts, with every interaction with the service being protected by encryption.\n",
      "            - Amazon FSx is a fully-managed service that provides feature-rich and high-performance file systems, offering two services: Amazon FSx for Windows File Server and Amazon FSx for Lustre.\n",
      "            - Amazon GuardDuty is a managed threat detection service that continuously monitors for malicious or unauthorized behavior to help protect AWS accounts and workloads.\n",
      "            - Amazon HealthLake is a service that allows customers in the healthcare and life sciences industries to store, transform, query, and analyze health data at petabyte scale. This service can be used to transmit, process, and store PHI.\n",
      "            - Amazon Inspector is an automated security assessment service for applications deployed on AWS, assessing them for vulnerabilities or deviations from best practices. Customers can run Amazon Inspector on EC2 instances that contain Personal Health Information (PHI).\n",
      "            - Amazon Managed Service for Apache Flink allows customers to quickly author SQL code that continuously reads, processes, and stores data in near real time.\n",
      "            - Amazon Kinesis Data Streams encrypts data using an AWS Key Management Service (KMS) key before storing it at rest. Amazon Kinesis Video Streams is a fully managed AWS service that allows customers to stream live video from devices to the AWS Cloud and can also be used to build applications for real-time video processing.\n",
      "            - Amazon Lex is an AWS service for building conversational interfaces for applications using voice and text. Amazon Lex uses the HTTPS protocol to communicate both with clients and other AWS services, and access to Lex is API-driven with the ability to enforce appropriate Identity and Access Management (IAM) least privilege.\n",
      "            - Amazon Managed Streaming for Apache Kafka (Amazon MSK) offers encryption features for data at rest and in-transit. For data at rest encryption, Amazon MSK cluster utilizes Amazon EBS server-side encryption and AWS KMS keys.\n",
      "            - Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. To access brokers, the following protocols with Transport Layer Security (TLS) enabled should be used: AMQP, MQTT, MQTT over WebSocket, OpenWire, and STOMP.\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_summary.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb754c4-4963-4d59-a816-cfa3363c7432",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "With `Map-Reduce`, the model is able to summarize a large document by overcoming the context limit of Stuffing method with parallel processing. \n",
    "However, it requires multiple calls to the model and potentially loses context between individual summaries of the chunks. To deal with this challenge, let us try another method that performs chunk-wise summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c6d9a-7b92-44fc-88c1-aaba42176117",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a9d41-399a-4e9b-928e-97afb4bc68f8",
   "metadata": {},
   "source": [
    "### 3. Refine with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f8a89-f7e2-411f-b898-a454daf05a29",
   "metadata": {},
   "source": [
    "The `Refine` method iteratively updates its answer by looping over the input documents. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer. This method is useful for refining a summary based on new context.`Refine` is a simpler alternative to `Map-Reduce` technique. It involves generating a summary for the first chunk, combining it with the second chunk, generating another summary, and continuing this process until a final summary is achieved. This method is suitable for large documents but requires less complexity compared to `Map-Reduce`.\n",
    "\n",
    "---\n",
    "\n",
    "![map-reduce](imgs/refine.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b8df1a4d-6ab0-4a2a-bed4-228973c532ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "refine_summary_chain = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=False)\n",
    "refine_summary_chain_french = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f34f0474-0789-4ab0-b6a4-b994ce400ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RefineDocumentsChain(initial_llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['text'], template='\\n                      Write a summary of this chunk of text that includes the main points and any important details.\\n                      {text}\\n                      '), llm=ChatBedrock(client=<botocore.client.BedrockRuntime object at 0x7fedea591de0>, model_id='mistral.mistral-large-2402-v1:0', model_kwargs={'max_tokens': 8192, 'temperature': 0.5, 'top_p': 1})), refine_llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['existing_answer', 'text'], template=\"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\"), llm=ChatBedrock(client=<botocore.client.BedrockRuntime object at 0x7fedea591de0>, model_id='mistral.mistral-large-2402-v1:0', model_kwargs={'max_tokens': 8192, 'temperature': 0.5, 'top_p': 1})), document_variable_name='text', initial_response_name='existing_answer')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_summary_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f02e05ef-8a19-415c-ac84-41bac282f1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initial llm chain prompt template\n",
    "initial_refine_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "refine_summary_chain.initial_llm_chain.prompt.template = initial_refine_prompt.template\n",
    "\n",
    "#refine llm chain prompt template\n",
    "refine_documents_prompt= PromptTemplate.from_template(\"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\")\n",
    "\n",
    "refine_summary_chain.refine_llm_chain.prompt.template = refine_documents_prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0c54fca3-b462-4468-8479-fd0bfe22a021",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\""
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_summary_chain.refine_llm_chain.prompt.template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "de58d720-bccb-4bee-b836-020d6ddf56ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    refine_summary = refine_summary_chain.run(hipaa_docs[50:71])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "0584649c-fd50-4527-a04f-8c4f1a4d25ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text discusses the measures to ensure HIPAA security and compliance on Amazon Web Services (AWS), focusing on various services including Amazon Elastic File System (EFS), Amazon Elastic Kubernetes Service (Amazon EKS), Amazon ElastiCache for Redis, Amazon OpenSearch Service, Amazon EMR, Amazon EventBridge, Amazon Forecast, Amazon FSx, Amazon GuardDuty, Amazon HealthLake, Amazon Inspector, and Amazon Managed Service for Apache Flink.\n",
      "\n",
      "For EFS and Amazon FSx, the text outlines methods for encrypting Protected Health Information (PHI) at-rest, with the recommendation to enable encryption during the creation of a new file system. For PHI in transit, encryption is provided by Transport Layer Security (TLS) for EFS and supported by Amazon FSx on compatible instances.\n",
      "\n",
      "Amazon EKS is a managed service that simplifies running Kubernetes on AWS, with security and compliance details available in the Architecting for HIPAA Security and Compliance on Amazon EKS whitepaper. Amazon ElastiCache for Redis can store PHI if customers ensure they are running the latest HIPAA-eligible ElastiCache for Redis engine version and current generation node types, with encryption at rest and in transit enabled.\n",
      "\n",
      "Amazon OpenSearch Service enables customers to run a managed OpenSearch or legacy Elasticsearch OSS cluster in a dedicated Amazon Virtual Private Cloud (Amazon VPC). When using OpenSearch Service with PHI, customers should use OpenSearch or Elasticsearch 6.0 or later, ensuring PHI is encrypted at-rest and in-transit. Node-to-node encryption should be enabled, and any PHI that enters an Amazon OpenSearch Service cluster should be sent over HTTPS.\n",
      "\n",
      "For Amazon EMR, the text refers to the Encryption Options for more information. Amazon EventBridge encrypts data by default using 256-bit Advanced Encryption Standard (AES-256) under an AWS owned CMK. Amazon Forecast is a fully managed service that uses machine learning to deliver highly accurate forecasts, with every interaction encrypted and any content processed encrypted with customer keys through Amazon Key Management Service.\n",
      "\n",
      "Amazon GuardDuty is a managed threat detection service that continuously monitors for malicious or unauthorized behavior to help customers protect their AWS accounts and workloads. However, Amazon GuardDuty should not encounter any PHI as this data is not to be stored in any of the AWS based data sources listed above.\n",
      "\n",
      "Amazon HealthLake enables customers in the healthcare and life sciences industries to store, transform, query, and analyze health data at petabyte scale, encrypting data at rest in customer’s data stores by default. All service data and metadata is encrypted with a service owned KMS key. Amazon HealthLake encrypts data both in transit and at rest, integrating with AWS CloudTrail to capture all API calls to the service as events.\n",
      "\n",
      "Amazon Inspector is an automated security assessment service that helps customers improve the security and compliance of applications deployed on AWS. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity. Customers may run Amazon Inspector on EC2 instances that contain PHI. Amazon Inspector encrypts all data transmitted over the network as well as all telemetry data stored at-rest.\n",
      "\n",
      "Amazon Managed Service for Apache Flink enables customers to quickly author SQL code that continuously reads, processes, and stores data in near real time. It integrates with both AWS CloudTrail and Amazon CloudWatch Logs for application monitoring. If the stream is encrypted, Managed Service for Apache Flink accesses the data in the encrypted stream seamlessly with no further configuration needed. It does not store unencrypted data read from Kinesis Data Streams.\n",
      "\n",
      "Amazon Kinesis Data Streams encrypts data using an AWS KMS key before storing it at-rest and decrypts it after retrieval. For Kinesis Video Streams, server-side encryption automatically encrypts data at rest using an AWS KMS key specified by the customer. The Amazon Kinesis Video Streams SDK uses TLS to encrypt frames and fragments. Amazon Kinesis Video Streams uses AWS CloudTrail to log all API calls.\n",
      "\n",
      "Amazon Lex is a service for building conversational interfaces using voice and text. It uses the HTTPS protocol to communicate with clients and other AWS services, and access to Lex is API-driven, allowing for appropriate IAM least privilege enforcement. Customers can monitor Amazon Lex using Amazon CloudWatch, getting metrics for individual Amazon Lex operations or for global Amazon Lex operations for their account. Customers can also set up CloudWatch alarms to be notified when one or more metrics exceeds a threshold that they define. Amazon Lex is also integrated with AWS CloudTrail to log Lex API calls.\n",
      "\n",
      "Amazon Managed Streaming for Apache Kafka (Amazon MSK) provides encryption features for data at rest and for data in-transit. For data at rest encryption, Amazon MSK cluster uses Amazon EBS server-side encryption and AWS KMS keys to encrypt data volumes and automated snapshots. For data in-transit, Amazon MSK clusters have encryption enabled via TLS for inter-broker communication. The encryption configuration setting is enabled when a cluster is created. Also, by default, in-transit encryption is set to TLS for clusters created from CLI or AWS Console. Additional configuration is required for clients to communicate with clusters using TLS encryption. Customers can change the default encryption setting by selecting the TLS/plaintext settings. Amazon MSK does not support public endpoints, ensuring that data can only be produced and consumed from an Amazon MSK cluster over a private connection between their clients in their VPC and the Amazon MSK cluster.\n",
      "\n",
      "Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. It works with existing applications and services without the need for a customer to manage, operate, or maintain their own messaging infrastructure. To provide the encryption of PHI data while in transit, the following protocols with TLS enabled should be used to access brokers: AMQP, MQTT, MQTT over WebSocket, OpenWire, STOMP. Amazon MQ encrypts messages at-rest and in transit using encryption keys that it manages and stores securely. Amazon MQ uses CloudTrail to log all API calls.\n",
      "\n",
      "Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Data containing PHI can now be retained in an encrypted instance of Amazon Neptune. An encrypted instance of Amazon Neptune can be specified only at the time of creation by choosing ‘Enable Encryption’ from the Amazon Neptune console. All logs, backups, and snapshots are encrypted for an Amazon Neptune encrypted instance. Key management for encrypted instances of Amazon Neptune is provided through the AWS KMS. Encryption of data in transit is provided through SSL/TLS. Amazon Neptune uses CloudTrail to log all API calls.\n",
      "\n",
      "AWS Network Firewall is a managed firewall service that makes it easy to deploy essential network protections for all your Amazon Virtual Private Cloud (Amazon VPC). The service automatically scales with network traffic volume to provide high-availability protections without the need to set up or maintain the underlying infrastructure. Both customer rules and access logs may contain end user IP addresses, which are encrypted both at rest and in transit within the AWS architecture. Furthermore, AWS Network Firewall encrypts all data at rest and in transit between component AWS services (Amazon S3, Amazon DynamoDB, Amazon CloudWatch Logs, Amazon EBS).\n"
     ]
    }
   ],
   "source": [
    "print(refine_summary.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbf269-684d-43ea-9519-3aca9373aea9",
   "metadata": {},
   "source": [
    "---\n",
    "Now that we have seen how the `refine` document chain constructs a response, let us try altering the refine_llm_chain prompt template to help highlight some of the multilingual capabilties of the [Mistral Large](https://mistral.ai/news/mistral-large/) model. Mistral Large demonstrates superior capabilities in handling multi-lingual tasks. Mistral-large has been specifically trained to understand and generate text in multiple languages, especially in French, German, Spanish, and Italian. This can be especially valuable for businesses and users that need to communicate in multiple languages. In the below cell, we set the `refine llm chain` prompt template to return the final summary in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b3e0d891-1522-44c1-b1eb-dc24aea5c1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#refine llm chain prompt template\n",
    "refine_documents_prompt_french= PromptTemplate.from_template(\"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary in French\"\n",
    "    \"If the context isn't useful, return the original summary.\")\n",
    "\n",
    "refine_summary_chain_french.refine_llm_chain.prompt.template = refine_documents_prompt_french.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7c15ebb8-0db7-42d0-a712-83a0840f24f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    refine_summary_french = refine_summary_chain_french.run(hipaa_docs[50:71])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "76d6cd69-38a5-415d-bb20-36e0420bbfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le texte traite des mesures visant à garantir la sécurité et la conformité HIPAA sur Amazon Web Services (AWS), en se concentrant sur divers services tels qu'Amazon Elastic File System (EFS), Amazon Elastic Kubernetes Service (Amazon EKS), Amazon ElastiCache for Redis, Amazon OpenSearch Service, Amazon EventBridge, Amazon Forecast, Amazon FSx, Amazon GuardDuty, Amazon HealthLake, Amazon Inspector, Amazon Managed Service for Apache Flink, Amazon Kinesis Streams, Amazon Data Firehose, Amazon Kinesis Video Streams, Amazon Managed Streaming for Apache Kafka (Amazon MSK), Amazon MQ et Amazon Lex. Il souligne l'importance de surveiller et de journaliser toutes les activités liées aux PHI sur AWS à l'aide d'outils tels qu'AWS CloudTrail et Amazon CloudWatch.\n",
      "\n",
      "Pour chaque service, le texte insiste sur les méthodes de chiffrement des PHI au repos et en transit, ainsi que sur les meilleures pratiques pour garantir la conformité HIPAA. Le texte mentionne également l'utilisation d'Amazon Neptune, un service de base de données de graphes entièrement géré, pour stocker et traiter des données hautement connectées contenant des PHI. Les données contenant des PHI peuvent être conservées dans une instance chiffrée d'Amazon Neptune, avec un chiffrement activé au moment de la création. Tous les journaux, sauvegardes et instantanés sont chiffrés pour une instance Amazon Neptune chiffrée. La gestion des clés pour les instances chiffrées d'Amazon Neptune est assurée par AWS KMS. Le chiffrement des données en transit est fourni via SSL/TLS. Amazon Neptune utilise CloudTrail pour journaliser tous les appels d'API.\n",
      "\n",
      "En outre, AWS Network Firewall est un service de pare-feu géré qui facilite le déploiement de protections réseau essentielles pour tous les Amazon Virtual Private Cloud (Amazon VPC). Le service chiffre toutes les données au repos et en transit entre les services AWS composants (Amazon S3, Amazon DynamoDB, Amazon CloudWatch Logs, Amazon EBS). Les clients doivent toujours s'assurer que les PHI sont chiffrés au repos et en transit, et activer le chiffrement de nœud à nœud lorsque cela est applicable.\n",
      "\n",
      "Enfin, le texte met en garde contre l'utilisation de PHI dans le cadre d'un nom de fichier ou de dossier pour tous les services AWS. La surveillance est importante pour maintenir la fiabilité, la disponibilité et les performances des services AWS des clients. Pour suivre la santé des services AWS, les clients peuvent utiliser Amazon CloudWatch. Avec CloudWatch, les clients peuvent obtenir des métriques pour les opérations AWS individuelles ou pour l'ensemble des opérations AWS. Les clients peuvent également configurer des alarmes CloudWatch pour être avertis lorsqu'une ou plusieurs métriques dépassent un seuil défini par les clients.\n"
     ]
    }
   ],
   "source": [
    "print(refine_summary_french.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe47f1-a1cc-4a06-97ff-6494256f8ef7",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "`Refine` has the potential to incorporate more relevant context compared to `Map-Reduce`, potentially resulting in a more comprehensive and accurate summary. However, it comes with a trade-off: `Refine` necessitates a significantly higher number of calls to the LLM than the `Stuff`. Moreover, these calls are not independent, which means they cannot be parallelized, potentially leading to longer processing times. Another consideration is that the Refine method may exhibit recency bias, where the most recent documents in the sequence could carry more weight or influence in the final summary, as the method processes documents in a specific order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca227dd3-5b63-4fd7-bf48-38bd36252a79",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Mistral AI\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
