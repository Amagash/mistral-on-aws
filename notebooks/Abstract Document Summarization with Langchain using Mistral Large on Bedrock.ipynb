{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfda1339-68fe-416d-b3b8-6349872524b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Abstract Document Summarization with Langchain using Mistral Large on Bedrock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353a3bf-98e8-4eb2-a24d-05dd210e6b3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "This notebook is meant to demonstrate using the [Mistral models](https://docs.mistral.ai/deployment/cloud/aws/) on Amazon Bedrock for Abstract document summarization tasks. Although all the Mistral models have relatively large context window sizes, when working with multiple large documents, there are several challenges that can arise. One of the main challenges is that the input text might exceed the model's context length. This limitation can lead to incomplete or inaccurate responses, as the model may not have access to all the relevant information within the document. Another challenge is that language models can sometimes hallucinate or generate factually incorrect responses when dealing with very long documents. This can happen because the model may lose track of the overall context or make incorrect inferences based on partial information. Additionally, processing large documents can lead to out-of-memory errors, especially on resource-constrained systems or when working with large language models that have high memory requirements.\n",
    "\n",
    "To address these challenges, this notebook will go through various summarization strategies that will use [LangChain](https://python.langchain.com/docs/get_started/introduction.html), a popular framework for developing applications powered by large language models (LLMs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3664e61-3f39-4229-9cf6-26ee090a8608",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Mistral Model Selection\n",
    "\n",
    "Today, there are three Mistral models available on Amazon Bedrock:\n",
    "\n",
    "### 1. Mistral 7B Instruct\n",
    "\n",
    "- **Description:** A 7B dense Transformer model, fast-deployed and easily customizable. Small yet powerful for a variety of use cases.\n",
    "- **Max Tokens:** 8,196\n",
    "- **Context Window:** 32K\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 2. Mixtral 8X7B Instruct\n",
    "\n",
    "- **Description:** A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Utilizes 12B active parameters out of 45B total.\n",
    "- **Max Tokens:** 4,096\n",
    "- **Context Window:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "### 3. Mistral Large\n",
    "\n",
    "- **Description:** A cutting-edge text generation model with top-tier reasoning capabilities. It can be used for complex multilingual reasoning tasks, including text understanding, transformation, and code generation.\n",
    "- **Max Tokens:** 8,196\n",
    "- **Context Window:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Synthetic Text Generation, Code Generation, RAG, or Agents\n",
    "\n",
    "### Performance and Cost Trade-offs\n",
    "\n",
    "The table below compares the model performance on the Massive Multitask Language Understanding (MMLU) benchmark and their on-demand pricing on Amazon Bedrock.\n",
    "\n",
    "| Model           | MMLU Score | Price per 1,000 Input Tokens | Price per 1,000 Output Tokens |\n",
    "|-----------------|------------|------------------------------|-------------------------------|\n",
    "| Mistral 7B Instruct | 62.5%      | \\$0.00015                    | \\$0.0002                      |\n",
    "| Mixtral 8x7B Instruct | 70.6%      | \\$0.00045                    | \\$0.0007                      |\n",
    "| Mistral Large | 81.2%      | \\$0.008                   | \\$0.024                     |\n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Mistral Model Selection Guide](https://docs.mistral.ai/guides/model-selection/)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce6389-42ff-405e-8c3b-1855c9db22cf",
   "metadata": {},
   "source": [
    "### Local Setup (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed96c9c-58c0-49a8-a032-8b547aa03419",
   "metadata": {
    "tags": []
   },
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a29e7-6312-4ac6-a338-a33cbd83b084",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f67cb8-1a0c-416c-a8c8-66814a52b72c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose ml.t3.medium.\n",
    "2. For Select Kernel, choose [conda_pytorch_p310](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e4415-8fbb-46ac-92e2-3ebcc4888153",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "Before we start building the agentic workflow, we'll first install some libraries:\n",
    "\n",
    "+ AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "+ [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) is a framework that provides off the shelf components to make it easier to build applications with large language models. It is supported in multiple programming languages, such as Python, JavaScript, Java and Go. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b1ce28-362c-44a7-bbde-fec066fea4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.14\n",
    "boto3==1.34.58\n",
    "botocore==1.34.101\n",
    "sqlalchemy==2.0.29\n",
    "pypdf==4.1.0\n",
    "langchain-aws==0.1.6\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ad3619-b8c3-49d3-9175-be3013c079e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b8017-48a6-4369-a556-fd0ecdbd80f8",
   "metadata": {},
   "source": [
    "#### Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c26578-567b-4629-a081-698ff82533fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c109e15-ad6f-4a53-a077-5eae296c67ec",
   "metadata": {},
   "source": [
    "\n",
    "## Initiate the Bedrock Client\n",
    "\n",
    "Import the necessary libraries, along with langchain for bedrock model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee536a3-1cb4-4bab-a8a0-6e41023c9cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "import json\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac5aff5b-4b38-48ea-b5ab-5bebc44b1d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(read_timeout=2000)\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', \n",
    "                       region_name='us-west-2',\n",
    "                       config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876864d8-d99b-45a3-b7e5-c8824ad8e9f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> Ensure that you have access to the Mistral model you wish to use through Bedrock.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61231ed9-0cac-4fee-b932-059fc253bbf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure LangChain with Boto3\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "With LangChain, you can access Bedrock once you pass the boto3 session information to LangChain. Below, we also specify Mistral Large in `model_id` and pass Mistral's inference parameters as desired in `model_kwargs`.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Supported parameters\n",
    "\n",
    "The Mistral AI models have the following inference parameters.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"max_tokens\" : int,\n",
    "    \"stop\" : [string],    \n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"top_k\": int\n",
    "}\n",
    "```\n",
    "\n",
    "The Mistral AI models have the following inference parameters:\n",
    "\n",
    "- **Temperature** - Tunes the degree of randomness in generation. Lower temperatures mean less random generations.\n",
    "- **Top P** - If set to float less than 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "- **Top K** - Can be used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.\n",
    "- **Maximum Length** - Maximum number of tokens to generate. Responses are not guaranteed to fill up to the maximum desired length.\n",
    "- **Stop sequences** - Up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a841ed0-eab4-4921-87b7-a2d07b961405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set the desired mistral model as the default model\n",
    "instruct_mistral7b_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "instruct_mixtral8x7b_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "mistral_large_2402_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "\n",
    "DEFAULT_MODEL = mistral_large_2402_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7901c8-7a58-4c98-bdeb-db0dc4d04dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=DEFAULT_MODEL,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 8192,  ## MAXIMUM NUMBER OF TOKENS for Mistral Large\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 1\n",
    "    },\n",
    "    client=bedrock,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38569ac6-5fad-4162-b0c0-c624e742ecb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello! I'm delighted to chat with you. How can I assist you today or is there a particular topic you'd like to discuss? Remember, I'm here to provide information, answer questions, or simply engage in a friendly conversation.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize conversation chain with Mistral Large on Bedrock\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=False, memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8307f-d16d-427a-b9e7-d7deac5cdb05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816dab8e-af09-456d-b6f1-4b11d33b5644",
   "metadata": {},
   "source": [
    "## Document Processing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23f76d-0a55-490e-8060-a6e821b8eb99",
   "metadata": {},
   "source": [
    "In this example, to demonstrate summarization, we will be using two documents that are both whitepapers from AWS. \n",
    "\n",
    "> The first document is a [whitepaper](https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf) on architecting HIIPA compliant workloads on AWS.\n",
    "\n",
    "> The second document is a [whitepaper](https://docs.aws.amazon.com/whitepapers/latest/containers-on-aws/containers-on-aws.pdf) about containers on AWS. \n",
    "\n",
    "Let's first download these files to build our document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c71e6ec-8500-404c-9dc3-d81b338fcd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "urls = [\n",
    "    'https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf',\n",
    "    'https://docs.aws.amazon.com/whitepapers/latest/containers-on-aws/containers-on-aws.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AWS-security-whitepaper.pdf',\n",
    "    'AWS-containers-whitepaper.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2023, source=filenames[0]),\n",
    "    dict(year=2023, source=filenames[1])\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2214de-07e2-4958-bf06-1339048abceb",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of `DirectoryLoader` from `PyPDF` available under LangChain and splitting them into smaller chunks.\n",
    "\n",
    "Note: For the sake of this use-case we are creating chunks of roughly 4000 characters with an overlap of 100 characters using `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d8742-23aa-4814-8e15-1c720ff9234b",
   "metadata": {},
   "source": [
    "#### HIPAA Compliance document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd92e7-cf45-49d3-aa4a-774b70f3e3e0",
   "metadata": {},
   "source": [
    "In this section, we will load the HIPAA compliance document with `PyPDFLoader`, append document fragments with the metadata, and use LangChain's `RecursiveCharacterTextSplitter` to split the documents in `hipaa_documents` list into smaller text chunks using the `split_documents` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930338ac-1950-4e1b-8388-8972ca92e4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='AWS Whitepaper\\nArchitecting for HIPAA Security and \\nCompliance on Amazon Web Services\\nCopyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.' metadata={'year': 2023, 'source': 'AWS-security-whitepaper.pdf'}\n",
      "\n",
      "Number of documents chunked and created from the HIPAA Security document: 152\n"
     ]
    }
   ],
   "source": [
    "#document 1 (HIPAA COMPLIANCE ON AWS)\n",
    "hipaa_documents = []\n",
    "\n",
    "# Load only the first file\n",
    "hipaa_file = filenames[0]\n",
    "hipaa_loader = PyPDFLoader(data_root + hipaa_file)\n",
    "hipaa_document = hipaa_loader.load()\n",
    "\n",
    "for idx, hipaa_document_fragment in enumerate(hipaa_document):\n",
    "    hipaa_document_fragment.metadata = metadata[0] if metadata else {}\n",
    "    hipaa_documents.append(hipaa_document_fragment)\n",
    "    \n",
    "#chunking\n",
    "hipaa_doc_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a  small chunk size, just to show.\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "hipaa_docs = hipaa_doc_text_splitter.split_documents(hipaa_documents)\n",
    "print(hipaa_docs[0])\n",
    "\n",
    "#chunked doc count\n",
    "hipaa_chunked_count = len(hipaa_docs)\n",
    "print(\n",
    "    f\"\\nNumber of documents chunked and created from the HIPAA Security document: {hipaa_chunked_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d759ea-f15f-4acd-a60d-15abc519135f",
   "metadata": {},
   "source": [
    "#### Containers on AWS Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb32bc-e7b4-488d-bdff-a52b6dc42e45",
   "metadata": {},
   "source": [
    "In this section, we will load the Containers on AWS document with `PyPDFLoader`, append document fragments with the metadata, and use LangChain's `RecursiveCharacterTextSplitter` to split the documents in `container_documents` list into smaller text chunks using the `split_documents` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12233e31-3079-4128-8ff2-0e79d07e2461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Containers on AWS AWS Whitepaper\\nContainers on AWS: AWS Whitepaper\\nCopyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.\\nAmazon's trademarks and trade dress may not be used in connection with any product or service \\nthat is not Amazon's, in any manner that is likely to cause confusion among customers, or in any \\nmanner that disparages or discredits Amazon. All other trademarks not owned by Amazon are \\nthe property of their respective owners, who may or may not be aﬃliated with, connected to, or \\nsponsored by Amazon.\" metadata={'year': 2023, 'source': 'AWS-security-whitepaper.pdf'}\n",
      "\n",
      "Number of documents chunked and created from the original: 57\n"
     ]
    }
   ],
   "source": [
    "#document 2 (Containers on AWS)\n",
    "container_documents = []\n",
    "\n",
    "# Load only the second file\n",
    "container_file = filenames[1]\n",
    "container_loader = PyPDFLoader(data_root + container_file)\n",
    "container_document = container_loader.load()\n",
    "\n",
    "for idx, container_document_fragment in enumerate(container_document):\n",
    "    container_document_fragment.metadata = metadata[0] if metadata else {}\n",
    "    container_documents.append(container_document_fragment)\n",
    "    \n",
    "#chunking\n",
    "container_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a small chunk size, just to show.\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "container_docs = container_text_splitter.split_documents(container_documents)\n",
    "print(container_docs[1])\n",
    "\n",
    "#chunked doc count\n",
    "container_chunked_count = len(container_docs)\n",
    "print(\n",
    "    f\"\\nNumber of documents chunked and created from the original: {container_chunked_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a712e-99d8-403d-85a2-cfa048ceaabf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d90c42-7a08-447d-8143-10571c411c90",
   "metadata": {},
   "source": [
    "## Summarizing Long Documents with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34410f8c-6d6e-4163-9428-7481abd02ef6",
   "metadata": {},
   "source": [
    "In the following sections, we will go over three different summarization techniques with LangChain:\n",
    "    \n",
    " #####   1. Stuff\n",
    " #####   2. Map Reduce\n",
    " #####   3. Refine\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c27632-85ba-46cf-bd43-763e328a8001",
   "metadata": {},
   "source": [
    "### 1. Stuff with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b9890-02bf-4927-8fbb-fea029ddc24c",
   "metadata": {},
   "source": [
    "Stuffing is the simplest method to pass data to a language model. It \"stuffs\" text into the prompt as context in a way that all of the relevant information can be processed by the model to get what you want.\n",
    "\n",
    "In LangChain, you can use `StuffDocumentsChain` as part of the `load_summarize_chain` method. What you need to do is set `stuff` as the `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4175ddb7-9a5a-4c9f-8524-0f36a98c8e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c9859-f01f-4b60-8139-a9b3dd2d0fb2",
   "metadata": {},
   "source": [
    "Next, let's take a look at the Prompt template used by the Stuff summarize chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4690f8f-4883-4291-829c-030374ac3b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afeaa3-21ff-4dd0-b9d5-d691e86e2a77",
   "metadata": {},
   "source": [
    "Here, we see that by default, the Prompt template for `llm_chain` has been set to: 'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'\n",
    "\n",
    "This can be altered by instantiating using `from_template` with LangChain to set a new prompt. We can do that below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41262dc2-3d6e-4296-9954-49edb989ec7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_prompt = PromptTemplate.from_template('Write a detailed and complete summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nDETAILED SUMMARY:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "469f2796-0dd6-4fc5-9101-fbcdc1e782d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template = stuff_prompt.template #set new prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf5e04-8189-4294-8927-5ad4280b82ba",
   "metadata": {},
   "source": [
    "Now that we have set the new prompt template, let us first try generating a summary of the **Containers on AWS** whitepaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4232059e-ef40-49b1-9290-14f38f5b1522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stuff_container_summary = stuff_summary_chain.run(container_docs) \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "110c7395-0a40-4f77-8d90-95467aff5d0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AWS Whitepaper \"Containers on AWS\" provides guidance and options for running containers on AWS. Containers offer a way to develop, ship, and run applications in an isolated environment. AWS complements containers and offers a wide range of scalable orchestration and infrastructure services for container deployment.\n",
      "\n",
      "The whitepaper covers various container orchestration and compute options such as AWS App Runner, Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), and AWS Fargate. It also discusses key considerations for container workloads on AWS, including container runtime, container-enabled AMIs, compute options, scheduling, container repositories, observability, storage, networking, security, build and deploy automation, infrastructure as code and platform automation, and scaling.\n",
      "\n",
      "The benefits of using containers include speed, consistency, density and resource efficiency, and portability. AWS provides a range of purpose-driven services for containers and complementary services, making it easy to get started with containers and run a variety of workloads using containers at scale.\n",
      "\n",
      "The whitepaper is intended for developers, architects, and IT professionals who are interested in running containers on AWS. It assumes a basic understanding of containers and AWS services. The document is organized into sections that cover abstract and introduction, container benefits, container orchestrations on AWS, key considerations, conclusion, contributors, further reading, document history, and notices. The whitepaper also includes a glossary of AWS terms.\n"
     ]
    }
   ],
   "source": [
    "print(stuff_container_summary.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b054f-e936-4595-825d-ebab6019dfc4",
   "metadata": {},
   "source": [
    "From the cell ouput above, we can see that since Stuffing only requires a single call to the LLM, it can be faster than other methods that require multiple calls. When summarizing text, the model has access to all the data at once, which can result in a fast response for the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a33dffb-c033-49a2-bf2f-6af050d01160",
   "metadata": {},
   "source": [
    "Next, let us use the **HIPAA and Security Compliance** on AWS whitepaper to see how the model deals with summarization using the `StuffDocumentChain` when presented with a longer document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4b9f80d-bc8b-4850-bcb8-dd0a76df371d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: This model's maximum context length is 32768 tokens. Please reduce the length of the prompt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stuff_hipaa_summary = stuff_summary_chain.run(hipaa_docs) # (ValidationException error) prompt over 32k window length / number of tokens exceeds window size \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cbe28-64c6-4302-8c42-deba21fa516f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes:\n",
    "In the output for the above cell, we see that an error is raised due to the prompt far exceeding the model's maximum context length. Since stuffing summarizes text by feeding the entire document to a large language model (LLM) in a single call, it is difficult to process long documents. The Mistral models have a context length of 32k tokens, which is the maximum number of tokens that can be processed in a single call. If the document is longer than the context length, stuffing will not work. Also the stuffing method is not suitable for summarizing large documents, as it can be slow and may not produce a good summary.\n",
    "\n",
    "Let's explore a couple chunk-wise summarization techniques with [LangChain](https://python.langchain.com/docs/get_started/introduction.html) to be able to mitigate the restrictions of your large documents not fitting into the context window of the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b1252-171a-43c7-a184-41fc47e1b836",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Map Reduce with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef8232-dc84-44c8-8124-4f4ff7e69f64",
   "metadata": {},
   "source": [
    "The `Map_Reduce` method involves summarizing each document individually (map step) and then combining these summaries into a final summary (reduce step). This approach is more scalable and can handle larger volumes of text. The map reduce technique is designed for summarizing large documents that exceed the token limit of the language model. It involves dividing the document into chunks, generating summaries for each chunk, and then combining these summaries to create a final summary. This method is efficient for handling large files and significantly reduces processing time.\n",
    "\n",
    "In LangChain, you can use `MapReduceDocumentsChain` as part of the `load_summarize_chain method`. What you need to do is set `map_reduce` as the `chain_type` of your chain.\n",
    "\n",
    "---\n",
    "\n",
    "![map-reduce](imgs/mapreduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "103e086a-d021-4357-901b-5670731087e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain, it then combines and iteratively reduces the mapped document\n",
    "map_reduce_summary_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae04456-08d3-4f3e-8491-e37d736f18db",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `ReduceDocumentsChain` handles taking the document mapping results and reducing them into a single output. It wraps a generic `CombineDocumentsChain` (like `StuffDocumentsChain`) but adds the ability to collapse documents before passing it to the `CombineDocumentsChain` if their cumulative size exceeds token_max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e8de7ca-64d6-4720-9278-a477638b4b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiation using from_template (recommended)\n",
    "#sets the prompt template for the summaries generated for all the individual document chunks.\n",
    "initial_map_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.llm_chain.prompt.template = initial_map_prompt.template\n",
    "\n",
    "#sets the prompt template for generating a cumulative summary of all the document chunks for reduce documents chain.\n",
    "reduce_documents_prompt= PromptTemplate.from_template(\"\"\"\n",
    "                      Write a detailed summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt.template = reduce_documents_prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9794d-7ff2-496c-8998-2801b1846a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Map-Reduce`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Map_Reduce works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c76502a-13c8-4501-9544-d957ec56bdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5627 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#this cell might take 5-10 minutes to run\n",
    "try:\n",
    "    map_reduce_summary = map_reduce_summary_chain.run(hipaa_docs[50:71])  \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "334fad6e-1ded-4cb3-8da2-1b6bd534b115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The text discusses the requirements for HIPAA (Health Insurance Portability and Accountability Act) security and compliance on Amazon Web Services (AWS).\n",
      "- To comply with the requirement that Protected Health Information (PHI) be encrypted at rest, Amazon Elastic File System (EFS) supports two methods: selecting the \"Enable encryption of data at rest\" option when creating a new file system, or encrypting data before placing it on EFS.\n",
      "- PHI should not be used as all or part of any file name or folder name.\n",
      "- Encryption of PHI while in transit for Amazon EFS is provided by Transport Layer Security (TLS) between the EFS service and the instance mounting the file system.\n",
      "- Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that makes it easy for customers to run Kubernetes on AWS without needing to stand up or maintain their own Kubernetes control plane.\n",
      "- Amazon ElastiCache for Redis is an in-memory data structure service that can be used as a data store or cache, and it is compatible with Redis. To store PHI, customers must use the latest HIPAA-eligible ElastiCache for Redis engine version and current generation node types.\n",
      "- Customers are responsible for ensuring that their clusters and nodes are configured to encrypt data at rest, enable transport encryption, and authenticate Redis commands.\n",
      "- Amazon ElastiCache for Redis provides data encryption for its cluster to protect data at rest. When customers enable encryption at-rest for a cluster at the time of creation, Amazon ElastiCache for Redis encrypts data on disk and automated Redis backups.\n",
      "- Amazon ElastiCache for Redis uses Transport Layer Security (TLS) to encrypt data in transit. Connections to ElastiCache for Redis containing Protected Health Information (PHI) must use transport encryption and the configuration should be consistent with the provided Guidance.\n",
      "- Amazon ElastiCache for Redis clusters (single/multi node) that contain PHI must provide a Redis AUTH token to enable authentication of Redis commands. This feature is available when both encryption at-rest and encryption-in transit are enabled.\n",
      "- Amazon ElastiCache for Redis clusters that contain PHI must be updated with the latest 'Security' type service updates on or before the 'Recommended Apply by Date'.\n",
      "- Amazon OpenSearch Service allows customers to run a managed OpenSearch or legacy Elasticsearch OSS cluster in a dedicated Amazon Virtual Private Cloud (Amazon VPC). When using OpenSearch Service with Protected Health Information (PHI), customers should use OpenSearch or Elasticsearch 6.0 or later and ensure that PHI is encrypted at-rest and in-transit.\n",
      "- Amazon EMR, which deploys and manages a cluster of Amazon EC2 instances into a customer’s account, has encryption options available.\n",
      "- Amazon EventBridge, previously known as Amazon CloudWatch Events, is a serverless event bus. It enables the creation of scalable event-driven applications. By default, EventBridge encrypts data using 256-bit Advanced Encryption Standard (AES-256) under an AWS owned CMK (Customer Master Key).\n",
      "- The text emphasizes the importance of configuring AWS resources that store, process, or transmit Protected Health Information (PHI) in accordance with best practices.\n",
      "- Various AWS services are discussed, including Amazon S3, AWS Forecast, and Amazon FSx. Amazon S3 log files can be encrypted with Server-Side Encryption using AWS Key Management Service (SSE-KMS), but digest files are encrypted with Amazon S3-managed encryption keys (SSE-S3).\n",
      "- Amazon FSx is a fully managed shared file storage service that supports two types of file systems: Windows File Server and Lustre. It offers encryption for data in transit and at rest.\n",
      "- Amazon GuardDuty is a managed threat detection service that continuously monitors for malicious or unauthorized behavior to help protect AWS accounts and workloads.\n",
      "- Amazon HealthLake is a service that allows customers in the healthcare and life sciences industries to store, transform, query, and analyze health data at petabyte scale, including Protected Health Information (PHI).\n",
      "- Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It can be run on EC2 instances that contain PHI.\n",
      "- Amazon Managed Service for Apache Flink allows customers to quickly author SQL code that continuously reads, processes, and stores data in near real time. It supports inputs from Kinesis Data Streams and Amazon Inspector.\n",
      "- Amazon Kinesis Video Streams is a fully managed AWS service that allows customers to stream live video from devices to the AWS Cloud. It supports server-side encryption for data at rest using an AWS Key Management Service (KMS) key.\n",
      "- Amazon Lex is a service for building conversational interfaces for applications using voice and text. It uses the same conversational engine that powers Amazon Alexa and supports encryption in transit using HTTPS.\n",
      "- Amazon MSK (Managed Streaming for Kafka) clusters have encryption enabled for data in-transit via Transport Layer Security (TLS) for inter-broker communication.\n",
      "- Amazon MQ is a managed message broker service for Apache ActiveMQ that ensures the encryption of PHI data while it is in transit.\n",
      "- Amazon Neptune is a fast, reliable, fully managed graph database service that now allows the retention of PHI data in an encrypted instance.\n",
      "- AWS Network Firewall is a managed firewall service that simplifies the deployment of essential network protections for all Amazon Virtual Private Cloud (Amazon VPC), encrypting all data at rest and in transit between component AWS services.\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_summary.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb754c4-4963-4d59-a816-cfa3363c7432",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "With `Map_Reduce`, the model is able to summarize a large document by overcoming the context limit of Stuffing method with parallel processing. \n",
    "However, it requires multiple calls to the model and potentially loses context between individual summaries of the chunks. To deal with this challenge, let us try another method that performs chunk-wise summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c6d9a-7b92-44fc-88c1-aaba42176117",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a9d41-399a-4e9b-928e-97afb4bc68f8",
   "metadata": {},
   "source": [
    "### 3. Refine with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f8a89-f7e2-411f-b898-a454daf05a29",
   "metadata": {},
   "source": [
    "The `Refine` method iteratively updates its answer by looping over the input documents. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer. This method is useful for refining a summary based on new context.`Refine` is a simpler alternative to `Map_Reduce` technique. It involves generating a summary for the first chunk, combining it with the second chunk, generating another summary, and continuing this process until a final summary is achieved. This method is suitable for large documents but requires less complexity compared to `Map_Reduce`.\n",
    "\n",
    "---\n",
    "\n",
    "![map-reduce](imgs/refine.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8df1a4d-6ab0-4a2a-bed4-228973c532ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run an initial prompt on a small chunk of data to generate a summary. Then, for each subsequent document, the output from the previous document is passed in along with the new document, and the LLM is asked to refine the output based on the new document.\n",
    "refine_summary_chain = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=False)\n",
    "refine_summary_chain_french = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=False) #refine summary chain for summarization in french"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf21cbc-1b13-4341-b00a-300b579ddcfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Refine`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Refine works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f02e05ef-8a19-415c-ac84-41bac282f1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initial llm chain prompt template\n",
    "initial_refine_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "refine_summary_chain.initial_llm_chain.prompt.template = initial_refine_prompt.template\n",
    "\n",
    "#refine llm chain prompt template\n",
    "refine_documents_prompt= PromptTemplate.from_template(\"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\")\n",
    "\n",
    "refine_summary_chain.refine_llm_chain.prompt.template = refine_documents_prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de58d720-bccb-4bee-b836-020d6ddf56ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    refine_summary = refine_summary_chain.run(hipaa_docs[50:71])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0584649c-fd50-4527-a04f-8c4f1a4d25ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text discusses the measures to ensure HIPAA security and compliance on Amazon Web Services (AWS), focusing on various services including Amazon Elastic File System (EFS), Amazon Elastic Kubernetes Service (Amazon EKS), Amazon ElastiCache for Redis, Amazon OpenSearch Service, Amazon EMR, Amazon EventBridge, Amazon Forecast, Amazon FSx, Amazon HealthLake, and Amazon Inspector.\n",
      "\n",
      "For EFS and Amazon FSx, the text outlines two methods to encrypt Protected Health Information (PHI) at rest. The first method involves enabling encryption during the creation of a new file system, which ensures all data is encrypted using AES-256 encryption and AWS Key Management Service (KMS)-managed keys. The second method is to encrypt data before placing it on EFS or FSx, but this places the responsibility of managing the encryption process and key management on the customer. The text also advises against using PHI as part of any file or folder name. For PHI in transit, encryption is provided by Transport Layer Security (TLS) between the EFS service and the instance mounting the file system, and by SMB protocol 3.0 or newer for FSx.\n",
      "\n",
      "Amazon EKS is a managed service that simplifies running Kubernetes on AWS without the need for customers to maintain their own Kubernetes control plane. For more information on security and compliance, the text refers to the Architecting for HIPAA Security and Compliance on Amazon EKS whitepaper.\n",
      "\n",
      "Amazon ElastiCache for Redis supports storing PHI when customers ensure they are running the latest HIPAA-eligible ElastiCache for Redis engine version and current generation node types. To store PHI securely, customers must configure the cluster and nodes within the cluster to encrypt data at rest, enable transport encryption using TLS, and enable authentication of Redis commands using a strong Redis AUTH token. This token must be set at the time of Redis replication group creation and can be updated later. AWS encrypts this token using AWS Key Management Service (AWS KMS). Additionally, customers must keep their Redis clusters updated with the latest 'Security' type service updates on or before the 'Recommended Apply by Date'.\n",
      "\n",
      "Amazon OpenSearch Service enables customers to run a managed OpenSearch or legacy Elasticsearch OSS cluster in a dedicated Amazon Virtual Private Cloud (Amazon VPC). When using OpenSearch Service with PHI, customers should use OpenSearch or Elasticsearch 6.0 or later. Customers should ensure PHI is encrypted at-rest and in-transit within Amazon OpenSearch Service. Customers may use AWS KMS key encryption to encrypt data at rest in their OpenSearch Service domains, which is only available for OpenSearch and Elasticsearch 5.1 or later. Each OpenSearch Service domain runs in its own VPC. Customers should enable node-to-node encryption, which is available in all OpenSearch versions, and in Elasticsearch 6.0 or later. If customers send data to OpenSearch Service over HTTPS, node-to-node encryption helps ensure that their data remains encrypted as OpenSearch distributes (and redistributes) it throughout the cluster. If data arrives unencrypted over HTTP, OpenSearch Service encrypts the data after it reaches the cluster. Therefore, any PHI that enters an Amazon OpenSearch Service cluster should be sent over HTTPS.\n",
      "\n",
      "Amazon EMR deploys and manages a cluster of Amazon EC2 instances into a customer’s account. For information on encryption with Amazon EMR, see Encryption Options.\n",
      "\n",
      "Amazon EventBridge, a serverless event bus that enables customers to create scalable event-driven applications, should ensure that any AWS resource emitting an event that is storing, processing, or transmitting PHI is configured in accordance with best practices. EventBridge is integrated with AWS CloudTrail and customers can view the most recent events in the CloudTrail console in Event history.\n",
      "\n",
      "Amazon Forecast is a fully managed service that uses machine learning to deliver highly accurate forecasts. Every interaction customers have with Amazon Forecast is protected by encryption. Any content processed by Amazon Forecast is encrypted with customer keys through Amazon Key Management Service, and encrypted at-rest in the AWS Region where customers are using the service. Amazon Forecast is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon Forecast. By default, the log files delivered by CloudTrail to their bucket are encrypted by Amazon server-side encryption with Amazon S3-managed encryption keys (SSE-S3). To provide a security layer that is directly manageable, customers can instead use server-side encryption with AWS KMS–managed keys (SSE-KMS) for their CloudTrail log files. However, this option is not available for the digest files, which are encrypted with Amazon S3-managed encryption keys (SSE-S3). When importing and exporting data from Amazon S3, customers should ensure S3 buckets are configured in a manner consistent with the guidance provided. Connections to Amazon S3 containing PHI must use endpoints that accept encrypted transport (that is, HTTPS). For a list of regional endpoints, see AWS service endpoints.\n",
      "\n",
      "Amazon HealthLake enables customers in the healthcare and life sciences industries to store, transform, query, and analyze health data at petabyte scale. Customers can use Amazon HealthLake to transmit, process, and store PHI. Amazon HealthLake encrypts data at rest in customer’s data stores by default. All service data and metadata is encrypted with a service owned KMS key. Per Fast Healthcare Interoperability Resources (FHIR) specifications, if a customer chooses to use their own KMS key, they can do so by specifying it during the data store creation process. When a customer deletes an FHIR resource, it will only be hidden from retrieval, and will be retained by the service for versioning. Amazon HealthLake enforces the requirement to export data to an encrypted Amazon S3 bucket when customers use StartFHIRImportJob API. Amazon HealthLake encrypts data both in transit and at rest. For the encryption of data in transit, customers can use AWS published API calls to access HealthLake through the network. Clients must support Transport Layer Security (TLS) 1.0 or later, with TLS 1.2 being required and TLS 1.3 recommended. Clients must also support cipher suites with perfect forward secrecy (PFS) such as Ephemeral Diﬃe-Hellman (DHE) or Elliptic Curve Ephemeral Diﬃe-Hellman (ECDHE). Most modern systems such as Java 7 and later support these modes. Additionally, requests must be signed by using an access key ID and a secret access key that is associated with an IAM principal. Alternatively, customers can use the AWS Security Token Service (AWS STS) to generate temporary security credentials to sign requests. Amazon HealthLake is integrated with AWS CloudTrail, which captures all API calls to Amazon HealthLake as events, including calls made as a result of interaction with AWS Management Console, command line interface (CLI), and programmatically using software development kit (SDK).\n",
      "\n",
      "Amazon Inspector is an automated security assessment service for customers seeking to improve their security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for vulnerabilities or deviations from best practices. After performing an assessment, it produces a detailed list of security findings prioritized by level of severity. These findings can be reviewed directly or as part of detailed assessment reports available via the Amazon Inspector console or API. Amazon Inspector is integrated with AWS Identity and Access Management (IAM), allowing customers to control which AWS resources Amazon Inspector can assess. Amazon Inspector does not require or store any PHI, and it does not access the contents of customer data stored in the assessed resources. Customers may run Amazon Inspector on EC2 instances that contain PHI. Amazon Inspector encrypts all data transmitted over the network as well as all telemetry data stored at-rest.\n",
      "\n",
      "Amazon Managed Service for Apache Flink enables customers to quickly author SQL code that continuously reads, processes, and stores data in near real time. Using standard SQL queries on the streaming data, customers can construct applications that transform and provide insights into their data. Managed Service for Apache Flink supports inputs from Kinesis Data Streams and Amazon Inspector. If the stream is encrypted, Managed Service for Apache Flink accesses the data in the encrypted stream seamlessly with no further configuration needed. Managed Service for Apache Flink does not store unencrypted data read from Kinesis Data Streams. Managed Service for Apache Flink integrates with both AWS CloudTrail and Amazon CloudWatch Logs for application monitoring.\n",
      "\n",
      "When customers send data from their data producers to their Kinesis data stream, Amazon Kinesis Data Streams encrypts data using an AWS KMS key before storing it at-rest. When the Firehose delivery stream reads data from the Kinesis stream, Kinesis Data Streams first decrypts the data and then sends it to Firehose. Firehose buffers the data in memory based on the buffering hints specified by the customer. It then delivers the data to the destinations without storing the unencrypted data at rest. For more information about encryption with Firehose, see Data Protection in Amazon Data Firehose. AWS provides various tools that customers can use to monitor Amazon Data Firehose, including Amazon CloudWatch metrics, Amazon CloudWatch Logs, Kinesis Agent and API logging and history.\n",
      "\n",
      "Amazon Kinesis Streams enables customers to build custom applications that process or analyze streaming data for specialized needs. The server-side encryption feature allows customers to encrypt data at rest. When server-side encryption is enabled, Kinesis Streams will use an AWS KMS key to encrypt the data before storing it on disks. For more information, see Data Protection in Amazon Kinesis Data Streams.\n",
      "\n",
      "Amazon Kinesis Video Streams is a fully managed AWS service that customers can use to stream live video from devices to the AWS Cloud, or build applications for real-time video processing or analysis. Server-side encryption is a feature in Kinesis Video Streams that automatically encrypts data at rest by using an AWS KMS key (formerly CMK) that is specified by the customer. Data is encrypted before it is written to the Kinesis Video Streams stream storage layer, and it is decrypted after it is retrieved from storage. The Amazon Kinesis Video Streams SDK can be used to transmit streaming video data containing PHI. By default, the SDK uses TLS to encrypt frames and fragments generated by the hardware device on which it is installed. The SDK does not manage or affect data stored at-rest. Amazon Kinesis Video Streams uses AWS CloudTrail to log all API calls.\n",
      "\n",
      "Amazon Lex is an AWS service for building conversational interfaces for applications using voice and text. With Amazon Lex, the same conversational engine that powers Amazon Alexa is now available to any developer, enabling customers to build sophisticated, natural language chatbots into their new and existing applications. Amazon Lex provides the deep functionality and flexibility of natural language understanding (NLU) and automatic speech recognition (ASR) so customers can build highly engaging user experiences with lifelike, conversational interactions, and create new categories of products. Lex uses the HTTPS protocol to communicate both with clients as well as other AWS services. Access to Lex is API-driven, and appropriate IAM least privilege can be enforced. Customers can monitor the performance of their Amazon Lex bots using Amazon CloudWatch. With CloudWatch, customers can get metrics for individual Amazon Lex operations or for global Amazon Lex operations for their account. Customers can also set up CloudWatch alarms to be notified when one or more metrics exceeds a threshold that customers define. For example, customers can monitor the number of requests made to a bot over a particular time period, view the latency of successful requests, or raise an alarm when errors exceed a threshold. Lex is also integrated with AWS CloudTrail to log Lex API calls. For more information, see Monitoring in Amazon Lex.\n",
      "\n",
      "Amazon Managed Streaming for Apache Kafka (Amazon MSK) provides encryption features for data at rest and for data in-transit. For data at rest encryption, Amazon MSK cluster uses Amazon EBS server-side encryption and AWS KMS keys to encrypt data volumes. For data in-transit encryption, Amazon MSK supports Transport Layer Security (TLS) protocol. This ensures that data transmitted between clients and brokers, and between brokers, is encrypted. The encryption configuration setting is enabled when a cluster is created. Also, by default, in-transit encryption is set to TLS for clusters created from CLI or AWS Console. Additional configuration is required for clients to communicate with clusters using TLS encryption. Customers can change the default encryption setting by selecting the TLS/plaintext settings. For more information, see Amazon MSK Encryption. Customers can monitor the performance of their clusters using the Amazon MSK console, Amazon CloudWatch console, or they can access JMX and host metrics using Open Monitoring with Prometheus, an open source monitoring solution. Tools that are designed to read from Prometheus exporters are compatible with Open Monitoring, like: Datadog , Lenses , New Relic, Sumologic , or a Prometheus server. For details on Open Monitoring, see Amazon MSK Open Monitoring documentation. Please note that the default version of Apache Zookeeper bundled with Apache Kafka does not support encryption. However, it is important to note that communications between Apache Zookeeper and Apache Kafka brokers is limited to broker, topic, and partition state information. The only way data can be produced and consumed from an Amazon MSK cluster is over a private connection between their clients in their VPC and the Amazon MSK cluster. Amazon MSK does not support public endpoints.\n",
      "\n",
      "Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Amazon MQ works with existing applications and services without the need for a customer to manage, operate, or maintain their own messaging infrastructure. To provide the encryption of PHI data while in transit, the following protocols with TLS enabled should be used to access brokers: AMQP, MQTT, MQTT over WebSocket, OpenWire, STOMP. Amazon MQ provides encryption in transit with TLS and encryption at rest using Amazon EBS encryption. Amazon MQ also supports authentication and authorization using AWS Identity and Access Management (IAM) and Amazon MQ’s built-in broker-level access control. Amazon MQ is integrated with AWS CloudTrail and Amazon CloudWatch for monitoring and logging.\n",
      "\n",
      "Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine that is optimized for storing billions of relationships and querying the graph with milliseconds latency. Amazon Neptune supports the popular graph query languages Apache TinkerPop Gremlin and W3C’s SPARQL. Data containing PHI can now be retained in an encrypted instance of Amazon Neptune. An encrypted instance of Amazon Neptune can be specified only at the time of creation by choosing ‘Enable Encryption’ from the Amazon Neptune console. All logs, backups, and snapshots are encrypted for an Amazon Neptune encrypted instance. Key management for encrypted instances of Amazon Neptune is provided through the AWS KMS. Encryption of data in transit is provided through SSL/TLS. Amazon Neptune uses CloudTrail to log all API calls.\n",
      "\n",
      "AWS Network Firewall is a managed firewall service that makes it easy to deploy essential network protections for all your Amazon Virtual Private Cloud (Amazon VPC). The service automatically scales with network traffic volume to provide high-availability protections without the need to set up or maintain the underlying infrastructure. Both customer rules and access logs may contain end user IP addresses, which are encrypted both at rest and in transit within the AWS architecture. Furthermore, AWS Network Firewall encrypts all data at rest and in transit between component AWS services (Amazon S3, Amazon DynamoDB, Amazon CloudWatch Logs, Amazon EBS).\n"
     ]
    }
   ],
   "source": [
    "print(refine_summary.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbf269-684d-43ea-9519-3aca9373aea9",
   "metadata": {},
   "source": [
    "---\n",
    "Now that we have seen how the `refine` document chain constructs a response, let us try altering the refine_llm_chain prompt template to help highlight some of the multilingual capabilties of the [Mistral Large](https://mistral.ai/news/mistral-large/) model. Mistral Large demonstrates superior capabilities in handling multi-lingual tasks. Mistral-large has been specifically trained to understand and generate text in multiple languages, especially in French, German, Spanish, and Italian. This can be especially valuable for businesses and users that need to communicate in multiple languages. In the below cell, we set the `refine llm chain` prompt template to return the final summary in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b3e0d891-1522-44c1-b1eb-dc24aea5c1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#refine llm chain prompt template\n",
    "refine_documents_prompt_french= PromptTemplate.from_template(\"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary in French\"\n",
    "    \"If the context isn't useful, return the original summary.\")\n",
    "\n",
    "refine_summary_chain_french.refine_llm_chain.prompt.template = refine_documents_prompt_french.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7c15ebb8-0db7-42d0-a712-83a0840f24f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    refine_summary_french = refine_summary_chain_french.run(hipaa_docs[50:71])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "76d6cd69-38a5-415d-bb20-36e0420bbfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le texte traite des mesures visant à garantir la sécurité et la conformité HIPAA sur Amazon Web Services (AWS), en se concentrant sur divers services tels qu'Amazon Elastic File System (EFS), Amazon Elastic Kubernetes Service (Amazon EKS), Amazon ElastiCache for Redis, Amazon OpenSearch Service, Amazon EventBridge, Amazon Forecast, Amazon FSx, Amazon GuardDuty, Amazon HealthLake, Amazon Inspector, Amazon Managed Service for Apache Flink, Amazon Kinesis Streams, Amazon Data Firehose, Amazon Kinesis Video Streams, Amazon Managed Streaming for Apache Kafka (Amazon MSK), Amazon MQ et Amazon Lex. Il souligne l'importance de surveiller et de journaliser toutes les activités liées aux PHI sur AWS à l'aide d'outils tels qu'AWS CloudTrail et Amazon CloudWatch.\n",
      "\n",
      "Pour chaque service, le texte insiste sur les méthodes de chiffrement des PHI au repos et en transit, ainsi que sur les meilleures pratiques pour garantir la conformité HIPAA. Le texte mentionne également l'utilisation d'Amazon Neptune, un service de base de données de graphes entièrement géré, pour stocker et traiter des données hautement connectées contenant des PHI. Les données contenant des PHI peuvent être conservées dans une instance chiffrée d'Amazon Neptune, avec un chiffrement activé au moment de la création. Tous les journaux, sauvegardes et instantanés sont chiffrés pour une instance Amazon Neptune chiffrée. La gestion des clés pour les instances chiffrées d'Amazon Neptune est assurée par AWS KMS. Le chiffrement des données en transit est fourni via SSL/TLS. Amazon Neptune utilise CloudTrail pour journaliser tous les appels d'API.\n",
      "\n",
      "En outre, AWS Network Firewall est un service de pare-feu géré qui facilite le déploiement de protections réseau essentielles pour tous les Amazon Virtual Private Cloud (Amazon VPC). Le service chiffre toutes les données au repos et en transit entre les services AWS composants (Amazon S3, Amazon DynamoDB, Amazon CloudWatch Logs, Amazon EBS). Les clients doivent toujours s'assurer que les PHI sont chiffrés au repos et en transit, et activer le chiffrement de nœud à nœud lorsque cela est applicable.\n",
      "\n",
      "Enfin, le texte met en garde contre l'utilisation de PHI dans le cadre d'un nom de fichier ou de dossier pour tous les services AWS. La surveillance est importante pour maintenir la fiabilité, la disponibilité et les performances des services AWS des clients. Pour suivre la santé des services AWS, les clients peuvent utiliser Amazon CloudWatch. Avec CloudWatch, les clients peuvent obtenir des métriques pour les opérations AWS individuelles ou pour l'ensemble des opérations AWS. Les clients peuvent également configurer des alarmes CloudWatch pour être avertis lorsqu'une ou plusieurs métriques dépassent un seuil défini par les clients.\n"
     ]
    }
   ],
   "source": [
    "print(refine_summary_french.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe47f1-a1cc-4a06-97ff-6494256f8ef7",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "`Refine` has the potential to incorporate more relevant context compared to `Map_Reduce`, potentially resulting in a more comprehensive and accurate summary. However, it comes with a trade-off: `Refine` necessitates a significantly higher number of calls to the LLM than the `Stuff` and `Map_Reduce` since it is an incremental process where the subsequent chunk's summary uses the previous chunk's summary. Moreover, these calls are not independent, which means they cannot be parallelized, potentially leading to longer processing times. Another consideration is that the Refine method may exhibit recency bias, where the most recent documents in the sequence could carry more weight or influence in the final summary, as the method processes documents in a specific order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca227dd3-5b63-4fd7-bf48-38bd36252a79",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Mistral AI\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
