{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Codestral on Amazon SageMaker with DeepSpeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[Codestral](https://mistral.ai/news/codestral/) is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers. Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral can assist developers in various coding environments and projects.\n",
    "\n",
    "SageMaker has rolled out [DeepSpeed container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers) which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting.\n",
    "\n",
    "[DJL](https://docs.djl.ai/) provides for the serving framework while [DeepSpeed](https://www.deepspeed.ai/) is the key sharding library we leverage to enable hosting of large models. We use DJLServing as the model serving solution in this example. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to this [blog post](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/).\n",
    "\n",
    "In this notebook, we deploy the open source `Codestral 22B` model across GPU's on a `ml.g5.12xlarge` instance. DeepSpeed is used for tensor parallelism inference while DJLServing handles inference requests and the distributed workers. For further reading on DeepSpeed you can refer to this [link](https://arxiv.org/pdf/2207.00032.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "As a 22B model, Codestral sets a new standard on the performance/latency space for code generation compared to previous models used for coding. With its larger context window of 32k (compared to 4k, 8k or 16k for competitors), Codestral outperforms all other models in RepoBench, a long-range eval for code generation.\n",
    "\n",
    "![codestral](imgs/codestral.png)\n",
    "\n",
    "<b><i>To deploy Codestral on to Sagemaker with TGI, please refer to the 'Deploy Codestral on TGI' notebook located in this folder.</b></i>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> Codestral is a 22B open-weight model licensed under the new Mistral AI Non-Production License, which means that you can use it for research and testing purposes. Codestral can be downloaded on HuggingFace.\n",
    "If you want to use the model in the course of commercial activity, Commercial licenses are also available on demand by reaching out to the Mistral team.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reach out to Mistral to explore Codestral for commercial use cases: [Contact the Mistral team](https://mistral.ai/contact/)\n",
    "\n",
    "##### More on the Mistral AI Non-Production License: [Mistral AI Non-Production License](https://mistral.ai/news/mistral-ai-non-production-license-mnpl/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose `ml.t3.medium`.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you would need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3==1.34.132 -qU --force --quiet --no-warn-conflicts\n",
    "!pip install sagemaker==2.224.1 -qU --force --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create SageMaker compatible Model artifact and Upload Model to S3\n",
    "\n",
    "SageMaker Large Model Inference containers can be used to host models without providing your own inference code. This is extremely useful when there is no custom pre-processing of the input data or postprocessing of the model's predictions.\n",
    "\n",
    "In this notebook, we demonstrate how to deploy a model without any inference code.\n",
    "\n",
    "SageMaker needs the model to be in a Tarball format. The tarball is in the following format:\n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── serving.properties\n",
    "\n",
    "``` \n",
    "\n",
    "- `serving.properties` is the configuration file that can be used to configure the model server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::570598552974:role/txt2sql-SageMakerExecutionRole-PAgMr5TND4x0\n",
      "sagemaker bucket: sagemaker-us-east-1-570598552974\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# execution role for the endpoint\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# sagemaker session for interacting with different AWS APIs\n",
    "sess = sagemaker.session.Session()\n",
    "\n",
    "# Region\n",
    "region_name = sess._region_name\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "bucket = None\n",
    "if bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    bucket = sess.default_bucket()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image URI of the DJL Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCL Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\",\n",
    "    region=region_name,\n",
    "    version=\"0.27.0\"\n",
    ")\n",
    "print(f\"DCL Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more details about DLC images [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers) and [here](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi/announcements/deepspeed-deprecation.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DJL parameters using Serving.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of settings that we use in this configuration file:\n",
    "\n",
    "- `engine`: The engine for DJL to use. In this case, we intend to use Accelerate and hence set it as **Python**.\n",
    "- `option.model_id`: The model id of a pretrained model hosted inside a model repository on [huggingface.co](https://huggingface.co/models). The container uses this model id to download the corresponding model repository on huggingface.co. This is an optional setting and is not needed in the scenario where you are brining your own model. If you are getting your own model, you can set `option.s3url` to the URI of the Amazon S3 bucket that contains the model.\n",
    "- `option.s3url` (Optional): Set this to the URI of the Amazon S3 bucket that contains the model. When this is set, the container leverages s5cmd to download the model from s3. This is extremely fast and useful when downloading large models like this one.\n",
    "- `option.dtype`: The data type you plan to cast the model weights to. If not provided, LMI will use fp16.\n",
    "- `option.task`: The task used in Hugging Face for different pipelines. Default is text-generation. For further reading on DJL parameters on SageMaker, follow the [link](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/deepspeed_user_guide.html)\n",
    "- `option.rolling_batch`: Enables continuous batching (iteration level batching) with one of the supported backends. Available backends differ by container, see [Inference Library Configurations](https://docs.djl.ai/docs/serving/serving/docs/lmi/deployment_guide/configurations.html#inference-library-configuration) for mappings.\n",
    "    - In the LMI Container:\n",
    "        - to use vLLM, use `option.rolling_batch=vllm`\n",
    "        - to use lmi-dist, use `option.rolling_batch=lmi-dist`\n",
    "        - to use huggingface accelerate, use `option.rolling_batch=auto` for text generation models, or option.rolling_batch=disable for non-text generation models.\n",
    "- `option.tensor_parallel_degree`: Set to the number of GPU devices over which DeepSpeed needs to partition the model. This parameter also controls the no of workers per model which will be started up when DJL serving runs. As an example if we have a 8 GPU machine and we are creating 8 partitions then we will have 1 worker per model to serve the requests. For further reading on DeepSpeed, follow the [link](https://www.deepspeed.ai/tutorials/inference-tutorial/#initializing-for-inference).\n",
    "- `option.device_map`: The HuggingFace accelerate device_map to use.\n",
    "\n",
    "For more details on the configuration options and an exhaustive list, you can refer the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile serving.properties\n",
    "\n",
    "engine=Python\n",
    "option.model_id=mistral-community/Codestral-22B-v0.1\n",
    "option.dtype=bf16\n",
    "option.task=text-generation\n",
    "option.rolling_batch=vllm\n",
    "option.tensor_parallel_degree=4\n",
    "option.device_map=auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm_code_artifact/\n",
      "sm_code_artifact/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "mkdir sm_code_artifact\n",
    "mv serving.properties sm_code_artifact/\n",
    "tar czvf sm_code_artifact.tar.gz sm_code_artifact/\n",
    "rm -rf sm_code_artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance_type: ml.g5.12xlarge\n",
      "model_id: mistral-community/Codestral-22B-v0.1\n",
      "s3_code_prefix: mistral-community-codestral-22b-v0x1/sm_code_artifact\n",
      "endpoint_name: codestral-22b-vllm-2024-06-25-13-10-12-119\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Model Id\n",
    "model_id = \"mistral-community/Codestral-22B-v0.1\"\n",
    "\n",
    "# SageMaker Instance Type\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Folder within bucket where code artifact will go\n",
    "s3_code_prefix = f\"\"\"{model_id.replace(\"/\", \"-\").replace(\".\", \"x\").lower()}/sm_code_artifact\"\"\"\n",
    "\n",
    "# Endpoint name\n",
    "endpoint_name_prefix = \"codestral-22b-vllm\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(endpoint_name_prefix)\n",
    "\n",
    "print(f\"instance_type: {instance_type}\")\n",
    "print(f\"model_id: {model_id}\")\n",
    "print(f\"s3_code_prefix: {s3_code_prefix}\")\n",
    "print(f\"endpoint_name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-570598552974/mistral-community-codestral-22b-v0x1/sm_code_artifact/sm_code_artifact.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"sm_code_artifact.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "model = sagemaker.Model(\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=s3_code_artifact,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference and chat with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Supported Inference Parameters\n",
    "\n",
    "---\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code generation questions\n",
    "\n",
    "1. \"Create a Python class for a multi-threaded web scraper that can handle rate limiting, proxy rotation, and dynamic content loading. Include methods for parsing HTML with BeautifulSoup and storing results in a SQLite database.\"\n",
    "2. \"Implement a Red-Black Tree data structure in C++ with methods for insertion, deletion, and rebalancing. Include a visualization function that prints the tree structure to the console.\"\n",
    "3. \"Write a Rust function that implements the Aho-Corasick string matching algorithm for efficient multi-pattern searching. Optimize it for memory usage and include comprehensive error handling.\"\n",
    "4. \"Develop a JavaScript module for a real-time collaborative text editor using operational transformation. Implement functions for handling concurrent edits, conflict resolution, and syncing with a backend server.\"\n",
    "5. \"Create a Python script that uses asyncio to concurrently process large CSV files, perform complex data transformations, and upload the results to an S3 bucket. Include proper error handling and logging.\"\n",
    "6. \"Implement a microservices architecture in Go for a basic e-commerce platform. Include services for user authentication, product catalog, order processing, and inventory management. Use gRPC for inter-service communication and implement circuit breaking for resilience.\"\n",
    "7. \"Provide me with a python script to recompile huggingface models with optimum neuron for inferentia\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize sagemaker client with the endpoint created in the prior step\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a complex task that requires a good understanding of Python, web scraping, and multithreading. Here's a basic outline of how you might approach this:\n",
      "\n",
      "1. Create a `Scraper` class with the following methods:\n",
      "   - `__init__`: Initialize the scraper with a list of proxies and a rate limit.\n",
      "   - `scrape`: Start the scraping process. This should create and start a new thread for each URL in the list.\n",
      "   - `_scrape_url`: A private method that scrapes a single URL. This should handle rate limiting, proxy rotation, and dynamic content loading.\n",
      "   - `_parse_html`: A private method that parses HTML using BeautifulSoup and returns the parsed data.\n",
      "   - `_store_data`: A private method that stores the scraped data in a SQLite database.\n",
      "\n",
      "2. For rate limiting, you can use the `time.sleep` function to pause the scraper for a certain amount of time between requests.\n",
      "\n",
      "3. For proxy rotation, you can use a queue to store the proxies and rotate them by dequeuing a proxy, using it, and then enqueuing it back at the end of the queue.\n",
      "\n",
      "4. For dynamic content loading, you can use a library like Selenium to load the page in a browser and wait for the dynamic content to load before scraping it.\n",
      "\n",
      "Here's a very basic example of what the `Scraper` class might look like:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import sqlite3\n",
      "import time\n",
      "from queue import Queue\n",
      "from threading import Thread\n",
      "\n",
      "class Scraper:\n",
      "    def __init__(self, proxies, rate_limit):\n",
      "        self.proxies = Queue()\n",
      "        for proxy in proxies:\n",
      "            self.proxies.put(proxy)\n",
      "        self.rate_limit = rate_limit\n",
      "        self.db = sqlite3.connect('scraped_data.db')\n",
      "\n",
      "    def scrape(self, urls):\n",
      "        threads = []\n",
      "        for url in urls:\n",
      "            thread = Thread(target=self._scrape_url, args=(url,))\n",
      "            threads.append(thread)\n",
      "            thread.start()\n",
      "        for thread in threads:\n",
      "            thread.join()\n",
      "\n",
      "    def _scrape_url(self, url):\n",
      "        proxy = self.proxies.get()\n",
      "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
      "        response = requests.get(url, headers=headers, proxies={'http': proxy, 'https': proxy})\n",
      "        self.proxies.put(proxy)\n",
      "        time.sleep(self.rate_limit)\n",
      "        data = self._parse_html(response.text)\n",
      "        self._store_data(data)\n",
      "\n",
      "    def _parse_html(self, html):\n",
      "        soup = BeautifulSoup(html, 'html.parser')\n",
      "        # Parse the data you need here\n",
      "        return data\n",
      "\n",
      "    def _store_data(self, data):\n",
      "        cursor = self.db.cursor()\n",
      "        # Insert the data into the database here\n",
      "        self.db.commit()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Create a Python class for a multi-threaded web scraper that can handle rate limiting, proxy rotation, and dynamic content loading. Include methods for parsing HTML with BeautifulSoup and storing results in a SQLite database.\"\n",
    "\n",
    "inputs = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_new_tokens\": 4000,\n",
    "        \"do_sample\": False\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(inputs)\n",
    "print(response['generated_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using Boto3 SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To implement a Red-Black Tree, you'll need to define a Node structure that includes a color, parent, left child, and right child. Then, implement the following methods:\n",
      "\n",
      "1. `insert(int value)`: Inserts a new node with the given value into the tree while maintaining the Red-Black Tree properties.\n",
      "2. `delete(int value)`: Deletes the node with the given value from the tree while maintaining the Red-Black Tree properties.\n",
      "3. `rebalance(Node* node)`: Rebalances the tree by performing rotations and color flips to maintain the Red-Black Tree properties.\n",
      "4. `visualize()`: Prints the tree structure to the console in a readable format.\n",
      "\n",
      "Here's a basic outline of how you can structure your code:\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "\n",
      "enum Color { RED, BLACK };\n",
      "\n",
      "struct Node {\n",
      "    int value;\n",
      "    Color color;\n",
      "    Node* parent;\n",
      "    Node* left;\n",
      "    Node* right;\n",
      "};\n",
      "\n",
      "class RedBlackTree {\n",
      "public:\n",
      "    RedBlackTree();\n",
      "    ~RedBlackTree();\n",
      "\n",
      "    void insert(int value);\n",
      "    void remove(int value);\n",
      "    void visualize();\n",
      "\n",
      "private:\n",
      "    Node* root;\n",
      "\n",
      "    void insertFixup(Node* node);\n",
      "    void leftRotate(Node* node);\n",
      "    void rightRotate(Node* node);\n",
      "    void transplant(Node* u, Node* v);\n",
      "    Node* minimum(Node* node);\n",
      "    void deleteFixup(Node* node);\n",
      "};\n",
      "```\n",
      "\n",
      "You can implement the methods as needed to satisfy the requirements.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Implement a Red-Black Tree data structure in C++ with methods for insertion, deletion, and rebalancing. Include a visualization function that prints the tree structure to the console.\"\n",
    "\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"temperature\": 0.8,\n",
    "                \"top_p\": 0.95,\n",
    "                \"max_new_tokens\": 4000,\n",
    "                \"do_sample\": False\n",
    "            },\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")[\"Body\"].read().decode(\"utf8\")\n",
    "\n",
    "print(json.loads(response)['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this post, we demonstrated how to use SageMaker large model inference containers to host Codestral 22B. We used DeepSpeed’s model parallel techniques with multiple GPUs on a single SageMaker machine learning instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "sess.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In case the end point failed we still want to delete the model\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
