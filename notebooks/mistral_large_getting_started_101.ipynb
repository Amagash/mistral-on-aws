{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Mistral Large on Amazon Bedrock \n",
    "\n",
    "This notebook will walk you through how to get started with Mistral Large on Bedrock and cover prompting 101.\n",
    "\n",
    "The notebook was tested using the Data Science 3.0 kernel in SageMaker Studio.\n",
    "\n",
    "# Amazon Bedrock \n",
    "[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that provides access to a wide range of powerful foundation models (FMs) through a unified API. It offers models from leading AI companies like Mistral, Anthropic, AI21 Labs, Cohere, Stability AI, and Amazon's own Titan models.\n",
    "\n",
    "# Mistral Large\n",
    "[Mistral Large](https://mistral.ai/news/mistral-large/) is the most advanced language model developed by French AI startup Mistral AI\n",
    "\n",
    "Key features of Mistral Large include:\n",
    "- Strong multilingual capabilities, with fluency in English,French, Spanish, German, and Italian\n",
    "\n",
    "- Impressive performance on reasoning, knowledge, math, and coding benchmarks\n",
    "\n",
    "- 32K token context window for processing long documents\n",
    "\n",
    "- Support for function calling and JSON format\n",
    "\n",
    "## Getting access to Mistral Large\n",
    "In order to start using Mistral Large, make sure that you have access to it from the Bedrock console:\n",
    "\n",
    "1. Log in to the AWS Console and navigate to the Amazon Bedrock service\n",
    "\n",
    "2. From the Bedrock home page, click \"Get started\"\n",
    "\n",
    "3. On the left-hand navigation menu, click \"Model access\"\n",
    "\n",
    "4. On the right side of the page, click \"Manage model access\"\n",
    "\n",
    "5. Select the base models you would like access to from the list of available models\n",
    "\n",
    "\n",
    "![Model Access](/imgs/model-access-img.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL= \"mistral.mistral-large-2402-v1:0\"\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\")\n",
    "model_id = DEFAULT_MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported parameters\n",
    "\n",
    "The Mistral AI models have the following [inference parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral.html).\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"max_tokens\" : int,\n",
    "    \"stop\" : [string],    \n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"top_k\": int\n",
    "}\n",
    "```\n",
    "\n",
    "- Temperature - Tunes the degree of randomness in generation. Lower temperatures mean less random generations.\n",
    "- Top P - If set to float less than 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "- Top K - Can be used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.\n",
    "- Maximum Length - Maximum number of tokens to generate. Responses are not guaranteed to fill up to the maximum desired length.\n",
    "- Stop sequences - Up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>The Napoleonic Wars were a series of conflicts that took place between 1803 and 1815, involving various European nations and led by the French Emperor Napoleon Bonaparte. The wars emerged from the French Revolution and its resulting political instability, with Napoleon rising to power as a military leader and eventually crowning himself emperor in 1804.  The Napoleonic Wars can be divided into several phases:  1. The War of the Third Coalition (1803-1806): This phase began when Britain declared war on France, leading to the formation of the Third Coalition, which included Austria, Russia, and other European nations. The most notable battle during this period was the Battle of Trafalgar in 1805, where the British navy, led by Admiral Nelson, defeated the combined French and Spanish fleets.  2. The War of the Fourth Coalition (1806-1807): Prussia joined the coalition against France, but Napoleon's forces quickly defeated them at the Battle of Jena-Auerstedt. The Russians then entered the war, but they too were defeated by Napoleon at the Battle of Friedland. The Treaties of Tilsit in 1807 ended this phase, with France and Russia becoming allies.  3. The Peninsular War (1808-1814): In 1808, Spain and Portugal rose up against French occupation, leading to a prolonged conflict known as the Peninsular War. British forces, led by Arthur Wellesley (later the Duke of Wellington), supported the Spanish and Portuguese resistance, eventually driving the French out of the Iberian Peninsula.  4. The War of the Fifth Coalition (1809): Austria attempted to challenge French dominance in Europe once again, but it was defeated at the Battle of Wagram. The Treaty of Schönbrunn further weakened Austria and strengthened France's position.  5. The Invasion of Russia (1812): Napoleon launched an invasion of Russia in an attempt to enforce his Continental System, a blockade against British trade. Despite initial successes, the French army suffered heavy losses due to Russian scorched-earth tactics, harsh weather conditions, and guerrilla warfare. The remnants of the Grande Armée retreated from Russia in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prompt = \"What is the history behind the Napoleonic Wars?\"\n",
    "body = json.dumps({\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 514,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\":[\"</s>\"]\n",
    "\n",
    "        })\n",
    "response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id\n",
    ")\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "df = pd.DataFrame(\n",
    "    {\"Response\": [response_body['outputs'][0]['text'].replace(\"\\n\", \" \")]}\n",
    "    )\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a prompt\n",
    "\n",
    "A prompt is a natural language instruction that guides a large language model (LLM) to perform a specific task effectively. Just like providing clear guidance to a skilled assistant, well-crafted prompts are crucial for leveraging the full potential of LLMs.\n",
    "\n",
    "An effective prompt should include clear instructions, context, examples, constraints, desired output format, and unbiased language. It may need iterative refinement based on the model's initial response. Appropriate length is also important for coherent and focused outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prompt w/o clear instructions and context</th>\n",
       "      <th>Prompt w/ clear instructions and context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>The value of the cloud is that it enables organizations to be more agile and responsive to changing business needs. It provides a way to quickly and easily provision resources, scale up or down as needed, and pay only for what is used. This can lead to significant cost savings and improved efficiency. Additionally, the cloud can provide access to innovative new technologies and services that can help organizations to better compete in today's digital economy.  ### What is cloud migration?  Cloud migration is the process of moving data, applications, and other IT resources from on-premises infrastructure to the cloud. This can be</td>\n",
       "      <td>The value of the cloud, specifically AWS, lies in several key areas:  1. **Cost Efficiency**: Cloud computing eliminates the expense of setting up and running on-site datacenters, which often includes racks of servers, electricity for power and cooling, and IT experts for managing the infrastructure. AWS uses a pay-as-you-go approach, meaning you only pay for the services you use.  2. **Scalability and Elasticity**: Cloud services like AWS provide the ability to quickly scale up or down based on demand. This means businesses no longer need to predict traffic or invest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define the prompts\n",
    "query = \"What is the value of the cloud?\"\n",
    "prompt_1 = f\"{query}\"\n",
    "prompt_2 = f\"\"\"You are an Amazon Web Services (AWS) Expert. \n",
    "Your role is to answer every technical question about AWS as accurately as possible.\n",
    "If you do not know the answer to a question, say I don't know. Give statistics \n",
    "around usage in the Enterprise world.{query}\"\"\"\n",
    "prompts = [prompt_1, prompt_2]\n",
    "\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    body = json.dumps({\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 128,\n",
    "        \"prompt\": prompt,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    })\n",
    "    response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Prompt w/o clear instructions and context\": [responses[0]],\n",
    "    \"Prompt w/ clear instructions and context\": [responses[1]]\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering \n",
    "Prompt engineering techniques like zero-shot, few-shot, and chain-of-thought prompting can further enhance the quality and behavior of model outputs. Zero-shot relies on the model's general knowledge, few-shot provides examples, and chain-of-thought encourages step-by-step reasoning.\n",
    "\n",
    "Let's take a look at the different techniques in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting:\n",
    "- Zero-shot prompting involves providing a pre-trained language model with a prompt it hasn't seen during training and expecting it to generate a relevant output.\n",
    "- The model relies on its general language understanding and patterns learned during pre-training to produce an output, without any additional examples or fine-tuning.\n",
    "- Zero-shot prompting leverages the model's ability to perform tasks it hasn't been explicitly trained on, using only the information learned from a diverse range of sources.\n",
    "- It is useful in scenarios where the model does not have specific training examples for the given task.\n",
    "- However, zero-shot prompting offers less control over the output compared to few-shot prompting which will be discussed next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Classify the sentiment of the following text: The movie was terrible, I hated every minute of it!?\"\n",
    "body = json.dumps({\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 128,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\":[\"</s>\"]\n",
    "\n",
    "        })\n",
    "response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id\n",
    ")\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "df = pd.DataFrame(\n",
    "    {\"Response\": [response_body['outputs'][0]['text'].replace(\"\\n\", \" \")]}\n",
    "    )\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Prompting:\n",
    "- Few-shot prompting involves providing a language model with a small number of examples (usually 2-5) demonstrating the desired task, along with the input prompt.\n",
    "- The examples serve as a conditioning context for the model, guiding it to generate a response similar to the provided examples.\n",
    "- Few-shot prompting enables the model to quickly adapt to new tasks by leveraging the patterns and structures provided in the examples.\n",
    "- It is more effective than zero-shot prompting for complex tasks and offers better control over the model's output.\n",
    "- The performance of few-shot prompting generally improves with larger model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    follow these examples to extract the keywords from the text.\n",
    "\n",
    "    Text: The hotel room was spacious and clean.\n",
    "    Keywords: hotel room, spacious, clean\n",
    "    Text: The actor's performance was unconvincing and dull. \n",
    "    Keywords: actor's performance, unconvincing, dull\n",
    "    Text: The new iPhone has an excellent camera and long battery life.\n",
    "    keywords:\n",
    "\n",
    "    \"\"\"\n",
    "body = json.dumps({\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 128,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\":[\"</s>\"]\n",
    "\n",
    "        })\n",
    "response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id\n",
    ")\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "df = pd.DataFrame(\n",
    "    {\"Response\": [response_body['outputs'][0]['text'].replace(\"\\n\", \" \")]}\n",
    "    )\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought (CoT) Prompting:\n",
    "- CoT prompting is a technique that encourages language models to explain their reasoning process when solving complex problems.\n",
    "- It involves providing the model with a few examples that include step-by-step reasoning, demonstrating how to break down a problem into intermediate steps.\n",
    "- The model is then expected to follow a similar \"chain of thought\" when answering the prompt, explaining its reasoning process.\n",
    "- CoT prompting is particularly effective for tasks that require arithmetic reasoning, commonsense reasoning, and symbolic reasoning.\n",
    "- It has been shown to improve the performance of large language models on complex reasoning tasks compared to standard prompting.\n",
    "- CoT prompting is an emergent property of model scale, meaning its benefits are more pronounced in larger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1. Let's denote Alice's current age as A and Betty's current age as B. We know that A + B = 42.  2. The statement \"Alice is twice as old as Betty was when Alice was as old as Betty is now\" can be translated into an equation. When Alice was as old as Betty is now (A = B), Betty was (B - A) years younger. So, Alice is twice as old as Betty was then, which can be written as A = 2*(B - A).  3.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\" Alice is twice as old as Betty was when Alice was as old as Betty is now.\n",
    "If the sum of their current ages is 42, how old is Alice now? Let's solve this step-by-step.\"\"\"\n",
    "body = json.dumps({\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 128,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\":[\"</s>\"]\n",
    "\n",
    "        })\n",
    "response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id\n",
    ")\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "df = pd.DataFrame(\n",
    "    {\"Response\": [response_body['outputs'][0]['text'].replace(\"\\n\", \" \")]}\n",
    "    )\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Issues\n",
    "When writing prompts for Large Language Models, there are common pitfalls one may encounter that can impact the quality and accuracy of the model's outputs. Effective prompt engineering is crucial to mitigate these issues and unlock the full potential of LLMs. Some of the most prevalent challenges include:\n",
    "\n",
    "Hallucination and factual inaccuracies: LLMs may generate outputs that contain false or made-up information, particularly when asked about topics outside their training data.\n",
    "\n",
    "Lack of coherence and logical reasoning: The generated text may suffer from logical inconsistencies, contradictions, or lack a coherent structure, especially in complex, multi-step tasks.\n",
    "\n",
    "Difficulty with complex, multi-step tasks: LLMs may struggle to maintain context and generate consistent outputs when faced with intricate, multi-step prompts that require reasoning and problem-solving abilities.\n",
    "\n",
    "Misunderstanding user intent: The model may misinterpret the user's intended meaning or goal, leading to irrelevant or off-topic responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hallucination and factual inaccuracies\n",
    "\n",
    "Hallucination and factual inaccuracies are an issue that can arise when LLM produce text outputs. Since these models are trained on vast amounts of data from the internet and other sources, their knowledge can sometimes be incomplete, biased, or simply incorrect.\n",
    "\n",
    "Solution: Provide clear instructions in the prompt and add any additional context.  In the example below, the context variable acts as the factual information to ground the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response w/o context</th>\n",
       "      <th>Response with context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Project Cobra is a research and development project that aims to create a new generation of autonomous vehicles. The project is led by a team of engineers, computer scientists, and roboticists who are working together to develop advanced technologies for self-driving cars. The goal of Project Cobra is to create vehicles that are safer, more efficient, and more reliable than traditional cars. The project is focused on developing new sensors, algorithms, and control systems that will enable autonomous vehicles to navigate complex environments and make decisions in real-time. Project Cobra is also exploring the use of artificial intelligence and machine learning to improve the</td>\n",
       "      <td>Project Cobra is an ambitious scientific endeavor aiming to develop a method of interstellar travel by creating and manipulating closed timelike curves (CTCs). This involves generating traversable wormholes stabilized with exotic matter, accelerating one wormhole mouth to near lightspeed to induce CTCs inside, which would allow a spacecraft to travel through the wormhole and exit in its own past. If successful, it could revolutionize interstellar travel but also carries huge risks due to potential misuse or unintended consequences.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define the prompts\n",
    "query = \"What is project Cobra?\"\n",
    "context = \"\"\"\n",
    "Project Cobra aims to develop a method of interstellar travel by creating and manipulating closed timelike curves (CTCs) - paths through spacetime that loop back on themselves, allowing travel to the past. The theoretical basis comes from solutions to Einstein's equations of general relativity that permit CTCs in unusual spacetime geometries like wormholes.\n",
    "\n",
    "To achieve this, Project Cobra plans to generate traversable wormholes stabilized with exotic matter, then accelerate one wormhole mouth to near lightspeed to induce CTCs inside. This would allow a spacecraft to travel through the wormhole and exit in its own past, appearing to an outside observer to have moved faster than light.\n",
    "\n",
    "Major challenges include creating and stabilizing the wormholes, accelerating them to form the time machine, navigating through them precisely, and avoiding paradoxes from changing the past.\n",
    "\n",
    "If successful, it could revolutionize interstellar travel by providing shortcuts through spacetime. However, it also carries huge risks of misuse or unintended consequences that would require stringent protocols and safety measures.\n",
    "\n",
    "In essence, it is an extremely ambitious endeavor aiming to translate predictions of general relativity into functional time travel technology for reaching distant stars, albeit with immense theoretical and engineering hurdles to overcome.\"\"\"\n",
    "\n",
    "prompt_1 = f\"{query}\"\n",
    "prompt_2 = f\"\"\"Using the conext given below, answer the question: {query}\n",
    "                Provided context: {context}\"\"\"\n",
    "prompts = [prompt_1, prompt_2]\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    body = json.dumps({\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 128,\n",
    "        \"prompt\": prompt,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    })\n",
    "    response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Response w/o context\": [responses[0]],\n",
    "    \"Response with context\": [responses[1]]\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How to improve coherence in responses \n",
    "Sometimes the response the model gives doesn't make sense or follow a clear logic. This happens because the model is not told to explain its thinking step-by-step. When asked a simple question, the model tries to give a direct answer without showing how it got there. This can lead to confusing or illogical responses.\n",
    "\n",
    "Solution: Use chain-of-thought prompting, where the model is prompted to break down its reasoning into a series of logical steps before providing a final answer. This can significantly improve performance on tasks requiring multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response w/o CoT</th>\n",
       "      <th>Response w/ CoT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Let's break this down:  1. The ratio of blue chairs to red chairs is 2:3.  2. This means that for every group of 5 chairs (2 blue + 3 red), there are 2 blue chairs.  3. To find out how many groups of 5 chairs there are in the classroom, we divide the total number of chairs by 5.  4. In this case, there are 30 chairs / 5 = &lt;&lt;30/5=6&gt;&gt;6 groups of 5 chairs.  5. Since each group contains 2 blue chairs, we multiply the number of groups by 2 to find the total number of blue chairs.  6. Therefore, there are 6 groups * 2 blue chairs/group = &lt;&lt;6*2=12&gt;&gt;12 blue chairs in the classroom.</td>\n",
       "      <td>Let's break this down:  1. The ratio of blue chairs to red chairs is 2:3. This means for every group of 5 chairs (2 blue + 3 red), there are 2 blue chairs. 2. If there are 30 chairs in total, we need to find out how many groups of 5 chairs there are. We do this by dividing the total number of chairs by 5.    Number of groups = Total chairs / Chairs per group    = 30 / 5    = 6 groups  3. Now that we know there are 6 groups of chairs, and each group has 2 blue chairs, we can calculate the total number of blue chairs.    Total blue chairs = Number of groups * Blue chairs per group    = 6 * 2    = 12 blue chairs  So, there are 12 blue chairs in the classroom.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the prompts\n",
    "query = \"\"\"A classroom has two blue chairs for every three red chairs.\n",
    " If there are a total of 30 chairs in the classroom, how many blue chairs are there?\"\"\"\n",
    "\n",
    "prompt_1 = f\"{query}\"\n",
    "prompt_2 = f\"\"\"Think-step-by-step and answer the following question: {query}\"\"\"\n",
    "prompts = [prompt_1, prompt_2]\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    body = json.dumps({\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 512,\n",
    "        \"prompt\": prompt,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    })\n",
    "    response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Response w/o CoT\": [responses[0]],\n",
    "    \"Response w/ CoT\": [responses[1]]\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How to tackle complex, multi-step tasks\n",
    "When faced with complex, multi-step tasks, prompting can be a significant challenge. The issue lies in the model's ability to comprehend and tackle tasks that require multiple steps, conditional logic, and critical thinking. Without proper guidance, the model may struggle to identify the necessary steps, prioritize tasks, or even understand the context of the problem. This can lead to incomplete, inaccurate, or irrelevant responses. Furthermore, complex tasks often require a deep understanding of the problem domain, making it difficult for the model to generate a coherent and logical solution.\n",
    "\n",
    "Solution: Decompose the complex task into a sequence of simpler sub-tasks in the prompt. Guide the model to solve each sub-task step-by-step. Least-to-most prompting, where sub-problems are solved in order of increasing difficulty, can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Prompt 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Step 5: Plan the table setting  Step 1: Appetizer: Caprese Salad Main Course: Chicken Parmesan Dessert: Tiramisu  Step 2: Grocery List: - 3 fresh mozzarella balls - 2 bunches of fresh basil - 3 tomatoes - Balsamic glaze - Olive oil - Salt and pepper - 6 chicken breasts - 2 cups of grated parmesan cheese - 2 cans of marinara sauce - 1 box of spaghetti - 2 cups of strong coffee - 1 package of ladyfingers - 1/4 cup of sugar - 1/2 cup of heavy cream - 16 oz of mascarpone cheese - Cocoa powder  Step 3: - Cutting board - Knife - Large mixing bowl - Small mixing bowl - Large skillet - Large pot - Baking dish - Electric mixer - Rubber spatula - Serving dishes  Step 4: Timeline: 1. Preheat oven to 375°F (190°C) 2. Prepare Caprese Salad (15 minutes) 3. Start cooking spaghetti according to package instructions (10 minutes) 4. Prepare Chicken Parmesan (30 minutes) 5. While chicken is baking, prepare Tiramisu (20 minutes) 6. Let Tiramisu chill in the fridge for at least 2 hours before serving  Step 5: Table Setting: - 6 dinner plates - 6 salad plates - 6 sets of cutlery (fork, knife, spoon) - 6 wine glasses - 6 water glasses - Tablecloth - Napkins - Centerpiece (optional)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the prompts\n",
    "query = \"The task is to plan a 3-course meal for 6 people.\"\n",
    "prompt = f\"\"\"{query} Do not add any preamble.Using the following subtasks:\n",
    "            Step 1: Choose recipes for appetizer, main course, and dessert\n",
    "            Step 2: Make a grocery list of all required ingredients \n",
    "            Step 3: Determine cooking utensils, pots/pans, bakeware needed\n",
    "            Step 4: Come up with a timeline to prep each course\"\"\"\n",
    "prompts = [prompt]\n",
    "\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    body = json.dumps({\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"prompt\": prompt,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    })\n",
    "    response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Response\": [responses[0]]\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Misunderstanding user intent\n",
    "\n",
    "Misunderstanding user intent is a common issue in prompting, where the model fails to grasp the user's underlying goals, needs, or requirements. This can lead to responses that are off-topic, irrelevant, or unhelpful. The root cause of this issue lies in the model's inability to understand the context and nuances of human communication, such as implied meaning, tone, and intent. Without clear guidance, the model may misinterpret the user's query, leading to a mismatch between the user's expectations and the model's response.\n",
    "\n",
    "Solution: Provide clear context and instructions in the prompt to guide the model. Use role prompting to define the model's persona and expertise to better address the user's needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Healthy eating principles include focusing on whole, unprocessed foods, maintaining a balanced diet, and managing portion sizes. Consuming whole foods like fruits, vegetables, lean proteins, and whole grains provides essential nutrients and helps limit the intake of added sugars, salts, and unhealthy fats. A balanced diet involves consuming a variety of these foods to ensure adequate intake of different nutrients. Lastly, being mindful of portion sizes can help prevent overeating and maintain a healthy weight.  To find the number of blue chairs in the classroom, first determine the ratio of blue to red chairs, which is 2:3. Since there are a total of 30 chairs, you can set up a proportion: (2 blue chairs) / (2+3 total parts) = (x blue chairs) / (30 total chairs). Solving for x, we find that there are 12 blue chairs in the classroom.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the prompts\n",
    "prompt_1 = f\"\"\"Do not give a preamble.You are a nutritonist. The audience is adults looking to improve their eating habits.\n",
    " Provide a 3-4 sentence paragraph clearly explaining 2-3 key principles of healthy eating that are backed by current nutritional research. {query}\"\"\"\n",
    "prompts = [prompt_1]\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    body = json.dumps({\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_tokens\": 512,\n",
    "        \"prompt\": prompt,\n",
    "        \"stop\": [\"</s>\"]\n",
    "    })\n",
    "    response = bedrock.invoke_model(\n",
    "    body = body,\n",
    "    modelId = model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Response\": [responses[0]]\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral Large vs Mixtral 8x7B\n",
    "\n",
    "Mistral Large and Mixtral 8x7B are AI models with distinct differences in capabilities, performance, and applications. \n",
    "### Capabilities:\n",
    "#### Mistral Large:\n",
    "- Multi-step reasoning and problem-solving\n",
    "- Handling complex, open-ended prompts\n",
    "- Advanced natural language understanding (NLU)\n",
    "- Long-range dependencies and contextual reasoning\n",
    "- General knowledge and information retrieval\n",
    "- Creative writing and text generation\n",
    "#### Mixtral 8x7B:\n",
    "- Efficient and fast processing for real-time applications\n",
    "- Specialized for specific tasks and domains (e.g., customer support, language translation)\n",
    "- Handling simple to moderately complex prompts\n",
    "- Basic NLU and contextual understanding\n",
    "### Performance:\n",
    "#### Mistral Large:\n",
    "- Excellent performance in complex tasks (e.g., multi-step reasoning, long-range dependencies)\n",
    "- High accuracy in general knowledge and information retrieval\n",
    "- Advanced creative writing and text generation capabilities\n",
    "#### Mixtral 8x7B:\n",
    "- Fast and efficient processing for real-time applications\n",
    "- Good performance in specific tasks and domains\n",
    "- Moderate accuracy in basic NLU and contextual understanding\n",
    "### Applications:\n",
    "#### Mistral Large:\n",
    "- Research and development\n",
    "- Advanced natural language processing (NLP) tasks\n",
    "- Creative writing and content generation\n",
    "- Complex problem-solving and decision-making\n",
    "#### Mixtral 8x7B:\n",
    "- Customer support and chatbots\n",
    "- Language translation and localization\n",
    "- Data analysis and processing\n",
    "\n",
    "### Training and Resources:\n",
    "#### Mistral Large:\n",
    "- Trained on large, diverse datasets\n",
    "#### Mixtral 8x7B:\n",
    "- Trained on smaller, specialized datasets\n",
    "\n",
    "### Inference Speed:\n",
    "- Mixtral 8x7B is significantly faster than Mistral Large during inference, making it suitable for real-time applications.\n",
    "\n",
    "### Specialization:\n",
    "- Mistral Large is a general-purpose model, while Mixtral 8x7B is more specialized for specific tasks and domains.\n",
    "### Prompting:\n",
    "- Mistral Large can handle more abstract and open-ended prompts, while Mixtral 8x7B requires more specific and guided prompting.\n",
    "### Error Handling:\n",
    "- Mistral Large is more robust to errors and can recover from mistakes more effectively, while Mixtral 8x7B may be more prone to errors and require more careful prompting.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "The following prompts demonstrate the contrasting capabilities of the Mixtral 8x7b and Mistral Large models in terms of critical thinking and multilingual proficiency. While the analytical responses from Mixtral 8x7b are not inaccurate, they lack the depth and thoroughness exhibited by Mistral Large. Regarding the multilingual examples, Mistral Large's translations consider the contextual nuances rather than adhering to a literal word-for-word approach. This contextual awareness allows Mistral Large's translations to convey the intended meaning more precisely and coherently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mixtral 8x7b Analytical Prompt</th>\n",
       "      <th>Mistral Large Analytical Prompt</th>\n",
       "      <th>Mixtral 8x7b Multilingual Prompt</th>\n",
       "      <th>Mistral Large Multilinigual Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Correctness: The function is not correct as it does not return the nth Fibonacci number for n &gt; 1. It enters into an infinite recursion due to the recursive calls in the else part.  Efficiency: Since the function never terminates, we cannot discuss its efficiency. However, if we were to assume that the function works correctly, then its efficiency would be very poor due to the repeated computation of the same Fibonacci numbers in the recursive calls.  To fix the function and make it efficient, we can use dynamic programming to store the computed Fibonacci numbers in a cache and reuse them when needed. Here's an example implementation:  def fibonacci(n, cache={}): if n &lt;= 0: return 0 elif n == 1: return 1 elif n in cache: return cache[n] else: cache[n] = fibonacci(n-1) + fibonacci(n-2) return cache[n]  This implementation has a time complexity of O(n) and a space complexity of O(n) due to the cache.</td>\n",
       "      <td>The function `fibonacci(n)` is a recursive implementation of the Fibonacci sequence. It correctly calculates the nth Fibonacci number based on the mathematical definition.  However, in terms of efficiency, this function is not optimal. The time complexity of this function is exponential (O(2^n)) due to the repeated computation involved in the recursive calls. This is because for each call to `fibonacci(n)`, it makes two additional calls, one to `fibonacci(n-1)` and another to `fibonacci(n-2)`. As a result, the function ends up calculating the same Fibonacci numbers multiple times.  To improve the efficiency of this function, you could use dynamic programming to store previously calculated Fibonacci numbers and reuse them instead of recalculating. This approach, known as memoization, can reduce the time complexity to linear (O(n)).  Here's an example of how you could implement the Fibonacci function using memoization:  ```python def fibonacci(n, memo={}):     if n &lt;= 0:         return 0     elif n == 1:         return 1     elif n not in memo:         memo[n] = fibonacci(n-1) + fibonacci(n-2)     return memo[n] ```  In this version of the function, a dictionary `memo` is used to store previously calculated Fibonacci numbers. Before calculating `fibonacci(n)`, the function checks if `n` is in `memo`. If it is, the function returns the stored value. If it's not, the function calculates the value, stores it in `memo`, and then returns it. This way, each Fibonacci number is only calculated once.</td>\n",
       "      <td>The sun is shining brightly today.\"  Spanish:              \"El zorro pardo rápido salta sobre el perro perezoso.               El sol está brillando intensamente hoy.\"  French:              \"Le renard brun rapide saute par-dessus le chien paresseux.               Le soleil brille vivement aujourd'hui.\"  German:              \"Der schnelle braune Fuchs springt über den faulen Hund.               Die Sonne scheint heute sehr hell.\"</td>\n",
       "      <td>If you can translate this, you can translate anything!\"  Spanish: \"El rápido zorro marrón salta sobre el perro perezoso. ¡Si puedes traducir esto, ¡puedes traducir cualquier cosa!\"  French: \"Le renard brun rapide saute par-dessus le chien paresseux. Si vous pouvez traduire ceci, vous pouvez traduire n'importe quoi!\"  German: \"Der schnelle braune Fuchs springt über den faulen Hund. Wenn Sie dies übersetzen können, können Sie alles übersetzen!\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the prompts\n",
    "\n",
    "prompt_1 = \"\"\"Evaluate this Python function for correctness and efficiency:\n",
    "            def fibonacci(n):\n",
    "            if n <= 0:\n",
    "            return 0\n",
    "            elif n == 1:\n",
    "            return 1\n",
    "            else:\n",
    "            return fibonacci(n-1) + fibonacci(n-2)\"\"\"\n",
    "prompt_2 = \"\"\"Translate the following text to Spanish, French and German:\n",
    "             \"The quick brown fox jumps over the lazy dog.\"\"\"\n",
    "prompts = [prompt_1, prompt_2]\n",
    "model_ids = [\"mistral.mixtral-8x7b-instruct-v0:1\", DEFAULT_MODEL]\n",
    "\n",
    "# Iterate through the prompts and get the responses\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    for model_id in model_ids:\n",
    "        body = json.dumps({\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 1024,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\": [\"</s>\"]\n",
    "        })\n",
    "        response = bedrock.invoke_model(\n",
    "        body = body,\n",
    "        modelId = model_id)\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        responses.append(response_body[\"outputs\"][0][\"text\"].replace(\"\\n\", \" \"))\n",
    "\n",
    "# Create a pandas DataFrame to display the responses side-by-side\n",
    "df = pd.DataFrame({\n",
    "    \"Mixtral 8x7b Analytical Prompt\": [responses[0]],\n",
    "    \"Mistral Large Analytical Prompt\": [responses[1]],\n",
    "    \"Mixtral 8x7b Multilingual Prompt\": [responses[2]],\n",
    "    \"Mistral Large Multilinigual Prompt\": [responses[3]],\n",
    "    \n",
    "\n",
    "})\n",
    "\n",
    "# Display the DataFrame as HTML\n",
    "display_html(df.to_html(index=False), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we discussed the common pitfalls that can arise when dealing with prompts and large language models (LLMs). These pitfalls include hallucination and factual inaccuracies, lack of coherence and logical reasoning in generated text, difficulty with complex multi-step tasks, and misunderstanding user intent.\n",
    "\n",
    "We then explored how to create strong prompts and the importance of prompt engineering techniques to mitigate these issues. The key prompt engineering techniques covered were zero-shot prompting, few-shot prompting, and chain-of-thought (CoT) prompting.\n",
    "\n",
    "Zero-shot prompting relies solely on the model's pre-existing knowledge without any additional examples. Few-shot prompting provides a small number of examples to guide the model's outputs. CoT prompting encourages the model to explain its reasoning process step-by-step, which is particularly useful for complex reasoning tasks.\n",
    "\n",
    "Finally, we discussed the differences between the Mixtral 8x7b and Mistral Large models and their respective use cases. While Mixtral 8x7b can provide analytical responses, Mistral Large offers more thorough and contextually aware outputs, especially for multilingual tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
