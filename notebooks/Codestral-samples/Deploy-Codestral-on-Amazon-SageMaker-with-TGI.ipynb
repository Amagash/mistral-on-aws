{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Codestral on Amazon SageMaker with TGI/LMI\n",
    "\n",
    "---\n",
    "\n",
    "[Codestral](https://mistral.ai/news/codestral/) is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers. Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral can assist developers in various coding environments and projects.\n",
    "\n",
    "In this notebook, you will learn how to deploy the [mistralai/Codestral-v0.1](https://huggingface.co/mistralai/Codestral-22B-v0.1) model to [Amazon SageMaker](https://aws.amazon.com/sagemaker/). We will utilize the Hugging Face LLM DLC, a purpose-built Inference Container designed to facilitate the deployment of Large Language Models (LLMs) in a secure and managed environment. This Deep Learning Container (DLC) is powered by <b>Text Generation Inference (TGI)</b>, a scalable and optimized solution for deploying and serving LLMs efficiently. Additionally, you are also able to use the <b>Language Model Inference (LMI)</b> container as an alternative DLC within this notebook. LMIs are specialized Docker containers for LLM inference, provided by AWS. With these containers, you can leverage high performance open-source inference libraries like vLLM, TensorRT-LLM, Transformers NeuronX to deploy LLMs on AWS SageMaker Endpoints. Detailed instance requirements for various model sizes will also be provided to ensure optimal deployment configurations. \n",
    "\n",
    "<b><i>To deploy Codestral on to Sagemaker with DeepSpeed, please refer to the 'Deploy Codestral on DeepSpeed' notebook located in this folder.</b></i>\n",
    "\n",
    "\n",
    "In the blog will cover how to:\n",
    "1. [Set up environment](#1-set-up-environment)\n",
    "2. [Retrieve the DLC](#2-retrieve-the-dlc)\n",
    "3. [Hardware requirements](#3-hardware-requirements)\n",
    "4. [Deploy Codestral to Amazon SageMaker](#4-deploy-codestral-to-amazon-sagemaker)\n",
    "5. [Run inference and chat with the model](#5-run-inference-and-chat-with-the-model)\n",
    "6. [Clean up](#5-clean-up)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> Codestral is a 22B open-weight model licensed under the new Mistral AI Non-Production License, which means that you can use it for research and testing purposes. Codestral can be downloaded on HuggingFace.\n",
    "If you want to use the model in the course of commercial activity, Commercial licenses are also available on demand by reaching out to the team.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reach out to Mistral to explore Codestral for commercial use cases: [Contact the Mistral team](https://mistral.ai/contact/)\n",
    "\n",
    "##### More on the Mistral AI Non-Production License: [Mistral AI Non-Production License](https://mistral.ai/news/mistral-ai-non-production-license-mnpl/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up environment\n",
    "\n",
    "##### Local Setup (Optional)\n",
    "\n",
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "This Jupyter Notebook can be run on a t3.medium instance (ml.t3.medium). However, to deploy `Mixtral 8X7B Instruct` and `BGE Large En` models, you may need to request a quota increase. \n",
    "\n",
    "To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `ml.g5.12xlarge` for endpoint usage\n",
    "   - `ml.g5.48xlarge` for endpoint usage\n",
    "4. If needed, request a quota increase for these resources.\n",
    "\n",
    "---\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "If using the `sagemaker` python SDK to deploy Codestral to Amazon SageMaker, we need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. \n",
    "\n",
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)- For Notebook Instance type, choose (ml.t3.medium).\n",
    "    \n",
    "2. For Select Kernel, choose [conda_pytorch_p310](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages to run this notebook\n",
    "!pip install sagemaker==2.219 --quiet # NOTE: Please use version 2.219 of sagemaker with this notebook\n",
    "!pip install gradio  --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Import Necessary Libraries\n",
    "\n",
    "In the below section, we import the necessary libraries to run this notebook.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> In order to be able to run the gradio app for Codestral, please ensure you have cloned in/replicated the <b>codestral_chat_ui</b> subfolder with the codestral_chat module\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "2.219.0\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import os\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "if not sagemaker.__version__ >= \"2.219.0\": print(\"You need to upgrade or restart the kernel if you already upgraded\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrieve the DLC\n",
    "\n",
    "#### 2.a. Retrieve the latest HuggingFace DLC\n",
    "\n",
    "The first step is to retrieve the DLC URI. This URI is crucial as it serves as a reference point for the HuggingFaceModel class, specifically through the image_uri parameter. The DLC is a pre-configured Docker image that encapsulates all the necessary dependencies and frameworks required to run our LLM efficiently in the SageMaker environment.\n",
    "To streamline this process, the sagemaker SDK provides a specialized method called `get_huggingface_llm_image_uri`. This method is designed to retrieve the most suitable Hugging Face LLM DLC URI based on three key parameters:\n",
    "\n",
    "<b>backend</b>: This specifies the deep learning inference framework, in this case which can be huggingface/tgi, lmi, etc.\n",
    "\n",
    "<b>region</b>: This refers to the AWS region where you're deploying your model. It's important to use the correct region to ensure optimal performance and compliance with data residency requirements.\n",
    "\n",
    "<b>version</b>: This indicates the specific version of the Hugging Face Transformers library you wish to use. Keeping this up-to-date ensures access to the latest features and optimizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi2.0.1-gpu-py310-cu121-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "# retrieve the huggingface llm image uri\n",
    "tgi_image_uri = get_huggingface_llm_image_uri(\n",
    "  backend=\"huggingface\", #tgi\n",
    "  region=region\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"huggingface llm image uri: {tgi_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b. Retrieve the latest LMI container\n",
    "\n",
    "LMI containers are a set of high-performance Docker Containers purpose built for large language model (LLM) inference. With these containers, you can leverage high performance open-source inference libraries like Deepspeed to deploy LLMs on AWS SageMaker Endpoints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmi image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-deepspeed0.10.0-cu118\n"
     ]
    }
   ],
   "source": [
    "# retrieve the lmi image uri\n",
    "lmi_image_uri = get_huggingface_llm_image_uri(\n",
    "  backend=\"lmi\", \n",
    "  region=region\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"lmi image uri: {lmi_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above section is meant to show how lmi images can be retrieved. To deploy Codestral on to Sagemaker with `DeepSpeed`, please refer to the `'Deploy Codestral on DeepSpeed'` notebook located in this folder.\n",
    "The below sections will use `TGI` as the default deep learning inference framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hardware requirements\n",
    "\n",
    "[Codestral](https://huggingface.co/mistralai/Codestral-22B-v0.1) is a 22B parameter open-weight model. It has a context length of 32k tokens and Mistral recommends that we use [mistral-inference](https://github.com/mistralai/mistral-inference). For the current deployment with TGI on Sagemaker, the model is limited to a maximum of 4096 tokens (input + max_new_tokens). In this case, it is sufficient to use a g5.12xlarge instance with 96GB of VRAM. For a 22B parameter model at BF16, this would be more than enough memeory to load in the model and perform inference, since the max length of generation including input text is 4096. For the purpose of this notebook, we will just be deploying the unquantized version of the model to a sagemaker endpoint with TGI on the g5.12xlarge.\n",
    "\n",
    "\n",
    "| Model                                                                       | Instance Type       | Quantization | NUM_GPUS | VRAM |\n",
    "|-----------------------------------------------------------------------------|---------------------|--------------|----------|------|\n",
    "| [Codestral](https://huggingface.co/mistralai/Codestral-22B-v0.1) | `(ml.)g5.12xlarge` | `-` / bitsnbytes (8-bit)        | 4        | 96GB |\n",
    "| [Codestral](https://huggingface.co/mistralai/Codestral-22B-v0.1) | `(ml.)g5.48xlarge`  | `-` / bitsnbytes (8-bit)        | 8        | 192GB |\n",
    "\n",
    "\n",
    "The parameters for `MAX_INPUT_LENGTH`, `MAX_TOTAL_TOKENS` and `MAX_BATCH_TOTAL_TOKENS` can be altered as needed. In this notebook, we run the default configuration without altering these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy Codestral to Amazon SageMaker\n",
    "\n",
    "To deploy [Codestral](https://huggingface.co/mistralai/Codestral-22B-v0.1) to Amazon SageMaker we create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type`, and `huggingface_hub_token`. We will use a `g5.12xlarge` instance type, which has 4 NVIDIA A10G GPUs and 192GB of GPU memory.You are also able to change the instance type to the `g5.48xlarge` as well if needed by changing `instance_type` in the sagemaker endpoint configuration. Depending on the instance type being used, you will also need to chnage the `number_of_gpus` to reflect this (refer to the table above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sagemaker endpoint configuration\n",
    "instance_type = \"ml.g5.12xlarge\"#/g5.12xlarge \n",
    "number_of_gpus = 4\n",
    "health_check_timeout = 900\n",
    "\n",
    "\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"mistralai/Codestral-22B-v0.1\", # model_id from HuggingFace\n",
    "  'HF_TASK': \"text-generation\", # huggingface inference pipeline\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpus), # Number of GPU used per replica\n",
    "  'HUGGING_FACE_HUB_TOKEN': \"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\", #add your huggingface hub access token with read permissions\n",
    "  #not supported currently - 'HF_MODEL_QUANTIZE': 'bitsandbytes' # You are also able to quantize the model to 8-bit quantization to further improve performance at the cost of a certain degree of loss to precision\n",
    "}\n",
    "\n",
    "# check if token is set\n",
    "assert config['HUGGING_FACE_HUB_TOKEN'] !=\"<REPLACE WITH YOUR TOKEN>\", \"Please set your Hugging Face Hub token\"\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=tgi_image_uri, #switch to lmi if needed\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.g5.48xlarge` instance type. TGI will automatically distribute and shard the model across all GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run inference and chat with the model\n",
    "\n",
    "After our endpoint is deployed we can run inference on it. Parameters can be defined as in the `parameters` attribute of the payload.\n",
    "\n",
    "The mistral models have the following prompt structure:\n",
    "  \n",
    "```\n",
    "<s> [INST] User Instruction 1 [/INST] Model answer 1</s> [INST] User instruction 2 [/INST]\n",
    "```\n",
    "Let's now define our prompt and set the parameters for our payload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sample code generation questions:\n",
    "1. \"Create a Python class for a multi-threaded web scraper that can handle rate limiting, proxy rotation, and dynamic content loading. Include methods for parsing HTML with BeautifulSoup and storing results in a SQLite database.\"\n",
    "2. \"Implement a Red-Black Tree data structure in C++ with methods for insertion, deletion, and rebalancing. Include a visualization function that prints the tree structure to the console.\"\n",
    "3. \"Write a Rust function that implements the Aho-Corasick string matching algorithm for efficient multi-pattern searching. Optimize it for memory usage and include comprehensive error handling.\"\n",
    "4. \"Develop a JavaScript module for a real-time collaborative text editor using operational transformation. Implement functions for handling concurrent edits, conflict resolution, and syncing with a backend server.\"\n",
    "5. \"Create a Python script that uses asyncio to concurrently process large CSV files, perform complex data transformations, and upload the results to an S3 bucket. Include proper error handling and logging.\"\n",
    "6. \"Implement a microservices architecture in Go for a basic e-commerce platform. Include services for user authentication, product catalog, order processing, and inventory management. Use gRPC for inter-service communication and implement circuit breaking for resilience.\"\n",
    "7. Provide me with a python script to recompile huggingface models with optimum neuron for inferentia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code Generation prompt\n",
    "prompt=f\"<s> [INST] Create a Python script that uses asyncio to concurrently process large CSV files, perform complex data transformations, and upload the results to an S3 bucket. Include proper error handling and logging [/INST] \"\n",
    "\n",
    "# payload params\n",
    "payload = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"stop\": [\"</s>\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay lets test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Create a Python script that uses asyncio to concurrently process large CSV files, perform complex data transformations, and upload the results to an S3 bucket. Include proper error handling and logging [/INST]  Sure, I can help you outline a basic structure for this script. However, please note that I can't run the code here, but I can provide you with a sample code that you can run in your local environment.\n",
      "\n",
      "Here's a basic outline of how you might structure this script:\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "import csv\n",
      "import boto3\n",
      "import logging\n",
      "from io import StringIO\n",
      "\n",
      "# Set up logging\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "\n",
      "# Set up S3 client\n",
      "s3 = boto3.client('s3')\n",
      "\n",
      "async def process_csv(file_path):\n",
      "    try:\n",
      "        # Open the CSV file\n",
      "        with open(file_path, 'r') as file:\n",
      "            # Read the CSV file\n",
      "            reader = csv.reader(file)\n",
      "\n",
      "            # Perform data transformations\n",
      "            transformed_data = [transform(row) for row in reader]\n",
      "\n",
      "        # Convert the transformed data to a CSV string\n",
      "        csv_buffer = StringIO()\n",
      "        writer = csv.writer(csv_buffer)\n",
      "        writer.writerows(transformed_data)\n",
      "\n",
      "        # Upload the transformed data to S3\n",
      "        s3.put_object(Body=csv_buffer.getvalue(), Bucket='your-bucket-name', Key='transformed-data.csv')\n",
      "\n",
      "        logging.info(f'Successfully processed and uploaded {file_path}')\n",
      "\n",
      "    except Exception as e:\n",
      "        logging.error(f'Error processing {file_path}: {e}')\n",
      "\n",
      "async def main():\n",
      "    # List of CSV files to process\n",
      "    files = ['file1.csv', 'file2.csv', 'file3.csv']\n",
      "\n",
      "    # Process the files concurrently\n",
      "    await asyncio.gather(*(process_csv(file) for file in files))\n",
      "\n",
      "# Run the main function\n",
      "asyncio.run(main())\n",
      "```\n",
      "\n",
      "This script does the following:\n",
      "\n",
      "1. Sets up logging and an S3 client.\n",
      "2. Defines an `async` function `process_csv` that opens a CSV file, performs data transformations, and uploads the transformed data to an S3 bucket.\n",
      "3. Defines an `async` function `main` that creates a list of CSV files to process and processes them concurrently using `asyncio.gather`.\n",
      "4. Runs the `main` function using `asyncio.run`.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "chat = llm.predict({\"inputs\":prompt, \"parameters\":payload})\n",
    "\n",
    "print(chat[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Streaming Responses with a Gradio Application\n",
    "\n",
    "[Amazon SageMaker supports streaming responses](https://aws.amazon.com/de/blogs/machine-learning/elevating-the-generative-ai-experience-introducing-streaming-support-in-amazon-sagemaker-hosting/) from your model. Using this capability, in the below section, let's build a gradio application to stream responses.\n",
    "\n",
    "Th code for the gradio application in the following steps can be found in [codestral_chat.py](../codestral_chat_ui/codestral_chat.py). The application will stream the responses from the model and display them in the UI. You can also use the application to test your model with your own inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://0f8abc81822edfef79.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0f8abc81822edfef79.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add directory to path\n",
    "sys.path.append(\"codestral_chat_ui\") \n",
    "from codestral_chat import create_gradio_app\n",
    "# params\n",
    "parameters = {\n",
    "   \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"stop\": [\"</s>\"]\n",
    "}\n",
    "\n",
    "# define format function for our input\n",
    "def format_prompt(message, history, system_prompt):\n",
    "    prompt = \"\"\n",
    "    for user_prompt, bot_response in history:\n",
    "        prompt = f\"<s> [INST] {user_prompt} [/INST] {bot_response}</s>\"\n",
    "        prompt += f\"### Instruction\\n{user_prompt}\\n\\n\"\n",
    "        prompt += f\"### Answer\\n{bot_response}\\n\\n\"  \n",
    "    # add new user prompt if history is not empty\n",
    "    if len(history) > 0:\n",
    "        prompt += f\" [INST] {message} [/INST] \"    \n",
    "    else:\n",
    "        prompt += f\"<s> [INST] {message} [/INST] \"\n",
    "    return prompt\n",
    "\n",
    "# create gradio app\n",
    "create_gradio_app(\n",
    "    llm.endpoint_name,           # Sagemaker endpoint name\n",
    "    session=sess.boto_session,   # boto3 session used to send request \n",
    "    parameters=parameters,       # Request parameters\n",
    "    system_prompt=None,          # System prompt to use -> Mistral does not support system prompts\n",
    "    format_prompt=format_prompt, # Function to format prompt\n",
    "    share=True,                  # Share app publicly\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
