{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0488cae-f2b2-4d0f-9e42-7cd4faae07d8",
   "metadata": {},
   "source": [
    "# Deploy Mixtral 8x7b with NVIDIA NIM on Amazon SageMaker\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf191c-ac98-4d56-a6e5-a43e6de87b13",
   "metadata": {},
   "source": [
    "## What is NIM\n",
    "\n",
    "[NVIDIA NIM](https://catalog.ngc.nvidia.com/orgs/nim/teams/mistralai/containers/mixtral-8x7b-instruct-v01) enables efficient deployment of large language models (LLMs) across various environments, including cloud, data centers, and workstations. It simplifies self-hosting LLMs by providing scalable, high-performance microservices optimized for NVIDIA GPUs. NIM's containerized approach allows for easy integration into existing workflows, with support for advanced language models and enterprise-grade security. Leveraging GPU acceleration, NIM offers fast inference capabilities and flexible deployment options, empowering developers to build powerful AI applications such as chatbots, content generators, and translation services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b92143-a092-4799-a492-8e0ee0908176",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "NIM abstracts away model inference internals such as execution engine and runtime operations. They are also the most performant option available whether it be with [TRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [vLLM](https://github.com/vllm-project/vllm) or others. NIM offers the following high performance features:\n",
    "\n",
    "1. Scalable Deployment that is performant and can easily and seamlessly scale from a few users to millions.\n",
    "2. Advanced Language Model support with pre-generated optimized engines for a diverse range of cutting edge LLM architectures.\n",
    "3. Flexible Integration to easily incorporate the microservice into existing workflows and applications. Developers are provided with an OpenAI API compatible programming model and custom NVIDIA extensions for additional functionality.\n",
    "4. Enterprise-Grade Security emphasizes security by using safetensors, constantly monitoring and patching CVEs in our stack and conducting internal penetration tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007649f-5837-4575-a7a9-a2b6c730f5a0",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "NIMs are packaged as container images on a per model/model family basis. Each NIM is its own Docker container with a model, such as [mistralai/Mixtral-8x7B](https://huggingface.co/mistralai). These containers include a runtime that runs on any NVIDIA GPU with sufficient GPU memory, but some model/GPU combinations are optimized. NIMs are distributed as [NGC](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html) container images through the NVIDIA NGC Catalog. NIM automatically downloads the model from NGC, leveraging a local filesystem cache if available. Each NIM is built from a common base, so once a NIM has been downloaded, downloading additional NIMs is extremely fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1048b66-14f3-4f47-91fd-e653979c7cd5",
   "metadata": {},
   "source": [
    "In this example we show how to deploy `Mixtral 8x7B` on a `p4d.24xlarge` instance with NIM on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d95e7-4bcc-4c83-90bd-1c492c67aa24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Card\n",
    "---\n",
    "### Mixtral 8x7B Instruct\n",
    "\n",
    "- **Description:** A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Utilizes 12B active parameters out of 45B total.\n",
    "- **Max Tokens:** 4,096\n",
    "- **Context Window:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935d62c-0675-4a62-9e46-d9e854491a26",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1599941-1c76-4352-b1c3-eca6f4a65aaa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b>  To run NIM on SageMaker you will need to have your `NGC API KEY` to access NGC resources. Check out <a href=\"https://build.nvidia.com/mistralai/mixtral-8x7b-instruct\"> this LINK</a> to learn how to get an NGC API KEY. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd7912-b1cb-478f-b344-ceb2f0bfb770",
   "metadata": {},
   "source": [
    "##### 1. Setup and retrieve API key:\n",
    "\n",
    "1. First you will need to sign into [NGC](9https://ngc.nvidia.com/signin) with your NVIDIA account and password.\n",
    "2. Navigate to setup.\n",
    "3. Select “Get API Key”.\n",
    "4. Generate your API key.\n",
    "5. Keep your API key secret and in a safe place. Do not share it or store it in a place where others can see or copy it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1f3df-a66e-490e-b4dc-7aa7b3a0ed6e",
   "metadata": {},
   "source": [
    "For more information on NIM, check out the [NIM LLM docs](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb4d59-c7ec-45de-9018-8c05052625ce",
   "metadata": {},
   "source": [
    "##### 2. You must have `ecr:CreateRepository` and appropriate push permissions associated with your execution role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ea72c-6da0-4236-8975-f70b616ceaa2",
   "metadata": {},
   "source": [
    "##### 3. NIM public ECR image is currently available only in `us-east-1` region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acad648-c837-40bc-b2e3-94016333ea2b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa383fca-0ffb-45f9-a6cf-1849d117a386",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686a50f-24d5-4778-a02d-28efc31373b7",
   "metadata": {},
   "source": [
    "Installs the dependencies and setup roles required to package the model and create SageMaker endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7578a7de-7ed3-4105-bec7-e5d3b04cd4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import json\n",
    "import os\n",
    "import sagemaker\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "region = sess.region_name\n",
    "sts_client = sess.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7acbd-edad-4e3e-9386-1e587879b2a5",
   "metadata": {},
   "source": [
    "### Set Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcee757",
   "metadata": {},
   "source": [
    "In this example, since we are deploying `Mixtral-8x7B` we define some configurations below for retrieving our ECR image for NIM along with some other requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f64791-1c45-4b84-9b1a-1e3ebc60d2de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mixtral-8x7b-instruct\n",
    "public_nim_image = \"public.ecr.aws/nvidia/nim:mixtral-8x7b-instruct-v01-1.0.0\"\n",
    "nim_model = \"nim-mixtral-8x7b-instruct\"\n",
    "sm_model_name = \"nim-mixtral-8x7b-instruct\"\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "payload_model = \"mistralai/mixtral-8x7b-instruct-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb05462",
   "metadata": {},
   "source": [
    "### NIM Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851abe8-9ca4-403b-be7f-aef56dfa4b9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "We first pull the NIM image from public ECR and then push it to private ECR repo within your account for deploying on SageMaker endpoint. \n",
    "\n",
    "Note, as mentioned previously:\n",
    "  - NIM ECR image is currently available only in `us-east-1` region\n",
    "  - You must have `ecr:CreateRepository` and appropriate push permissions associated with your execution role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944110e5-15bc-4a94-a731-7d4ea9344e9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS account ID: 570598552974\n",
      "Public NIM Image: public.ecr.aws/nvidia/nim:mixtral-8x7b-instruct-v01-1.0.0\n",
      "mixtral-8x7b-instruct-v01-1.0.0: Pulling from nvidia/nim\n",
      "Digest: sha256:29183d1f5b27fbb95963cc539f313c1d7f92458bc440aaa9599d4eecdae4a582\n",
      "Status: Image is up to date for public.ecr.aws/nvidia/nim:mixtral-8x7b-instruct-v01-1.0.0\n",
      "public.ecr.aws/nvidia/nim:mixtral-8x7b-instruct-v01-1.0.0\n",
      "Resolved account: 570598552974\n",
      "Resolved region: us-west-2\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "The push refers to repository [570598552974.dkr.ecr.us-west-2.amazonaws.com/nim-mixtral-8x7b-instruct]\n",
      "368e1459fd45: Preparing\n",
      "1c581f8c3425: Preparing\n",
      "d1247f59f9e1: Preparing\n",
      "1226ae4cbcd1: Preparing\n",
      "a73a9c988583: Preparing\n",
      "ff45a40404c2: Preparing\n",
      "fa87cb652a48: Preparing\n",
      "69a5953b82a5: Preparing\n",
      "9cc945697f55: Preparing\n",
      "861b0723f458: Preparing\n",
      "277cc9d6a73f: Preparing\n",
      "0b60d597c587: Preparing\n",
      "8c01e44155fa: Preparing\n",
      "8921a0fe7f12: Preparing\n",
      "28e03e31e934: Preparing\n",
      "41c3d70405ca: Preparing\n",
      "0a7e6d30c81b: Preparing\n",
      "1e25686ea5fb: Preparing\n",
      "ff45a40404c2: Waiting\n",
      "dcedf2759659: Preparing\n",
      "fa87cb652a48: Waiting\n",
      "9cc945697f55: Waiting\n",
      "942dbc2d9c5e: Preparing\n",
      "69a5953b82a5: Waiting\n",
      "861b0723f458: Waiting\n",
      "022efe544587: Preparing\n",
      "277cc9d6a73f: Waiting\n",
      "0b60d597c587: Waiting\n",
      "98f7d20dcc25: Preparing\n",
      "6c92fd942c38: Preparing\n",
      "8c01e44155fa: Waiting\n",
      "8df5e2594156: Preparing\n",
      "8921a0fe7f12: Waiting\n",
      "759fa8105e11: Preparing\n",
      "28e03e31e934: Waiting\n",
      "39b39650c39c: Preparing\n",
      "41c3d70405ca: Waiting\n",
      "0a7e6d30c81b: Waiting\n",
      "4d132af2080a: Preparing\n",
      "bae3163c64b8: Preparing\n",
      "1e25686ea5fb: Waiting\n",
      "f0fc8a1ca0cb: Preparing\n",
      "dcedf2759659: Waiting\n",
      "90efea7ecd8e: Preparing\n",
      "b6a0147bcf99: Preparing\n",
      "8ceb9643fb36: Preparing\n",
      "942dbc2d9c5e: Waiting\n",
      "6c92fd942c38: Waiting\n",
      "8df5e2594156: Waiting\n",
      "98f7d20dcc25: Waiting\n",
      "022efe544587: Waiting\n",
      "759fa8105e11: Waiting\n",
      "39b39650c39c: Waiting\n",
      "4d132af2080a: Waiting\n",
      "bae3163c64b8: Waiting\n",
      "f0fc8a1ca0cb: Waiting\n",
      "8ceb9643fb36: Waiting\n",
      "b6a0147bcf99: Waiting\n",
      "90efea7ecd8e: Waiting\n",
      "368e1459fd45: Layer already exists\n",
      "d1247f59f9e1: Layer already exists\n",
      "1c581f8c3425: Layer already exists\n",
      "a73a9c988583: Layer already exists\n",
      "1226ae4cbcd1: Layer already exists\n",
      "fa87cb652a48: Layer already exists\n",
      "ff45a40404c2: Layer already exists\n",
      "69a5953b82a5: Layer already exists\n",
      "9cc945697f55: Layer already exists\n",
      "861b0723f458: Layer already exists\n",
      "277cc9d6a73f: Layer already exists\n",
      "0b60d597c587: Layer already exists\n",
      "28e03e31e934: Layer already exists\n",
      "8c01e44155fa: Layer already exists\n",
      "8921a0fe7f12: Layer already exists\n",
      "41c3d70405ca: Layer already exists\n",
      "0a7e6d30c81b: Layer already exists\n",
      "dcedf2759659: Layer already exists\n",
      "942dbc2d9c5e: Layer already exists\n",
      "022efe544587: Layer already exists\n",
      "1e25686ea5fb: Layer already exists\n",
      "8df5e2594156: Layer already exists\n",
      "98f7d20dcc25: Layer already exists\n",
      "759fa8105e11: Layer already exists\n",
      "6c92fd942c38: Layer already exists\n",
      "39b39650c39c: Layer already exists\n",
      "bae3163c64b8: Layer already exists\n",
      "4d132af2080a: Layer already exists\n",
      "f0fc8a1ca0cb: Layer already exists\n",
      "90efea7ecd8e: Layer already exists\n",
      "b6a0147bcf99: Layer already exists\n",
      "8ceb9643fb36: Layer already exists\n",
      "latest: digest: sha256:29183d1f5b27fbb95963cc539f313c1d7f92458bc440aaa9599d4eecdae4a582 size: 7010\n",
      "570598552974.dkr.ecr.us-west-2.amazonaws.com/nim-mixtral-8x7b-instruct\n",
      "Errors: WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get AWS account ID\n",
    "result = subprocess.run(['aws', 'sts', 'get-caller-identity', '--query', 'Account', '--output', 'text'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error getting AWS account ID: {result.stderr}\")\n",
    "else:\n",
    "    account = result.stdout.strip()\n",
    "    print(f\"AWS account ID: {account}\")\n",
    "\n",
    "bash_script = f\"\"\"\n",
    "echo \"Public NIM Image: {public_nim_image}\"\n",
    "docker pull {public_nim_image}\n",
    "\n",
    "\n",
    "echo \"Resolved account: {account}\"\n",
    "echo \"Resolved region: {region}\"\n",
    "\n",
    "nim_image=\"{account}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
    "\n",
    "# Ensure the repository name adheres to AWS constraints\n",
    "repository_name=$(echo \"{nim_model}\" | tr '[:upper:]' '[:lower:]' | tr -cd '[:alnum:]._/-')\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"$repository_name\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"$repository_name\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin \"{account}.dkr.ecr.{region}.amazonaws.com\"\n",
    "\n",
    "docker tag {public_nim_image} $nim_image\n",
    "docker push $nim_image\n",
    "echo -n $nim_image\n",
    "\"\"\"\n",
    "nim_image=f\"{account}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
    "# Run the bash script and capture real-time output\n",
    "process = subprocess.Popen(bash_script, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == b'' and process.poll() is not None:\n",
    "        break\n",
    "    if output:\n",
    "        print(output.decode().strip())\n",
    "\n",
    "stderr = process.stderr.read().decode()\n",
    "if stderr:\n",
    "    print(\"Errors:\", stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18863fd4-0893-4022-9ab0-38e1af1512d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "We print the private ECR NIM image in your account that we will be using for SageMaker deployment. \n",
    "- Should be similar to  `\"<ACCOUNT ID>.dkr.ecr.<REGION>.amazonaws.com/<NIM_MODEL>:latest\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79bb63ac-2a7a-4beb-b0dd-77a4473e1a67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570598552974.dkr.ecr.us-west-2.amazonaws.com/nim-mixtral-8x7b-instruct\n"
     ]
    }
   ],
   "source": [
    "print(nim_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8451e1-5683-4e88-995c-3a0250c99e39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518de4e-dcad-4944-9025-484878edb00b",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9efc86-0cf2-403b-9502-fd294acb4cb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Before proceeding further, please set your NGC API Key.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f1f264-ebd8-4c6a-9926-7a21afd89ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set your NGC API key here\n",
    "NGC_API_KEY = \"SET KEY HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5bce6-3807-43c3-866f-80543cfdedbf",
   "metadata": {},
   "source": [
    "Pass in the **NGC_API_KEY**, and define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b784149-2ec3-4e29-a7cf-3636843dee8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": nim_image,\n",
    "    \"Environment\": {\"NGC_API_KEY\": NGC_API_KEY}\n",
    "}\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2f0e6-0377-4a13-9cc3-c345adf08c86",
   "metadata": {},
   "source": [
    "Next we create endpoint configuration, here we are deploying the Mixtral model on the specified instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af8b7c-9347-4203-aea5-f44392449f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = sm_model_name\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 850\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51121a-a662-4078-a0c6-b163cda0a718",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75add3d0-100f-4740-b326-6f54af7e9c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-west-2:570598552974:endpoint/nim-mixtral-8x7b-instruct\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = sm_model_name\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2d4bc4-b77b-4137-930e-7517295a041c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:570598552974:endpoint/nim-mixtral-8x7b-instruct\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97e4c-6dd8-4d9a-841c-e443b7c1583f",
   "metadata": {},
   "source": [
    "## Test Inference and Streaming Inference with Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9146f44-b85c-4125-b842-fceaf5c3cfa8",
   "metadata": {},
   "source": [
    "Once we have the endpoint's status as `InService` we can use a sample text to do a chat completion inference request using json as the payload format. For inference request format, currently NIM on SageMaker supports the OpenAI API chat completions inference protocol. For explanation of supported parameters please see [this link](https://platform.openai.com/docs/api-reference/chat). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d36583-d6b0-4fdf-a659-c088f913034a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> The model's name in the inference request payload needs to be the name of the NIM model. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a57265e9-98bb-4255-ad7d-143e3aeaf9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-c6262b90892348b19e2eea30965366af\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1722008071,\n",
      "  \"model\": \"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \" Optimum Neuron is a library developed by Hugging Face for efficiently training and deploying large language models (LLMs) on Amazon Web Services (AWS) infrastructure. The library provides a simple interface for defining and launching training jobs on AWS, as well as optimized model configurations that take advantage of hardware acceleration.\\n\\nHere's a detailed explanation of how Optimum Neuron helps compile LLMs for AWS infrastructure:\\n\\n1. Preparing the model: First, you need to select a model architecture and pre-trained weights from the Hugging Face Model Hub or train your own model. Optimum Neuron provides pre-configured model classes for various architectures such as BERT, RoBERTa, DistilBERT, etc. These model classes have built-in optimizations for various hardware and software configurations.\\n2. AWS Infrastructure setup: Optimum Neuron integrates with Amazon SageMaker, a fully managed machine learning service that makes it easy to build, train, and deploy machine learning models. You can use SageMaker's managed services to launch and manage training jobs, data storage, and model deployment.\\n3. Data pre-processing: Optimum Neuron provides built-in data pre-processing utilities for various data formats, including CSV, JSON, and TFRecord. These utilities help you prepare your data for training and make it compatible with the AWS infrastructure.\\n4. Model compilation: Once you have prepared the model and data, you can use Optimum Neuron to compile the model for AWS infrastructure. Optimum Neuron optimizes the model for various hardware accelerators on AWS, such as GPUs and TPUs. You can also specify the configuration of the hardware to optimize the model for specific use cases.\\n5. Training job launch: After compiling the model, you can use Optimum Neuron to launch a training job on AWS. Optimum Neuron integrates with SageMaker's managed services to launch and manage training jobs. You can specify the training job configuration, such as the instance type, data location, and training script.\\n6. Monitoring and evaluation: During training, you can monitor the progress of the training job using SageMaker's managed services. Optimum Neuron provides built-in evaluation metrics to help you assess the performance of the model.\\n7. Model deployment: Once the training is complete, you can deploy the model using Optimum Neuron. Optimum Neuron provides utilities for deploying the model to various endpoints, such as Amazon Elastic Container Service (ECS), Amazon Elastic Kubernetes Service (EKS), or Amazon Lambda. You can also use Optimum Neuron to package the model as a Docker container for deployment on other platforms.\\n\\nOverall, Optimum Neuron simplifies the process of training and deploying LLMs on AWS infrastructure by providing a unified interface for data preparation, model compilation, training job launch, and model deployment. Optimum Neuron's optimized model configurations and built-in utilities help you take advantage of hardware acceleration and achieve optimal performance on AWS.\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 57,\n",
      "    \"total_tokens\": 728,\n",
      "    \"completion_tokens\": 671\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain to me in detail how Optimum Neuron helps compile LLMs for AWS infrastructure\"}\n",
    "]\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 1024\n",
    "}\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1112217-3d99-409c-8329-b8e86c17782b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Optimum Neuron is a library developed by Hugging Face for efficiently training and deploying large language models (LLMs) on Amazon Web Services (AWS) infrastructure. The library provides a simple interface for defining and launching training jobs on AWS, as well as optimized model configurations that take advantage of hardware acceleration.\n",
      "\n",
      "Here's a detailed explanation of how Optimum Neuron helps compile LLMs for AWS infrastructure:\n",
      "\n",
      "1. Preparing the model: First, you need to select a model architecture and pre-trained weights from the Hugging Face Model Hub or train your own model. Optimum Neuron provides pre-configured model classes for various architectures such as BERT, RoBERTa, DistilBERT, etc. These model classes have built-in optimizations for various hardware and software configurations.\n",
      "2. AWS Infrastructure setup: Optimum Neuron integrates with Amazon SageMaker, a fully managed machine learning service that makes it easy to build, train, and deploy machine learning models. You can use SageMaker's managed services to launch and manage training jobs, data storage, and model deployment.\n",
      "3. Data pre-processing: Optimum Neuron provides built-in data pre-processing utilities for various data formats, including CSV, JSON, and TFRecord. These utilities help you prepare your data for training and make it compatible with the AWS infrastructure.\n",
      "4. Model compilation: Once you have prepared the model and data, you can use Optimum Neuron to compile the model for AWS infrastructure. Optimum Neuron optimizes the model for various hardware accelerators on AWS, such as GPUs and TPUs. You can also specify the configuration of the hardware to optimize the model for specific use cases.\n",
      "5. Training job launch: After compiling the model, you can use Optimum Neuron to launch a training job on AWS. Optimum Neuron integrates with SageMaker's managed services to launch and manage training jobs. You can specify the training job configuration, such as the instance type, data location, and training script.\n",
      "6. Monitoring and evaluation: During training, you can monitor the progress of the training job using SageMaker's managed services. Optimum Neuron provides built-in evaluation metrics to help you assess the performance of the model.\n",
      "7. Model deployment: Once the training is complete, you can deploy the model using Optimum Neuron. Optimum Neuron provides utilities for deploying the model to various endpoints, such as Amazon Elastic Container Service (ECS), Amazon Elastic Kubernetes Service (EKS), or Amazon Lambda. You can also use Optimum Neuron to package the model as a Docker container for deployment on other platforms.\n",
      "\n",
      "Overall, Optimum Neuron simplifies the process of training and deploying LLMs on AWS infrastructure by providing a unified interface for data preparation, model compilation, training job launch, and model deployment. Optimum Neuron's optimized model configurations and built-in utilities help you take advantage of hardware acceleration and achieve optimal performance on AWS.\n"
     ]
    }
   ],
   "source": [
    "content = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21b99b-ce34-4375-9bc7-d7e8ef80f262",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Try streaming inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b89469-1f88-45d4-b103-ce2df4232037",
   "metadata": {},
   "source": [
    "NIM on SageMaker also supports streaming inference and you can enable that by setting **`\"stream\"` as `True`** in the payload and by using [`invoke_endpoint_with_response_stream`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint_with_response_stream.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71632bf8-5297-45db-9a92-a4a371b7da26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain to me in detail what inference engines and llm serving frameworks are\"}\n",
    "]\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"stream\": True\n",
    "}\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/jsonlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9d24a-a23e-40aa-afdb-da2a63f624fe",
   "metadata": {},
   "source": [
    "We have some postprocessing code for the streaming output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e05742b6-f8e4-4116-92ed-e9b156b6cfe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'd be happy to explain!\n",
      "\n",
      "An inference engine is a key component of expert systems and other artificial intelligence (AI) applications. It is responsible for reasoning and making decisions based on the rules and knowledge encoded in the system. The inference engine applies various logical inference techniques to draw conclusions and make predictions based on the available data. In other words, it uses the knowledge base to make inferences about new situations or data.\n",
      "\n",
      "Inference engines can be classified into several categories based on the inference techniques they use, such as forward chaining, backward chaining, resolution, and constraint satisfaction. Forward chaining starts with the available data and applies the rules to deduce new knowledge until a goal is achieved. Backward chaining, on the other hand, starts with the goal and works backward to find the evidence needed to reach that goal.\n",
      "\n",
      "On the other hand, an LLM (Language Learning Model) serving framework is a system used to deploy and serve large pre-trained language models for natural language processing tasks, such as text classification, summarization, translation, and question answering. These models are typically very large, consisting of billions of parameters, and require significant computational resources to run.\n",
      "\n",
      "An LLM serving framework provides a set of tools and APIs that allow developers to easily deploy these models on the cloud or edge devices and serve predictions to client applications. The framework handles tasks such as loading the model, managing model versions, scaling to handle multiple concurrent requests, and optimizing the inference speed. It also provides interfaces for users to interact with the model, such as REST APIs, SDKs, and web interfaces.\n",
      "\n",
      "Some popular LLM serving frameworks include TensorFlow Serving, ONNX Runtime, and TorchServe. These frameworks support various deep learning models, and provide features such as model optimization, multi-model serving, and multi-language support.\n",
      "\n",
      "In summary, an inference engine is a system that uses logical inference techniques to make decisions based on available data, while an LLM serving framework is a system that deploys and serves large pre-trained language models for natural language processing tasks."
     ]
    }
   ],
   "source": [
    "event_stream = response['Body']\n",
    "accumulated_data = \"\"\n",
    "start_marker = 'data:'\n",
    "end_marker = '\"finish_reason\":null}]}'\n",
    "\n",
    "for event in event_stream:\n",
    "    try:\n",
    "        payload = event.get('PayloadPart', {}).get('Bytes', b'')\n",
    "        if payload:\n",
    "            data_str = payload.decode('utf-8')\n",
    "\n",
    "            accumulated_data += data_str\n",
    "\n",
    "            # Process accumulated data when a complete response is detected\n",
    "            while start_marker in accumulated_data and end_marker in accumulated_data:\n",
    "                start_idx = accumulated_data.find(start_marker)\n",
    "                end_idx = accumulated_data.find(end_marker) + len(end_marker)\n",
    "                full_response = accumulated_data[start_idx + len(start_marker):end_idx]\n",
    "                accumulated_data = accumulated_data[end_idx:]\n",
    "\n",
    "                try:\n",
    "                    data = json.loads(full_response)\n",
    "                    content = data.get('choices', [{}])[0].get('delta', {}).get('content', \"\")\n",
    "                    if content:\n",
    "                        print(content, end='', flush=True)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing event: {e}\", flush=True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19063f6-b6c0-4de2-a193-e482f26f7406",
   "metadata": {},
   "source": [
    "---\n",
    "### Delete endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5db083f-4705-4c68-a488-f82da961be4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'c0e3552f-ead3-4380-b9ec-192c8ef17cc2',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c0e3552f-ead3-4380-b9ec-192c8ef17cc2',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Fri, 26 Jul 2024 15:35:52 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd7ee8-e2ca-4d15-b3e6-7e56849c61c6",
   "metadata": {},
   "source": [
    "---\n",
    "### Acknowledgements\n",
    "\n",
    "Draws inspiration from NVIDIA's [nim-deploy](https://github.com/NVIDIA/nim-deploy) samples repositiory that showcases different ways NVIDIA NIMs can be deployed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f467d8-a9ff-49d7-9425-37e83e54cd19",
   "metadata": {},
   "source": [
    "---\n",
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Mistral AI\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
