{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d367a8-7115-4105-b464-d436ecae6143",
   "metadata": {},
   "source": [
    "# Transitioning from OpenAI to Mistral: A Guide\n",
    "\n",
    "*This notebook should work well with the Data Science 3.0 kernel in SageMaker Studio*\n",
    "\n",
    "This guide is designed to assist you in transitioning your OpenAI prompts and workloads to Mistral Models. Our goal is to demonstrate how Mistral models can match the capabilities of their OpenAI counterparts in handling specific tasks. While this notebook may not cover every edge case or unique use case scenario, it will provide a strong foundation for starting your journey with Mistral.\n",
    "\n",
    "We will begin by reviewing how to format prompts and create a chat completion interface. Additionally, we will highlight the performance and efficiency of Mistral models through various examples, illustrating their effectiveness in different applications \n",
    "\n",
    "## Mistral Large\n",
    "\n",
    "Mistral Large is the top-tier reasoning model for high-complexity tasks. The most powerful model of the Mistral AI family.\n",
    "\n",
    "-Fluent in English, French, Italian, German, Spanish, and strong in code<br>\n",
    "-Context window of 32k tokens, with excellent recall for retrieval augmentation<br>\n",
    "-Native function calling capacities (coming soon to Bedrock), JSON outputs<br>\n",
    "-Concise, useful, unopinionated, with fully modular<br>\n",
    "\n",
    "Mistral Large has the following inference parameters on Bedrock:<br>\n",
    "\n",
    "{<br>\n",
    "    \"prompt\": string,<br>\n",
    "    \"max_tokens\" : int,<br>\n",
    "    \"stop\" : [string],<br>\n",
    "    \"temperature\": float,<br>\n",
    "    \"top_p\": float,<br>\n",
    "    \"top_k\": int<br>\n",
    "}<br>\n",
    "\n",
    "Context window: 32k\n",
    "\n",
    "## Mixtral 8x7b\n",
    "\n",
    "Mixtral is a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.\n",
    "\n",
    "\n",
    "-It gracefully handles a context of 32k tokens.<br>\n",
    "-It handles English, French, Italian, German and Spanish.<br>\n",
    "-It shows strong performance in code generation.<br>\n",
    "\n",
    "Mixtral 8x7b has the following inference parameters on Bedrock:<br>\n",
    "\n",
    "{<br>\n",
    "    \"prompt\": string,<br>\n",
    "    \"max_tokens\" : int,<br>\n",
    "    \"stop\" : [string],<br>\n",
    "    \"temperature\": float,<br>\n",
    "    \"top_p\": float,<br>\n",
    "    \"top_k\": int<br>\n",
    "}<br>\n",
    "\n",
    "Context window: 32k\n",
    "\n",
    "\n",
    "## Benchmarks\n",
    "\n",
    "\n",
    "Mistral-Large\n",
    "\n",
    "\n",
    "| Model        | MMLU  | HellaS | WinoG | Arc C (5) | Arc C (25) | TriQA | TruthfulQA |\n",
    "|--------------|-------|--------|-------|-----------|------------|-------|------------|\n",
    "| Mistral Large| 81.2% | 89.2%  | 86.7% | 94.2%     | 94.0%      | 82.7% | 50.5%      |\n",
    "| LLAMA 2 70B  | 69.9% | 87.1%  | 83.2% | 86.0%     | 85.1%      | 77.6% | 44.7%      |\n",
    "| GPT 3.5      | 70.0% | 85.5%  | 81.6% | 85.2%     | 85.2%      | —     | —          |\n",
    "| GPT 4        | 86.4% | 95.3%  | 87.5% | —         | 96.3%      | —     | —          |\n",
    "| Claude 2     | 78.5% | —      | —     | 91.0%     | —          | 87.5% | —          |\n",
    "| Gemini Pro 1.0 | 71.8% | 84.7% | —     | —         | —          | —     | —          |\n",
    "\n",
    "\n",
    "\n",
    "Mixtral 8x7b\n",
    "\n",
    "|                    | LLAMA 2 70B | GPT-3.5 | Mixtral 8x7B |\n",
    "|--------------------|-------------|---------|--------------|\n",
    "| **MMLU (MCQ in 57 subjects)** | 69.9%      | 70.0%   | 70.6%        |\n",
    "| **HellaSwag (10-shot)**       | 87.1%      | 85.5%   | 86.7%        |\n",
    "| **ARC Challenge (25-shot)**   | 85.1%      | 85.2%   | 85.8%        |\n",
    "| **WinoGrande (5-shot)**       | 83.2%      | 81.6%   | 81.2%        |\n",
    "| **MBPP (pass@1)**             | 49.8%      | 52.2%   | 60.7%        |\n",
    "| **GSM-8K (5-shot)**           | 53.6%      | 57.1%   | 58.4%        |\n",
    "| **MT Bench (for Instruct Models)** | 6.86       | 8.32    | 8.30         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616e7ab-d13d-4369-88dd-cec31718147b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898fa5a8-eb65-4f2b-b6d3-a24b0576f2e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = \"mistral.mistral-large-2402-v1:0\"\n",
    "mixtral_model = \"mistral.mixtral-8x7b-instruct-v0:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c57230-dccd-4afc-8309-d3729c5a9f36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, model_id):\n",
    "        self.model_id = model_id\n",
    "        self.bedrock = boto3.client(service_name=\"bedrock-runtime\")\n",
    "        \n",
    "    def invoke(self, prompt, temperature=0.0, max_tokens=3000):\n",
    "        body = json.dumps({\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"prompt\": prompt,\n",
    "            \"stop\": [\"</s>\"]\n",
    "        })\n",
    "        response = self.bedrock.invoke_model(\n",
    "            body=body, \n",
    "            modelId=self.model_id)\n",
    "\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        return response_body['outputs'][0]['text']\n",
    "\n",
    "\n",
    "DEFAULT_MODEL = \"mistral.mistral-large-2402-v1:0\"\n",
    "llm_mistral_large = LLM(DEFAULT_MODEL)\n",
    "\n",
    "\n",
    "mixtral_model = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "llm_mixtral = LLM(mixtral_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29911a4-2120-4d29-adcc-aed6ee0521ba",
   "metadata": {},
   "source": [
    "## How to Format Prompts for Mistral on Bedrock\n",
    "\n",
    "- Start with `<s>` to indicate the beginning and end of your input.\n",
    "- Include your instructions between `[INST]` and `[/INST]`.\n",
    "\n",
    "*`<s>` token denotes the beginning of a prompt - Mistral will understand that there will be no other tokens before this special beginning of sequence token.*\n",
    "\n",
    "*`<s>` token denotes the end of a prompt - Mistral will understand that there will be no other tokens after this special end of sequence token.*\n",
    "\n",
    "### Comparison with OpenAI Prompts\n",
    "\n",
    "OpenAI models use a chat completion API where your instructions for the model go into:\n",
    "\n",
    "System Role: This role sets the context or instructions for the model.<br>\n",
    "User Role: This role includes the specific task or question posed to the model.<br>\n",
    "Assistant Role: This role is the model's response.\n",
    "\n",
    "Mistral models on Bedrock behave differently. There are no assigned roles - your instructions for the model go between the `[INST]` strings.\n",
    "\n",
    "Mistral models are designed for text input and text output only; they do not currently support multimodal capabilities.\n",
    "\n",
    "### Best Practices for Prompt Engineering\n",
    "\n",
    "Like many LLMs today, Mistral Models generally benefit from the following:\n",
    "\n",
    "- Be clear in your instructions.\n",
    "- Delimiters like `###`, `<<< >>>` specify the boundary between different sections of the text.\n",
    "- A few-shot approach (multiple examples in context) is typically more performative than a zero-shot approach (no examples).\n",
    "- For more complex reasoning tasks, chain of thought or thinking step by step can help the model reason through problems.\n",
    "- Bad data in can lead to poor data out - make sure to preprocess your data and if your data is larger than the context window, make sure to chunk your text.\n",
    "\n",
    "The typical rules of prompt engineering that you are familiar with apply to Mistral as they do to OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a5d3e-05b6-4dc7-9d39-1506ff78334d",
   "metadata": {},
   "source": [
    "## Use Mistral Large for your Reasoning Tasks\n",
    "\n",
    "Mistral Large is the closest comparison to the GPT-4 family of models offered at the time of this writing. If you are looking to migrate from GPT-4 and you want to use Bedrock, then Mistral Large is the model you should evaulate first from Mistral. Similar to GPT-4, Mistral Large excels in handling complex reasoning and mathematical tasks. If you're looking for Mixtral 8x22b, please use SagemMaker Jumpstart. Let's examine a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91048d9-a7e3-4cf0-b572-35facba7b0e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasoning_problem = \"<s>[INST] Sara, Mary, and Jane are at a party. Sara is wearing red pants, and the person wearing green pants is standing next to the person in blue pants. Jane is standing next to Sara. What color pants is Jane wearing? [/INST] \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee76dce-5033-4934-902a-c2beddccc622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasoning_prompt = llm_mistral_large.invoke(reasoning_problem, temperature=0.0, max_tokens=300)\n",
    "print(reasoning_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afa3c8-621b-410b-9008-e4f967c2400b",
   "metadata": {},
   "source": [
    "Mistral Large thought out loud and correctly answered our trick question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679e3ce-ab44-4c9c-8493-0390ac6e4d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "car_calculation = \"\"\"<s>[INST]\n",
    "\n",
    "Sara bought a car that is valued at $20,000 today in 2024. According to her research, the car will depreciate in value at a rate of 7% per year. She also went to a car dealership that told her the car would decrease at a rate of 8.5% per year.\n",
    "\n",
    "Sara wants to plan for the future. She is planning to sell her car in 6 years directly to a friend. What will the car be worth then? Walk Sara through the math and how much the car will be worth starting today to six years from now. [/INST] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a1694-73ab-4ae9-a76b-17f7d36367f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasoning_prompt_2= llm_mistral_large.invoke(car_calculation, temperature=0.0, max_tokens=500)\n",
    "print(reasoning_prompt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c27536-a58f-4ffa-bcb4-5c9af0eae146",
   "metadata": {},
   "source": [
    "Mistral Large correctly calculated the depreciation for both Sara & the dealership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5bc21b-c7a4-4ec0-af1a-4db5dedeb639",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_calculation = \"\"\"<s>[INST]\n",
    "\n",
    "Calculate the visible horizon for the following people: \n",
    "\n",
    "Preston is 6'3\n",
    "Amy is 6'1\n",
    "John is 5'11\n",
    "Emma is 6'5\n",
    "Tim is 6'5\n",
    "\n",
    "Show your work for each person and then decide who can see the farthest.[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe82c718-e57a-4565-a219-9b092604f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_distance= llm_mistral_large.invoke(horizon_calculation, temperature=0.0, max_tokens=500)\n",
    "print(horizon_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1c9ae-789a-451d-8522-58950c264991",
   "metadata": {},
   "source": [
    "## Creating a Chatbot with Mistral Large on Bedrock\n",
    "\n",
    "On Bedrock, you can create a chat completion by following these steps:\n",
    "\n",
    "1. **Send the Initial Prompt**: Start by sending your initial prompt to the model.\n",
    "2. **Receive the Response**: Get the response from the model.\n",
    "3. **Append and Format**: For the next turn, append the initial prompt and the response. Bookend the prompts with `[INST]` and the responses with `</s>`.\n",
    "\n",
    "For more detailed guidance and resources, visit: [How to Prompt Mistral AI Models and Why](https://community.aws/content/2dFNOnLVQRhyrOrMsloofnW0ckZ/how-to-prompt-mistral-ai-models-and-why?lang=en).\n",
    "\n",
    "### Example\n",
    "\n",
    "Below is an example demonstrating how this process works:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5f222-ac3b-4cef-ab56-5c00e5b12e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial prompt\n",
    "prompt_3 = \"<s>[INST] I enjoy hiking in the mountains. [/INST]\"\n",
    "\n",
    "# Get the first response from the model\n",
    "chat_completion = llm_mistral_large.invoke(prompt_3, temperature=0.3, max_tokens=100)\n",
    "print(chat_completion)\n",
    "\n",
    "# Append the first prompt and the response to create the next prompt\n",
    "next_prompt = (\n",
    "    f\"<s>[INST] I enjoy hiking in the mountains. [/INST] {chat_completion}</s> \"\n",
    "    \"[INST] What are some tips for mountain hiking? [/INST]\"\n",
    ")\n",
    "\n",
    "# Get the response for the next prompt\n",
    "next_response = llm_mistral_large.invoke(next_prompt, temperature=0.3, max_tokens=100)\n",
    "print(next_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b8d5a-096e-4f4e-9595-472b87a0afe0",
   "metadata": {},
   "source": [
    "## Complex Reasoning Task: Bedrock Pricing Calculations\n",
    "\n",
    "Lastly, we'll perform a complex reasoning task involving Bedrock pricing for models. We'll follow a specific formula for calculations, reasoning, and decision making. This will demonstrate how Mistral Large can handle intricate tasks involving both reasoning and mathematical computations within a chatbot interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec1335-08e3-4eb8-8ef4-295e006f4738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiturn_conversation(llm, conversation_history):\n",
    "    \"\"\"\n",
    "    Conducts a multi-turn conversation using the provided LLM instance.\n",
    "\n",
    "    Args:\n",
    "    - llm: An instance of the LLM class.\n",
    "    - conversation_history: The conversation history containing prompts and responses.\n",
    "\n",
    "    Returns:\n",
    "    - A response from the model for the latest prompt.\n",
    "    \"\"\"\n",
    "    # Join conversation history to create prompt\n",
    "    prompt = \" \".join(conversation_history)\n",
    "\n",
    "    # Invoke the model with the prompt\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fcaddd-aac2-456c-9f09-3a808a3b2f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data = [\n",
    "    {\"Model\": \"Claude 3\", \"Version\": \"Haiku\", \"Pricing Metric\": \"per 1k tokens\", \"Input Price\": 0.00025, \"Output Price\": 0.00125, \"Input Task Size\": 0, \"Output Task Size\": 0, \"Throughput\": 0, \"Input Usage\": 0, \"Output Usage\": 0, \"Estimated Cost\": 0, \"Pricing Model\": \"On-Demand\", \"Fine-tuning Available\": \"No\", \"Available\": \"Yes\"},\n",
    "    {\"Model\": \"Claude 3\", \"Version\": \"Sonnet\", \"Pricing Metric\": \"per 1k tokens\", \"Input Price\": 0.00300, \"Output Price\": 0.01500, \"Input Task Size\": 0, \"Output Task Size\": 0, \"Throughput\": 0, \"Input Usage\": 0, \"Output Usage\": 0, \"Estimated Cost\": 0, \"Pricing Model\": \"On-Demand\", \"Fine-tuning Available\": \"No\", \"Available\": \"Yes\"},\n",
    "    {\"Model\": \"Claude 3\", \"Version\": \"Opus\", \"Pricing Metric\": \"per 1k tokens\", \"Input Price\": 0.01500, \"Output Price\": 0.07500, \"Pricing Model\": \"On-Demand\", \"Fine-tuning Available\": \"No\", \"Available\": \"Yes\"},\n",
    "    {\"Model\": \"Claude Instant\", \"Version\": \"Claude Instant\", \"Pricing Metric\": \"per 1k tokens\", \"Input Price\": 0.00080, \"Output Price\": 0.00240, \"Pricing Model\": \"On-Demand\", \"Fine-tuning Available\": \"No\"},\n",
    "    {\"Model\": \"Claude 2.0/2.1\", \"Version\": \"Claude 2.0/2.1\", \"Pricing Metric\": \"per 1k tokens\", \"Input Price\": 0.00800, \"Output Price\": 0.02400, \"Pricing Model\": \"On-Demand\", \"Fine-tuning Available\": \"No\"},\n",
    "    {\"Model\": \"Jurassic\", \"Version\": \"Jurassic-2 Mid\", \"Pricing Metric\": \"per 1k tokens\", \"Input Price\": 0.01250, \"Output Price\": 0.01250, \"Pricing Model\": \"On-Demand\", \"Fine-tuning Available\": \"No\"},\n",
    "    {\"Model\": \"Jurassic\", \"Version\": \"Jurassic-2 Ultra\", \"Pricing Metric\": \"per 1k tokens\", \"Input Price\": 0.01880, \"Output Price\": 0.01880, \"Pricing Model\": \"On-Demand\", \"Fine-tuning Available\": \"No\"},\n",
    "    {\"Model\": \"Command\", \"Version\": \"Command-Light\", \"Pricing Metric\": \"per 1k tokens\", \"Input Price\": 0.00030, \"Output Price\": 0.00060, \"Pricing Model\": \"On-Demand\", \"Price to Train 1,000 Tokens\": 0.0010, \"Price to Store per Month\": 1.95, \"Fine-tuning Available\": \"Yes\"},\n",
    "    {\"Model\": \"Command\", \"Version\": \"Command\", \"Pricing Metric\": \"per 1k tokens\", \"Input Price\": 0.00150, \"Output Price\": 0.00200, \"Pricing Model\": \"On-Demand\", \"Price to Train 1,000 Tokens\": 0.0040, \"Price to Store per Month\": 1.95, \"Fine-tuning Available\": \"Yes\"},\n",
    "    {\"Model\": \"Llama-2\", \"Version\": \"Llama-2 Chat (13b)\", \"Pricing Metric\": \"per 1k tokens\", \"Input Price\": 0.00075, \"Output Price\": 0.00100, \"Pricing Model\": \"On-Demand\", \"Fine-tuning Available\": \"No\"},\n",
    "    {\"Model\": \"Llama-2\", \"Version\": \"Llama-2 Chat (70b)\", \"Pricing Metric\": \"per 1k tokens\", \"Input Price\": 0.00195, \"Output Price\": 0.00256, \"Pricing Model\": \"On-Demand\", \"Fine-tuning Available\": \"No\"},\n",
    "    {\"Model\": \"Llama-2\", \"Version\": \"Llama 2 Pre-trained (13b)\", \"Pricing Metric\":\"per 1k tokens\", \"Price to Train 1,000 Tokens\": 0.0015, \"Price to Store per Month\": 1.95, \"Fine-tuning Available\": \"Yes\"} \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad4c21-017b-4aae-8c1d-354733f2b445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_4 =f\"\"\"<s>[INST]\n",
    "\n",
    "Data:{model_data}\n",
    "\n",
    "The table includes model details, such as model version, pricing metric, input and output prices, average token counts for input and output tasks, and requests per minute. Based on this, you will compute the monthly input and output token usage and the estimated cost for each model.\n",
    "\n",
    "Objective:\n",
    "\n",
    "You are a generative AI expert and an expert in Amazon ML and AI products and services. Focus only on utilizing models available on Amazon Bedrock provided in the model data.\n",
    "\n",
    "User Interaction Guide:\n",
    "\n",
    "1. Understanding the Use Case: Engage with users to comprehend their specific generative AI needs.\n",
    "2. Data Availability: Inquire if they possess substantial in-house data. This helps determine the necessity for model fine-tuning. Typically, fine-tuning is not recommended unless required for extensive classification tasks or specific copywriting applications.\n",
    "3. Task Sizes: Ascertain the average token counts for both input (the task size) and output (the result size).\n",
    "4. Request Frequency: Determine their expected number of requests per minute.\n",
    "\n",
    "Calculations:\n",
    "\n",
    "Using the gathered information, calculate the following for the models that you choose for the user:\n",
    "- Monthly input usage in tokens. (Input task size * Throughput * 60 * 24 * 30)\n",
    "- Monthly output usage in tokens. (Output task size * Throughput * 60 *24 * 30)\n",
    "- Estimated monthly cost. (Monthy input usage in tokens * Input price) + (Monthly output usage in tokens * Output price)\n",
    "- Prices should be calculated per 1000 tokens\n",
    "\n",
    "It's ok for you to make some assumptions and use averages based on what people tell you. Always round up from one token to many and when you give a recommendation make sure its two different model providers. Always show the customer the math you are doing. \n",
    "\n",
    "[/INST]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eeca9f-a11c-4c4b-bb78-49ad8ee2a5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_response = llm_mistral_large.invoke(prompt_4, temperature=0.1, max_tokens=1000)\n",
    "print(initial_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104d243-6026-4905-8f0f-f984de01147e",
   "metadata": {},
   "source": [
    "Let's run a test to ensure our initial work is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74856887-4934-4177-8dd6-91963c41364a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next_prompt = f\"<s>[INST] {prompt_4} {initial_response}</s> [INST] We have a classification use case that has the following details: each input is a paragraph, each output is a word in english and we expect to be classiying 100 inputs per minute. [/INST]\"\n",
    "\n",
    "# Invoke the model for the next response\n",
    "next_response = llm_mistral_large.invoke(next_prompt, temperature=0.1, max_tokens=2000)\n",
    "print(\"Next response from model:\", next_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b29f8e-e449-48d0-9f53-590b1a16a64d",
   "metadata": {},
   "source": [
    "Let's put everything we just reviewed together. Below we will create a multiturn chatbot that takes the user input and the model responses and appends them to the next response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a713a0b-4dba-46d2-88cc-3438501f06e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize conversation history with the initial prompt and response\n",
    "conversation_history = [prompt_4, initial_response]\n",
    "\n",
    "# Conduct the conversation interactively\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # Check for exit condition\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting conversation...\")\n",
    "        break\n",
    "\n",
    "    # Append user input to conversation history\n",
    "    conversation_history.append(f\"</s> [INST] {user_input} [/INST]\")\n",
    "\n",
    "    # Conduct multiturn conversation\n",
    "    response = multiturn_conversation(llm_mistral_large, conversation_history)\n",
    "\n",
    "    # Print model response\n",
    "    print(\"Model:\", response)\n",
    "\n",
    "    # Update conversation history with model response\n",
    "    conversation_history.append(f\"{response}</s>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc620b-661d-4eef-a37b-31f139cd32ae",
   "metadata": {},
   "source": [
    "## Additional Examples\n",
    "\n",
    "Let's examine several examples of Mistral Large completing tasks that would be benefitial to an enterprise - involving translation, classification, and Q&A.\n",
    "\n",
    "We'll utilize Mistral Large to accomplish common downstream tasks necessary for extracting insights from a call transcript. The transcript we're using is a generated transcript initiated by the author. JSON mode for Mistral Large is coming soon to Bedrock - Large can still output reliable JSON objects when prompted to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe9990-964e-4e7e-9b60-23a517fbd273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript = \"\"\"Customer: Hi, I purchased a green widget from your store about three months ago, and it's not working properly. I need some urgent help troubleshooting it.\n",
    "\n",
    "Customer Service Agent: I'm sorry to hear that your green widget isn't functioning as expected. I'll do my best to assist you. Could you please provide me with your order number or any other details about your purchase?\n",
    "\n",
    "Customer: My order number is 123456789. I just can't believe this widget stopped working already. I spent a lot of money on it and expected better quality.\n",
    "\n",
    "Customer Service Agent: I understand your frustration, and I apologize for the inconvenience. Let's see what we can do to resolve this issue. Can you describe the problem you're encountering with the green widget?\n",
    "\n",
    "Customer: Well, it seems like the widget doesn't hold a charge anymore. I've tried charging it multiple times, but it still won't turn on. It's incredibly frustrating, especially since I rely on it for my daily tasks.\n",
    "\n",
    "Customer Service Agent: I see. Let's try a few troubleshooting steps to see if we can find a solution. Could you please try using a different charging cable and power source to rule out any issues with the charger?\n",
    "\n",
    "Customer Alright, I'll give it a shot. No luck, it's still not turning on.\n",
    "\n",
    "Customer Service Agent: Thank you for trying that. It sounds like there might be a problem with the widget itself. Since you purchased it three months ago, it should still be under warranty. I can assist you with initiating a warranty claim for a replacement.\n",
    "\n",
    "Customer: I appreciate that, but I really need a solution as soon as possible. I can't afford to be without a functioning widget for too long, especially with my workload.\n",
    "\n",
    "Customer Service Agent: I completely understand your urgency, and I'll do my best to expedite the process for you. Can you confirm the serial number of the green widget for me? It should be located on the back or bottom of the device.\n",
    "\n",
    "Customer: Let me check... Okay, the serial number is GW123456789.\n",
    "\n",
    "Customer Service Agent: Thank you for providing that. I'll prioritize your warranty claim and ensure it's processed as quickly as possible. You should receive an email confirmation shortly with further instructions on how to proceed.\n",
    "\n",
    "Customer: Alright, I'll keep an eye out for the email. I hope this gets resolved soon.\n",
    "\n",
    "Customer Service Agent: I understand, and I'll make sure to follow up to ensure everything is resolved to your satisfaction. If you have any further questions or concerns, feel free to reach out to us. We're here to help.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d918121-2910-4b6d-9d0d-60afc0db8d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_extraction=f\"\"\"<s>[Inst] \n",
    "\n",
    "Transcript: {transcript}\n",
    "\n",
    "You are an expert in working with transcripts. Given the transcript below, I need you to accomplish the following tasks: \n",
    "\n",
    "1. Extract the intent of the customer/caller\n",
    "2. Extract the order number, serial number, customer name and details if provided and label them as such\n",
    "3. Answer the question: Did the service agent resolve the customer issue? Yes/No & why with specific examples from the transcript\n",
    "4. Grade the overall interaction on a scale of 1-5 with 5 being excellent. Explain why you gave the interaction that grade.\n",
    "\n",
    "Format your response as the following: \n",
    "\n",
    "1: Customer Intent, Customer Issue\n",
    "2. Product Data\n",
    "3. Interaction Grade\n",
    "\n",
    "Format #2 as a JSON object.\n",
    "\n",
    "[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d52365-141e-43eb-9393-2ca2bc34481f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcription= llm_mistral_large.invoke(transcript_extraction, temperature=0.0, max_tokens=2000)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc2629-de94-4365-95b8-4d566186329c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emails= \"\"\"\n",
    "\n",
    "\"I recently bought your RGB gaming keyboard and absolutely love the customizable lighting features! Can you guide me on how to set up different profiles for each game I play?\"\n",
    "\"I'm trying to use the macro keys on the gaming keyboard I just purchased, but they don't seem to be registering my inputs. Could you help me figure out what might be going wrong?\"\n",
    "\"I'm considering buying your gaming keyboard and I'm curious about the key switch types. What options are available and what are their main differences?\"\n",
    "\"I wanted to report a small issue where my keyboard's space bar is a bit squeaky. However, your quick-start guide was super helpful and I fixed it easily by following the lubrication tips. Just thought you might want to know!\"\n",
    "\"My new gaming keyboard stopped working within a week of purchase. None of the keys respond, and the lights don't turn on. I need a solution or a replacement as soon as possible.\"\n",
    "\"I've noticed that the letters on the keys of my gaming keyboard are starting to fade after several months of use. Is this covered by the warranty?\"\n",
    "\"I had an issue where my keyboard settings would reset every time I restarted my PC. I figured out it was due to a software conflict and resolved it by updating the firmware. Just wanted to ask if there are any new updates coming soon?\"\n",
    "\"I've been having trouble with the keyboard software not saving my configurations, and it's starting to get frustrating. What can be done to ensure my settings are saved permanently?\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d89b67-0848-4cb3-a170-f9f843e3cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_tasks = f\"\"\"<s> [INST]\n",
    "\n",
    "You're a customer service expert. Your job is to follow my instructions and perform the actions described below./n\n",
    "\n",
    "email={emails}\n",
    "\n",
    "1. [Task 1] Classify the following customer service emails into two categories - Inquiry (a customer question) or Issue (a customer has a problem we need to help them solve)./n\n",
    "2. [Task 2] Once you classify each customer service email, translate the email into french. /n\n",
    "\n",
    "The format for output should look like:/n\n",
    "\n",
    "<Label> /n <english version of email, french translation>/n\n",
    "\n",
    "[/INST]<s>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685dd6d3-8f70-4789-9660-5540300aad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step = llm_mistral_large.invoke(email_tasks,temperature=0.1, max_tokens=2000)\n",
    "print(multi_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6d2de-1219-49ff-83f0-4dc4b28629f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Mixtral 8x7b for Your GPT-3.5-Turbo Tasks\n",
    "\n",
    "Here we'll review common tasks that GPT-3.5-Turbo would typically handle and showcase how Mixtral can accomplish similar tasks with better performance. We'll also measure the time it takes to execute our API calls, referred to as Wall Time, to illustrate how fast Mixtral is. Generally, this comparison between both models hinges on fast responses and time to first token.\n",
    "\n",
    "In our examples below, we are looking at the total time to execute our specific code blocks, which includes waiting for external resources and networking requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b150a8-1ca7-433e-9d9c-e48aadcf0a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fast_objects=\"<s>[INST]List a two very fast objects that are man made or created by nature that exist on earth only, not in space.[/INST]<s> \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573057f-6725-4f85-a0f4-623716bd1a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time mixtral_response_3=llm_mixtral.invoke(fast_objects, temperature=0.5, max_tokens=500)\n",
    "print(mixtral_response_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375fd6b-af6e-4e63-8265-cf05d3c286c2",
   "metadata": {},
   "source": [
    "### Answering Questions from a Transcript\n",
    "\n",
    "In this example, we provide a transcript and ask Mixtral to answer specific questions based on the information given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbab0a-671b-44bb-a538-1bd6035f8eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript_mixtral=f\"\"\"<s>[INST]\n",
    "\n",
    "transcript={transcript}\n",
    "\n",
    "Given the transcript above, answering the following questions. If you do not know the answer, then state 'Answer not provided in transcript'. \n",
    "\n",
    "1. What was the customer's name?\n",
    "2. What color was the widget that the customer was calling about?\n",
    "3. What was the order number?\n",
    "4. Did we resolve the customer issue?\n",
    "5. What was the solution to the customer's problem?\n",
    "[/INST]<s>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fcc198-6bfd-4619-bce4-5140de9f1371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time mixtral_response_1=llm_mixtral.invoke(transcript_mixtral, temperature=0.0, max_tokens=500)\n",
    "print(mixtral_response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef85a0e-451e-4435-bca2-fa73b43aafc2",
   "metadata": {},
   "source": [
    "Mixtral was able to provide us with the correct information here; however, Mistral Large would most likely give us a more comprehensive completion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe3e76-0e21-4286-af82-d0bd385c3f34",
   "metadata": {},
   "source": [
    "### Translation\n",
    "\n",
    "We'll use the generated emails as our context and translate them into the following languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e401c8-dfcd-4fe0-b52c-3480a6411da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translate_mixtral=f\"\"\"<s>[INST]\n",
    "\n",
    "emails={emails}\n",
    "\n",
    "Translate the following customer emails into these languages only: \n",
    "\n",
    "1. French\n",
    "2. German\n",
    "3. Spanish\n",
    "\n",
    "Label each language section accordingly [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c375882-4c89-4c03-a13e-8e8eb0c690a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mixtral_prompt_2=llm_mixtral.invoke(translate_mixtral, temperature=0.0, max_tokens=3000)\n",
    "print(mixtral_prompt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f35f12-1441-40a9-bb2b-2bbeb833d35f",
   "metadata": {},
   "source": [
    "Mixtral continues to demonstrate fast and efficient performance even with complex and lengthy tasks."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
